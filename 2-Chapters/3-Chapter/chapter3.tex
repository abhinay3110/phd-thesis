%!TEX root = ../../PhD_thesis__Edouard_Leurent.tex

\graphicspath{{2-Chapters/3-Chapter/}}

\chapter{Problem Statement}
\label{chapter:3}

\begin{flushright}
	\begin{tabular}{@{}l@{}}
		\emph{Notre héritage n'est précédé d'aucun testament.}\\
		\emph{On ne se bat bien que pour les causes qu'on modèle soi-même}\\
		\emph{et avec lesquelles on se brûle en s'identifiant.}\\
	\end{tabular}
	
%%	René Char, \href{https://eleurent.github.io/sisyphe/texts/feuillets-d-hypnos.html}{\enquote{Feuillets d'Hynos}} {\small (62--63)}. \emph{Fureur et mystère.}
	René Char, \href{https://eleurent.github.io/sisyphe/texts/feuillets-d-hypnos.html}{\emph{Feuillets d'Hynos}} {\small (62--63)}.
\end{flushright}


\abstractStartChapter{}%
Having discussed at length the range of \glsxtrlong[hyper=false]{AD} modelling perspectives in \Cref{chapter:2}, we now formalize the specific problem that we are going to consider in this thesis. This chapter attempts to cast Behavioural Planning as a \glsxtrlong{MDP}, by specifying each element of a $(\cS, \cA, \Ps, \R, \discount)$ tuple suitable for a set of tactical decision-making tasks.
\minitocStartChapter{}

\section{Perceived states}

As discussed in \Cref{sec:nuts-and-bolts,sec:partial-observability}, information about the world is typically obtained from noisy sensory measurements. The Perception module is responsible for recognizing and tracking the signal from the noise, so as to provide a high-level probabilistic description of the scene. In particular, 
\begin{enumerate}[label=(\roman*)]
	\item a \emph{Mapping} layer reconstructs the geometry of the road network and its associated signage, including stop signs and traffic lights;
	\item a \emph{Localisation} layer recovers the position, velocity and heading of the ego-vehicle;
	\item a \emph{Scene understanding} layer returns the position, velocity and geometry of any vehicle or obstacle nearby.
\end{enumerate}

Since we focus on the Decision module, we will take a simplifying assumption and ignore all aspects related to Perception.
Namely, we take the liberty of assuming to a noise-free access to every feature of the driving scene that we will deem relevant.

\paragraph{Vehicles}

As mentioned in \Cref{chapter:1}, the main challenge of Behavioural Planning is to interact with other vehicles. Therefore, the state should include a description of every vehicle nearby. In addition to the ego-vehicle, indexed by 0, the scene contains a number $N_v$ of other vehicles indexed by the range $[1, N_v]$.
Any vehicle of index $i\in[0,N_v]$ is represented by 
\begin{enumerate}[label=(\roman*)]
	\item its position $(p^x_{i}, p^y_{i})\in\Real^2$,
	\item its forward speed $v_i\in\Real$, 
	\item its heading $\psi_i\in\Real$.
\end{enumerate}

The resulting joint state is the traffic description: 
\begin{equation}
\label{eq:state-space}
s = \begin{bmatrix}
p^x_{0} & p^y_{0} & v_0 & \psi_0\\
\vdots & \vdots & \vdots & \vdots\\
p^x_{{N_v}} & p^y_{{N_v}} & v_{N_v} & \psi_{N_v}\\
\end{bmatrix}
\in\gls+{cS} \eqdef\Real^{(N_v+1) \times 4}.
\end{equation}
We can make a few observations: first, the state space is continuous, which means we will have to resort to function approximation to represent either the policy $\pi$, the value function $Q$ or the dynamics $P$. Second, it has a variable size, since it depends on the number of vehicles nearby, which the function approximation scheme will have to accommodate. Its dimensionality should be in the order of fifty at most, for a dozen observed vehicles.

\paragraph{Roads}

We also assume knowledge of the road network, comprising:
\begin{enumerate}[label=(\roman*)]
	\item a graph description of the network topology, where the nodes represent intersections and the edges represent road segments;
	\item the geometry of every lane $L$ in the network (every edge), described by its centre-line parametric curve
	$
	s \rightarrow (p^x_{L}(s), p^y_{L}(s))\in \Real^2,
	$
	\noindent and heading $\psi_L:s \rightarrow \tan^{-1}\left({\odv{p^y_{L}}{s}(s)}/{\odv{p^x_{L}}{s}(s)}\right) \in \Real$
	(tangent to the curve) where $s\in[0, l_L]$ is the curvilinear abscissa and $l_L$ is the length of the lane $L$.
\end{enumerate}

However, we do not include these information as part of the state but rather of the system dynamics, described later. Consequently, model-free algorithms will learn policies tailored for the particular scene seen during training, and will not be able to adapt to different scenes, unless the state space is augmented to include road features. Conversely, model-based algorithms can leverage road information in their dynamics models and thus generalize to unseen scenes.

\section{Behavioural decisions}

We follow the hierarchical architecture of the Decision module discussed in \Cref{sec:nuts-and-bolts,sec:temporal-abstraction}. Since we focus on Behavioural Planning specifically, we assume the availability\footnote{These two modules are described as part of the dynamics.} of
\begin{enumerate}[label=(\roman*)]
	\item a Route Planning layer, that automatically selects the next road segment to be followed at each intersection, \eg the proper exit on a highway, or the right direction at an intersection;
	\item a Motion Planning and Control layer, that controls the vehicle by way of low-level throttle and steering actuators to reach any desired position and speed in the selected road segment. 
\end{enumerate}

Thus, the purpose of the Behavioural Planning layer is to specify short-term instructions for the Motion Planning layer, in the form of a lane to follow and a speed to adapt. The produced trajectory will always conform to the planned route, but the Behavioural Planner is in charge of \eg deciding when to merge on a highway, negotiating right of way at an intersection, overtaking vehicles, \etc. To that end, we specify the following space of \emph{meta-actions}:

\begin{equation}
\label{eq:action-space}
\gls+{cA} \eqdef \left\{ \begin{array}{c}
\text{change to the left lane},\, \text{change to the right lane}, \\
\text{drive faster},\, \text{drive slower},\, \text{maintain speed and lane}
\end{array}\right\}
\end{equation}

Meta-actions are rather slow to affect the state of the vehicle, and are thus executed at a low frequency of $\SI{1}{\hertz}$. We will consider a behavioural planning horizon of a dozen seconds (enough to \eg take/give way to a vehicle and merge in traffic), which corresponds to a dozen of decision points. We describe next how these meta-actions influence the evolution of the state $s\in\cS$.

\section{Traffic dynamics}

This section describes how the behavioural decisions influence the evolution of the perceived states, under their above definitions, through the dynamics distribution $\gls+{transition}\parentheses{s' \mid s,a}$. As explained in \Cref{chapter:1}, it is crucial that the simulated vehicles in the scene are able to \emph{react} to the actions of the ego-vehicle, so that interaction patterns can be learnt.

\subsection{Kinematics}

We represent the non-holonomic motion capabilities of every vehicle $i\in[0, N_v]$ in the scene by the Kinematic Bicycle Model \citep[see \eg][]{Polack2017}:
\begin{align}
\begin{split}
\dot{p}^x_i &= v_i\cos(\psi_i + \beta_i), \\
\dot{p}^y_i &= v_i\sin(\psi_i + \beta_i),\\
\dot{\psi}_i &= \frac{v_i}{l}\sin(\beta_i),
\end{split}
\end{align}
where $l$ is the vehicle half-length, $\dot{v}_i$ is the throttle command and $\beta_i$ is the slip angle at the centre of gravity, used as a steering command.

\subsection{Motion planning and control}

We equip the ego-vehicle --and also other vehicles $i\in[0, N_v]$ in the scene-- with a capability to execute the meta actions $\cA$. This requires the ability to follow a lane $L_i$, described by the road information mentioned above through its lateral position $p^y_{L_i}$ and heading $\psi_{L_i}$ . To that end, vehicles follow a cascade controller of lateral position and heading in the form
\begin{align}
\label{eq:heading-command}
\begin{split}
\dot{\psi}_i &= K_i^\psi\left(\psi_{L_i}+\sin^{-1}\left(\frac{\tilde{v}_{i,y}}{v_i}\right)-\psi_i\right),\\
\tilde{v}_{i,y} &= K_i^y (p^y_{L_i}-p^y_i),
\end{split}
\end{align}
where $K_i^y\in\Real$ and $K_i^\psi\in\Real$ are control gains.
Note that the corresponding steering command $\beta_i$ can be obtained from \eqref{eq:heading-command} as: $$\beta_i = \sin^{-1}\left(\frac{l}{v_i}\dot{\psi}_i\right).$$

Furthermore, the ego-vehicle needs to be able to control its speed as per the meta actions $\cA$. To that end, we use a linear longitudinal controller
\begin{equation*}
\dot{v}_0 = K_0^v(v_r - v_0),
\end{equation*}
where $v_r\in\Real$ is the reference speed, incremented by $\pm \SI[per-mode=symbol]{5}{\meter\per\second}$ by the \emph{drive faster} and \emph{drive slower} meta-actions, and $K_0^v\in\Real$ is a control gain.

\subsection{Behavioural models}

Other simulated vehicles follow simple and behavioural models from the traffic simulation literature, that dictate how they accelerate and steer on the road.

\paragraph{Longitudinal behaviour}

The acceleration command $a_i$ of a vehicle $i\in[1, N_v]$ is controlled directly by the \gls{IDM} from \citep{Treiber2000}:
\begin{align}
\begin{split}
\dot{v}_i &= a_i\left[1-\left(\frac{v}{v_i^0}\right)^\delta - \left(\frac{d^{\star}_i}{d_i}\right)^2\right], \\
\text{where }\quad d^{\star}_i &= d_i^0 + T_i v_i + \frac{v_i\Delta v_i}{2\sqrt{a_i^+ b_i}},
\end{split}
\end{align}
$v_i$ is the vehicle velocity, $d_i$ is the distance to its front vehicle.
The dynamics of vehicle $i\in[1, N_v]$ are thus parametrised by the desired velocity $v_i^0$ , the time gap $T_i$, the jam distance $d_i^0$, the maximum acceleration $a_i$ and deceleration $b_i$, and the velocity exponent $\delta$.

\paragraph{Lateral behaviour}

The discrete lane change decisions are given by the \gls{MOBIL} model from \citep{Kesting2007}.
According to this model, a vehicle $i\in[1, N_v]$ decides to change lane when

\begin{enumerate}[label=(\roman*)]
	\item it is \emph{safe} to cut-in:
		\begin{equation*}
			\tilde{a}_n \geq - b_\text{safe};
		\end{equation*}
	\item there is an \emph{incentive}, for the ego-vehicle and possibly its followers:
		\begin{equation*}
		\underbrace{\tilde{a}_c - a_c}_{\text{the vehicle}} + p\left(\underbrace{\tilde{a}_n - a_n}_{\text{new follower}} + \underbrace{\tilde{a}_o - a_o}_{\text{old follower}}\right) \geq \Delta a_\text{th};
		\end{equation*}
\end{enumerate}
where $c$ is the centre vehicle, $o$ is its old follower \emph{before} the lane change, and $n$ is its new follower \emph{after} the lane change; $a$ and $\tilde{a}$ are the predicted accelerations of the vehicles \emph{before} and \emph{after} the lane change respectively; $p$ is a politeness coefficient, $\Delta a_\text{th}$ is the acceleration gain required to trigger a lane change; and $b_\text{safe}$ is the maximum braking imposed to a vehicle during a cut-in.

A lane change decision modifies the target lane $L_i$ followed by vehicle $i$ on its current road segment. The actual trajectory planning and steering control to track this lane is then performed by the lateral controller of \eqref{eq:heading-command}.

\subsection{Route planning}

So far, we explained how both the ego-vehicle ($i=0$) and the other simulated vehicles ($i\in[1, N_v]$) behave on a multi-lanes road segment, through their Behavioural and Control layers. The Route Planning layer is finally responsible for selecting the sequence of road segments leading to a destination, sampled randomly at initialisation. To that end, the Route Planning performs a Breadth First Search in the graph description of the road network mentioned above, and returns a shortest path of road segments from initial position to destination.

In the end, we make the decision of framing the state space $\cS$ as fully observable but subjected to uncertain transition dynamics $\Ps\parentheses{s' \mid s, a}$, which are parametrised by several unobserved variables including the destinations of agents in the scenes and the parameters of their Behavioural and Control layers.

\section{Rewards}

As discussed in \Cref{sec:irl}, choosing an appropriate reward function that yields realistic optimal driving behaviour is a challenging problem, that we do not address in this thesis. In particular, we do not wish to specify every single aspect of the expected driving behaviour inside the reward function, such as keeping a safe distance to the front vehicle. Instead, we would rather only specify a reward function as simple and straightforward as possible, and focus solely on the difficulties related to safe decision-making under uncertainty, in the hope to see adequate behaviour emerge from learning. In this perspective, keeping a safe distance would be optimal not for being directly rewarded but for robustness against the uncertain behaviour of the leading vehicle, which could brake at any time.

Thus, we focus on only two features: a vehicle should
\begin{enumerate*}[label=(\roman*)]
	\item progress quickly on the road;
	\item avoid collisions.
\end{enumerate*}

Since the \gls{MDP} formalism requires rewards to be bounded, by convention we normalize them in the $[0, 1]$ range.
Note that we forbid negative rewards, since they may incentivise the agent to prefer terminating an episode early (by causing a collision) rather than risking suffering a negative return if no satisfying trajectory can be found.

Thus, unless otherwise stated, the reward function $\R$ is chosen as follows:
\begin{equation}
\label{eq:reward-function}
\gls+{reward}(s,a) = 
\begin{cases}
1 & \text{if the ego-vehicle is at full speed;}\\
0 & \text{if the ego-vehicle has collided with another vehicle;}\\
0.5 & \text{else.}
\end{cases}
\end{equation}

A more realistic reward function may include comfort terms, such as penalizing high acceleration or jerk, and lane changes manoeuvres, but we do not consider them for simplicity.

This reward function is \emph{dense}\footnote{Rewards are said to be \emph{dense} when they are a rich signal obtained at (nearly) every step of decisions, which helps quickly shaping the behaviour and guiding exploration. In contrast, \emph{sparse} rewards are obtained for only a few goal states which are seldom reached (\eg only at the exit of a maze, or the end of a board game), which makes exploration much harder and requires assigning \emph{credit} to the action(s) responsible for a win/loss}, since the maximum reward can easily be obtained from any state by accelerating, which should guide exploration to efficient driving styles. However, it is also \emph{non-convex}, since \eg the collision penalty is incurred at the locations of any two obstacles but not in-between. It is even \emph{non-smooth}, given that it its discontinuous at collision states.

\section{Implementation}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.6\linewidth]{img/he-git}\\
	\includegraphics[height=0.5cm]{img/he-buil.pdf}
	\includegraphics[height=0.5cm]{img/he-docs.pdf}
	\includegraphics[height=0.5cm]{img/he-qual.pdf}
	\includegraphics[height=0.5cm]{img/he-cov.pdf}
	\includegraphics[height=0.5cm]{img/he-contr.pdf}
	\includegraphics[height=0.5cm]{img/he-envs.pdf}
	\caption{\highwayenv repository status (on 30/06/2020).}
	\label{fig:highway-env-status}
\end{figure}

I created the \href{https://github.com/eleurent/highway-env}{\textsc{highway-env}} environment, a minimalist driving simulator tailored for behavioural planning tasks following the \gls{MDP} formalization presented in this chapter. It is written in Python and published online under an open-source license \citep{highway-env}. An extensive documentation is also available, along with instructions to reproduce every numerical experiments presented throughout this manuscript. We discuss at length the features and architecture of this software in \Cref{chapter:a}. We mention that in addition to our own works, several students and researchers already make use of this environment, as shown in \Cref{fig:highway-env-status}.

% TODO: dire que les implémentations des agents sont dans rl-agents ?

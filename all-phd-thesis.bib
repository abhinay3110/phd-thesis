@Comment{Chap1,
	title =	{Introduction},
}

@article{Foot1967,
	volume = {5},
	pages = {5--15},
	title = {The Problem of Abortion and the Doctrine of Double Effect},
	year = {1967},
	journal = {Oxford Review},
	author = {Philippa Foot}
}

@incollection{Lin2015,
	author={Lin, Patrick},
	title={Why Ethics Matters for Autonomous Cars},
	booktitle={Autonomes Fahren: Technische, rechtliche und gesellschaftliche Aspekte},
	year={2015},
	publisher={Springer Berlin Heidelberg},
	address={Berlin, Heidelberg},
	pages={69--85},
	abstract={If motor vehicles are to be truly autonomous and able to operate responsibly on our roads, they will need to replicate -- or do better than -- the human decision-making process. But some decisions are more than just a mechanical application of traffic laws and plotting a safe path. They seem to require a sense of ethics, and this is a notoriously difficult capability to reduce into algorithms for a computer to follow.},
	isbn={978-3-662-45854-9},
	doi={10.1007/978-3-662-45854-9_4},
	url={https://doi.org/10.1007/978-3-662-45854-9_4}
	}

@article{Bonnefon2016,
	title={The social dilemma of autonomous vehicles},
	volume={352},
	ISSN={1095-9203},
	url={http://dx.doi.org/10.1126/science.aaf2654},
	DOI={10.1126/science.aaf2654},
	number={6293},
	journal={Science},
	publisher={American Association for the Advancement of Science (AAAS)},
	author={Bonnefon, J.-F. and Shariff, A. and Rahwan, I.},
	year={2016},
	month={Jun},
	pages={1573–1576}
}

﻿@Article{Gogoll2017,
	author={Gogoll, Jan
	and M{\"u}ller, Julian F.},
	title={Autonomous Cars: In Favor of a Mandatory Ethics Setting},
	journal={Science and Engineering Ethics},
	year={2017},
	month={Jun},
	day={01},
	volume={23},
	number={3},
	pages={681-700},
	abstract={The recent progress in the development of autonomous cars has seen ethical questions come to the forefront. In particular, life and death decisions regarding the behavior of self-driving cars in trolley dilemma situations are attracting widespread interest in the recent debate. In this essay we want to ask whether we should implement a mandatory ethics setting (MES) for the whole of society or, whether every driver should have the choice to select his own personal ethics setting (PES). While the consensus view seems to be that people would not be willing to use an automated car that might sacrifice themselves in a dilemma situation, we will defend the somewhat contra-intuitive claim that this would be nevertheless in their best interest. The reason is, simply put, that a PES regime would most likely result in a prisoner's dilemma.},
	issn={1471-5546},
	doi={10.1007/s11948-016-9806-x},
	url={https://doi.org/10.1007/s11948-016-9806-x}
}

@article{Awad2018,
	author={Awad, Edmond
	and Dsouza, Sohan
	and Kim, Richard
	and Schulz, Jonathan
	and Henrich, Joseph
	and Shariff, Azim
	and Bonnefon, Jean-Fran{\c{c}}ois
	and Rahwan, Iyad},
	title={The Moral Machine experiment},
	journal={Nature},
	year={2018},
	month={Nov},
	day={01},
	volume={563},
	number={7729},
	pages={59-64},
	abstract={With the rapid development of artificial intelligence have come concerns about how machines will make moral decisions, and the major challenge of quantifying societal expectations about the ethical principles that should guide machine behaviour. To address this challenge, we deployed the Moral Machine, an online experimental platform designed to explore the moral dilemmas faced by autonomous vehicles. This platform gathered 40 million decisions in ten languages from millions of people in 233 countries and territories. Here we describe the results of this experiment. First, we summarize global moral preferences. Second, we document individual variations in preferences, based on respondents' demographics. Third, we report cross-cultural ethical variation, and uncover three major clusters of countries. Fourth, we show that these differences correlate with modern institutions and deep cultural traits. We discuss how these preferences can contribute to developing global, socially acceptable principles for machine ethics. All data used in this article are publicly available.},
	issn={1476-4687},
	doi={10.1038/s41586-018-0637-6},
	url={https://doi.org/10.1038/s41586-018-0637-6}
}

@inproceedings{Noothigattu2018,
  author    = {Ritesh Noothigattu and
               Snehalkumar (Neil) S. Gaikwad and
               Edmond Awad and
               Sohan Dsouza and
               Iyad Rahwan and
               Pradeep Ravikumar and
               Ariel D. Procaccia},
  editor    = {Sheila A. McIlraith and
               Kilian Q. Weinberger},
  title     = {A Voting-Based System for Ethical Decision Making},
  booktitle = {Proceedings of the Thirty-Second {AAAI} Conference on Artificial Intelligence,
               (AAAI-18)},
  month     = {February 2-7},
  address   = {New Orleans, Louisiana, USA},
  pages     = {1587--1594},
  publisher = {{AAAI} Press},
  year      = {2018},
  url       = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17052},
  timestamp = {Tue, 23 Oct 2018 06:42:15 +0200},
  biburl    = {https://dblp.org/rec/conf/aaai/NoothigattuGADR18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@misc{DeFreitas2019,
	title={Doubting Driverless Dilemmas},
	url={psyarxiv.com/a36e5},
	DOI={10.31234/osf.io/a36e5},
	publisher={PsyArXiv},
	author={De Freitas, Julian and Anthony, Sam E and Alvarez, George and Censi, Andrea},
	year={2019},
	month={Jan},
	note={preprint}
}

@misc{trolley2009,
	author = {Jesse Prinz},
	title = {Subcortex.com},
	year = 2009,
	howpublished={\url{http://subcortex.com/}},
	urldate = {2010-09-30}
}


@article{Abraham1960,
	title={Le prix d'une vie humaine dans les d{\'e}cisions {\'e}conomiques},
	author={Abraham, C. and Th{\'e}di{\'e}, J.},
	journal={Revue Française de Recherche Op{\'e}rationnelle},
	volume={16},
	pages={157-168},
	year={1960}
}

@article{Dreze1962,
	title={L'utilit{\'e} sociale d'une vie humaine},
	author={Dr{\`e}ze, Jacques},
	journal={Revue Française de Recherche Op{\'e}rationnelle},
	volume={23},
	pages={93-118},
	issn={0556-7815},
	year={1962}
}

@article{Schelling1968life,
	title = {The life you save may be your own},
	journal = {Problems in Public Expenditure.},
	pages = {127--162},
	series= {Studies of Government Finance.},
	author = {Schelling, Thomas C. and Bailey, Martin J. and Fromm, Gary},
	address = {Washington, DC},
	month = {September 15 - 16},
	publisher = {Brookings Institution},
	year = {1968},
}

@article{Banzhaf2014,
	Author = {Banzhaf, H. Spencer},
	Title = {Retrospectives: The Cold-War Origins of the Value of Statistical Life},
	Journal = {Journal of Economic Perspectives},
	Volume = {28},
	Number = {4},
	Year = {2014},
	Month = {November},
	Pages = {213-26},
	DOI = {10.1257/jep.28.4.213},
	URL = {https://www.aeaweb.org/articles?id=10.1257/jep.28.4.213}
}

@book{Tirole2017,
	ISBN = {9780691175164},
	URL = {http://www.jstor.org/stable/j.ctvc77hng},
	abstract = {From Nobel Prize-winning economist Jean Tirole, a bold new agenda for the role of economics in society When Jean Tirole won the 2014 Nobel Prize in Economics, he suddenly found himself being stopped in the street by complete strangers and asked to comment on issues of the day, no matter how distant from his own areas of research. His transformation from academic economist to public intellectual prompted him to reflect further on the role economists and their discipline play in society. The result is Economics for the Common Good, a passionate manifesto for a world in which economics, far from being a "dismal science," is a positive force for the common good. Economists are rewarded for writing technical papers in scholarly journals, not joining in public debates. But Tirole says we urgently need economists to engage with the many challenges facing society, helping to identify our key objectives and the tools needed to meet them. To show how economics can help us realize the common good, Tirole shares his insights on a broad array of questions affecting our everyday lives and the future of our society, including global warming, unemployment, the post-2008 global financial order, the euro crisis, the digital revolution, innovation, and the proper balance between the free market and regulation. Providing a rich account of how economics can benefit everyone, Economics for the Common Good sets a new agenda for the role of economics in society. },
	author = {Jean Tirole and Steven Rendall},
	publisher = {Princeton University Press},
	title = {Economics for the Common Good},
	year = {2017}
}

@article{Charpentier2019,
	title={La valeur de la vie humaine},
	author={Arthur Charpentier and Béatrice Cherrier},
	journal={Risques},
	volume={118},
	pages={107--111},
	year={2019},
	month={June}
}

@book{Buehler2009,
	author = {Buehler, Martin and Iagnemma, Karl and Singh, Sanjiv},
	title = {The DARPA Urban Challenge: Autonomous Vehicles in City Traffic},
	year = {2009},
	isbn = {3642039901},
	publisher = {Springer Publishing Company, Incorporated},
	edition = {1st}
}

@article{janai2017computer,
	url = {http://dx.doi.org/10.1561/0600000079},
	year = {2020},
	volume = {12},
	journal = {Foundations and Trends® in Computer Graphics and Vision},
	title = {Computer Vision for Autonomous Vehicles: Problems, Datasets and State of the Art},
	doi = {10.1561/0600000079},
	issn = {1572-2740},
	number = {1–3},
	pages = {1-308},
	author = {Joel Janai and Fatma Güney and Aseem Behl and Andreas Geiger}
}

@phdthesis{Polack2018,
	title = {{Consistency and stability of hierarchical planning and control systems for autonomous driving}},
	author = {Polack, Philip},
	url = {https://pastel.archives-ouvertes.fr/tel-02096788},
	number = {2018PSLEM025},
	school = {{PSL Research University}},
	year = {2018},
	month = Oct,
	keywords = {Vehicle dynamics ; Motion planning ; Autonomous driving ; Model predictive control ; Model-free control ; Longitudinal and lateral control ; Planification de trajectoire ; Conduite autonome ; Commande pr{\'e}dictive {\`a} mod{\`e}le ; Contr{\^o}le longitudinal et lat{\'e}ral ; Dynamique du v{\'e}hicule},
	type = {Theses},
	pdf = {https://pastel.archives-ouvertes.fr/tel-02096788/file/2018PSLEM025_archivage.pdf},
	hal_id = {tel-02096788},
	hal_version = {v1},
}

@inproceedings{Baker2008,
  author    = {Christopher R. Baker and
               John M. Dolan},
  title     = {Traffic interaction in the urban challenge: Putting boss on its best
               behavior},
  booktitle = {2008 {IEEE/RSJ} International Conference on Intelligent Robots and
               Systems, September 22-26, 2008},
  address   = {Acropolis Convention Center, Nice, France},
  pages     = {1752--1758},
  publisher = {{IEEE}},
  year      = {2008},
  doi       = {10.1109/IROS.2008.4651211},
  timestamp = {Wed, 16 Oct 2019 14:14:51 +0200},
  biburl    = {https://dblp.org/rec/conf/iros/BakerD08.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Trautman2010,
	abstract = {In this paper, we study the safe navigation of a mobile robot through crowds of dynamic agents with uncertain trajectories. Existing algorithms suffer from the "freezing robot" problem: once the environment surpasses a certain level of complexity, the planner decides that all forward paths are unsafe, and the robot freezes in place (or performs unnecessary maneuvers) to avoid collisions. Since a feasible path typically exists, this behavior is suboptimal. Existing approaches have focused on reducing the predictive uncertainty for individual agents by employing more informed models or heuristically limiting the predictive covariance to prevent this overcautious behavior. In this work, we demonstrate that both the individual prediction and the predictive uncertainty have little to do with the frozen robot problem. Our key insight is that dynamic agents solve the frozen robot problem by engaging in "joint collision avoidance": They cooperatively make room to create feasible trajectories. We develop IGP, a nonparametric statistical model based on dependent output Gaussian processes that can estimate crowd interaction from data. Our model naturally captures the non-Markov nature of agent trajectories, as well as their goal-driven navigation. We then show how planning in this model can be efficiently implemented using particle based inference. Lastly, we evaluate our model on a dataset of pedestrians entering and leaving a building, first comparing the model with actual pedestrians, and find that the algorithm either outperforms human pedestrians or performs very similarly to the pedestrians. We also present an experiment where a covariance reduction method results in highly overcautious behavior, while our model performs desirably. {\textcopyright}2010 IEEE.},
	author = {Trautman, Peter and Krause, Andreas},
	booktitle = {2010 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
	doi = {10.1109/IROS.2010.5654369},
	isbn = {9781424466757},
	title = {{Unfreezing the robot: Navigation in dense, interacting crowds}},
	year = {2010},
	month={18-22 Oct},
	address={Taipei, Taiwan},
	pages={797-803},
}

@Comment{Chap2_1,
	title =	{Motion Planning},
}

@article{Gonzalez2016,
	author={D. {González} and J. {Pérez} and V. {Milanés} and F. {Nashashibi}},
	journal={IEEE Transactions on Intelligent Transportation Systems}, 
	title={A Review of Motion Planning Techniques for Automated Vehicles}, 
	year={2016},
	volume={17},
	number={4},
	pages={1135-1145},
}

@ARTICLE{Paden2016,
  author={Brian Paden and
	Michal C{\'{a}}p and
	Sze Zheng Yong and
	Dmitry S. Yershov and
	Emilio Frazzoli},
  journal={IEEE Transactions on Intelligent Vehicles},
  title={A Survey of Motion Planning and Control Techniques for Self-Driving Urban Vehicles},
  year={2016},
  volume={1},
  number={1},
  pages={33-55},
}

﻿@Article{Dijkstra1959,
	author={Dijkstra, E. W.},
	title={A note on two problems in connexion with graphs},
	journal={Numerische Mathematik},
	year={1959},
	month={Dec},
	day={01},
	volume={1},
	number={1},
	pages={269-271},
	issn={0945-3245},
	doi={10.1007/BF01386390},
	url={https://doi.org/10.1007/BF01386390}
}

@INPROCEEDINGS{Fraichard1993,
	author={T. {Fraichard}},
	booktitle={Proceedings of 1993 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS '93)}, 
	title={Dynamic trajectory planning with dynamic constraints: A 'state-time space' approach}, 
	year={1993},
	volume={2},
	number={},
	month={26-30 July},
	address = {Yokohama, Japan},
	pages={1393-1400 vol.2},
}

@ARTICLE{Laumond1994,
	author={J. -. {Laumond} and P. E. {Jacobs} and M. {Taix} and R. M. {Murray}},
	journal={IEEE Transactions on Robotics and Automation}, 
	title={A motion planner for nonholonomic mobile robots}, 
	year={1994},
	volume={10},
	number={5},
	pages={577-593},
}

@article{Hart1968,
	abstract = {Although the problem of determining the minimum cost path through a graph arises naturally in a number of interesting applications, there has been no underlying theory to guide the development of efficient search procedures. Moreover, there is no adequate conceptual framework within which the various ad hoc search strategies proposed to date can be compared. This paper describes how heuristic information from the problem domain can be incorporated into a formal mathematical theory of graph searching and demonstrates an optimality property of a class of search strategies. Copyright {\textcopyright} 1968 by The Institute of Electrical and Electronics Engineers, Inc.},
	author = {Hart, Peter E. and Nilsson, Nils J. and Raphael, Bertram},
	doi = {10.1109/TSSC.1968.300136},
	issn = {21682887},
	journal = {IEEE Transactions on Systems Science and Cybernetics},
	title = {{A Formal Basis for the Heuristic Determination of Minimum Cost Paths}},
	year = {1968},
	volume={4},
	number={2},
	pages={100-107},
}

@inproceedings{Stentz1994,
	abstract = {The task of planning trajectories for a mobile robot has received considerable attention in the research literature. Most of the work assumes the robot has a complete and accurate model of its environment before it begins to move; less attention has been paid to the problem of partially known environments. This situation occurs for an exploratory robot or one that must move to a goal location without the benefit of a floorplan or terrain map. Existing approaches plan an initial path based on known information and then modify the plan locally or replan the entire path as the robot discovers obstacles with its sensors, sacrificing optimality or computational efficiency respectively. This paper introduces a new algorithm, D, capable of planning paths in unknown, partially known, and changing environments in an efficient, optimal, and complete manner.},
	author = {Stentz, Anthony},
	booktitle = {Proceedings of the 1994 IEEE International Conference on Robotics and Automation (ICRA)},
	doi = {10.1109/robot.1994.351061},
	isbn = {0818653329},
	issn = {10504729},
	title = {{Optimal and efficient path planning for partially-known environments}},
	year = {1994},
	pages={3310-3317 vol.4},
	month={8-13 May},
	address={San Diego, CA, USA}
}

@inproceedings{Ziegler2009,
	abstract = {We present a method for motion planning in the presence of moving obstacles that is aimed at dynamic on-road driving scenarios. Planning is performed within a geometric graph that is established by sampling deterministically from a manifold that is obtained by combining configuration space and time. We show that these graphs are acyclic and shortest path algorithms with linear runtime can be employed. By reparametrising the configuration space to match the course of the road, it can be sampled very economically with few vertices, and this reduces absolute runtime further. The trajectories generated are quintic splines. They are second order continuous, obey nonholonomic constraints and are optimised for minimum square of jerk. Planning time remains below 20 ms on general purpose hardware. {\textcopyright} 2009 IEEE.},
	author = {Ziegler, Julius and Stiller, Christoph},
	booktitle = {2009 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2009},
	doi = {10.1109/IROS.2009.5354448},
	isbn = {9781424438044},
	title = {{Spatiotemporal state lattices for fast trajectory planning in dynamic on-road driving scenarios}},
	year = {2009}
}

@article{Bohren2008,
	author = {Bohren, Jonathan and Foote, Tully and Keller, Jim and Kushleyev, Alex and Lee, Daniel and Stewart, Alex and Vernaza, Paul and Derenick, Jason and Spletzer, John and Satterfield, Brian},
	title = {Little Ben: The Ben Franklin Racing Team's entry in the 2007 DARPA Urban Challenge},
	journal = {Journal of Field Robotics},
	volume = {25},
	number = {9},
	pages = {598-614},
	doi = {10.1002/rob.20260},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/rob.20260},
	abstract = {Abstract This paper describes “Little Ben,” an autonomous ground vehicle constructed by the Ben Franklin Racing Team for the 2007 DARPA Urban Challenge in under a year and for less than \$250,000. The sensing, planning, navigation, and actuation systems for Little Ben were carefully designed to meet the performance demands required of an autonomous vehicle traveling in an uncertain urban environment. We incorporated an array of a global positioning system (GPS)/inertial navigation system, LIDARs, and stereo cameras to provide timely information about the surrounding environment at the appropriate ranges. This sensor information was integrated into a dynamic map that could robustly handle GPS dropouts and errors. Our planning algorithms consisted of a high-level mission planner that used information from the provided route network definition and mission data files to select routes, whereas the lower level planner used the latest dynamic map information to optimize a feasible trajectory to the next waypoint. The vehicle was actuated by a cost-based controller that efficiently handled steering, throttle, and braking maneuvers in both forward and reverse directions. Our software modules were integrated within a hierarchical architecture that allowed rapid development and testing of the system performance. The resulting vehicle was one of six to successfully finish the Urban Challenge. © 2008 Wiley Periodicals, Inc.},
	year = {2008}
}

@article{Bacha2008,
	author = {Bacha, Andrew and Bauman, Cheryl and Faruque, Ruel and Fleming, Michael and Terwelp, Chris and Reinholtz, Charles and Hong, Dennis and Wicks, Al and Alberi, Thomas and Anderson,, David and Cacciola, Stephen and Currier,, Patrick and Dalton, Aaron and Farmer, Jesse and Hurdus, Jesse and Kimmel, Shawn and King, Peter and Taylor, Andrew and Covern, David Van and Webster, Mike},
	title = {Odin: Team VictorTango's entry in the DARPA Urban Challenge},
	journal = {Journal of Field Robotics},
	volume = {25},
	number = {8},
	pages = {467-492},
	doi = {10.1002/rob.20248},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/rob.20248},
	abstract = {Abstract The DARPA Urban Challenge required robotic vehicles to travel more than 90 km through an urban environment without human intervention and included situations such as stop intersections, traffic merges, parking, and roadblocks. Team VictorTango separated the problem into three parts: base vehicle, perception, and planning. A Ford Escape outfitted with a custom drive-by-wire system and computers formed the basis for Odin. Perception used laser scanners, global positioning system, and a priori knowledge to identify obstacles, cars, and roads. Planning relied on a hybrid deliberative/reactive architecture to analyze the situation, select the appropriate behavior, and plan a safe path. All vehicle modules communicated using the JAUS (Joint Architecture for Unmanned Systems) standard. The performance of these components in the Urban Challenge is discussed and successes noted. The result of VictorTango's work was successful completion of the Urban Challenge and a third-place finish. © 2008 Wiley Periodicals, Inc.},
	year = {2008}
}

@article{Montemerlo2008,
	author = {Montemerlo, Michael and Becker, Jan and Bhat, Suhrid and Dahlkamp, Hendrik and Dolgov, Dmitri and Ettinger, Scott and Haehnel, Dirk and Hilden, Tim and Hoffmann, Gabe and Huhnke, Burkhard and Johnston, Doug and Klumpp, Stefan and Langer, Dirk and Levandowski, Anthony and Levinson, Jesse and Marcil, Julien and Orenstein, David and Paefgen, Johannes and Penny, Isaac and Petrovskaya, Anna and Pflueger, Mike and Stanek, Ganymed and Stavens, David and Vogt, Antone and Thrun, Sebastian},
	title = {Junior: The Stanford entry in the Urban Challenge},
	journal = {Journal of Field Robotics},
	volume = {25},
	number = {9},
	pages = {569-597},
	doi = {10.1002/rob.20258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.20258},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/rob.20258},
	abstract = {Abstract This article presents the architecture of Junior, a robotic vehicle capable of navigating urban environments autonomously. In doing so, the vehicle is able to select its own routes, perceive and interact with other traffic, and execute various urban driving skills including lane changes, U-turns, parking, and merging into moving traffic. The vehicle successfully finished and won second place in the DARPA Urban Challenge, a robot competition organized by the U.S. Government. © 2008 Wiley Periodicals, Inc.},
	year = {2008}
}

@article{Kammel2008,
	author = {Kammel, Sören and Ziegler, Julius and Pitzer, Benjamin and Werling, Moritz and Gindele, Tobias and Jagzent, Daniel and Schröder, Joachim and Thuy, Michael and Goebl, Matthias and Hundelshausen, Felix von and Pink, Oliver and Frese, Christian and Stiller, Christoph},
	title = {Team AnnieWAY's autonomous system for the 2007 DARPA Urban Challenge},
	journal = {Journal of Field Robotics},
	volume = {25},
	number = {9},
	pages = {615-639},
	doi = {10.1002/rob.20252},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.20252},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/rob.20252},
	abstract = {Abstract This paper reports on AnnieWAY, an autonomous vehicle that is capable of driving through urban scenarios and that successfully entered the finals of the 2007 DARPA Urban Challenge competition. After describing the main challenges imposed and the major hardware components, we outline the underlying software structure and focus on selected algorithms. Environmental perception mainly relies on a recent laser scanner that delivers both range and reflectivity measurements. Whereas range measurements are used to provide three-dimensional scene geometry, measuring reflectivity allows for robust lane marker detection. Mission and maneuver planning is conducted using a hierarchical state machine that generates behavior in accordance with California traffic laws. We conclude with a report of the results achieved during the competition. © 2008 Wiley Periodicals, Inc.},
	year = {2008}
}

@article{Urmson2008,
	author = {Urmson, Chris and Anhalt, Joshua and Bagnell, Drew and Baker, Christopher and Bittner, Robert and Clark, M. N. and Dolan, John and Duggins, Dave and Galatali, Tugrul and Geyer, Chris and Gittleman, Michele and Harbaugh, Sam and Hebert, Martial and Howard, Thomas M. and Kolski, Sascha and Kelly, Alonzo and Likhachev, Maxim and McNaughton, Matt and Miller, Nick and Peterson, Kevin and Pilnick, Brian and Rajkumar, Raj and Rybski, Paul and Salesky, Bryan and Seo, Young-Woo and Singh, Sanjiv and Snider, Jarrod and Stentz, Anthony and Whittaker, William “Red” and Wolkowicki, Ziv and Ziglar, Jason and Bae, Hong and Brown, Thomas and Demitrish, Daniel and Litkouhi, Bakhtiar and Nickolaou, Jim and Sadekar, Varsha and Zhang, Wende and Struble, Joshua and Taylor, Michael and Darms, Michael and Ferguson, Dave},
	title = {Autonomous driving in urban environments: Boss and the Urban Challenge},
	journal = {Journal of Field Robotics},
	volume = {25},
	number = {8},
	pages = {425-466},
	doi = {10.1002/rob.20255},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.20255},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/rob.20255},
	abstract = {Abstract Boss is an autonomous vehicle that uses on-board sensors (global positioning system, lasers, radars, and cameras) to track other vehicles, detect static obstacles, and localize itself relative to a road model. A three-layer planning system combines mission, behavioral, and motion planning to drive in urban environments. The mission planning layer considers which street to take to achieve a mission goal. The behavioral layer determines when to change lanes and precedence at intersections and performs error recovery maneuvers. The motion planning layer selects actions to avoid obstacles while making progress toward local goals. The system was developed from the ground up to address the requirements of the DARPA Urban Challenge using a spiral system development process with a heavy emphasis on regular, regressive system testing. During the National Qualification Event and the 85-km Urban Challenge Final Event, Boss demonstrated some of its capabilities, qualifying first and winning the challenge. © 2008 Wiley Periodicals, Inc.},
	year = {2008}
}

@techreport{Lavalle98,
	author = {Steven M. Lavalle},
	title = {Rapidly-Exploring Random Trees: A New Tool for Path Planning},
	institution = {Computer Science Department, Iowa State University},
	year = {1998}
}

@article{Karaman2011,
	author = {Sertac Karaman and Emilio Frazzoli},
	title ={Sampling-based algorithms for optimal motion planning},
	journal = {The International Journal of Robotics Research},
	volume = {30},
	number = {7},
	pages = {846-894},
	year = {2011},
	doi = {10.1177/0278364911406761},
}

@article{Kavraki1996,
	abstract = {A new motion planning method for robots in static workspaces is presented. This method proceeds in two phases: a learning phase and a query phase. In the learning phase, a probabilistic roadmap is constructed and stored as a graph whose nodes correspond to collision-free configurations and whose edges correspond to feasible paths between these configurations. These paths are computed using a simple and fast local planner. In the query phase, any given start and goal configurations of the robot are connected to two nodes of the roadmap; the roadmap is then searched for a path joining these two nodes. The method is general and easy to implement. It can be applied to virtually any type of holonomic robot. It requires selecting certain parameters (e.g., the duration of the learning phase) whose values depend on the scene, that is the robot and its workspace. But these values turn out to be relatively easy to choose. Increased efficiency can also be achieved by tailoring some components of the method (e.g., the local planner) to the considered robots. In this paper the method is applied to planar articulated robots with many degrees of freedom. Experimental results show that path planning can be done in a fraction of a second on a contemporary workstation (≈ 150 MIPS), after learning for relatively short periods of time (a few dozen seconds). {\textcopyright} 1996 IEEE.},
	author = {Kavraki, Lydia E. and {\v{S}}vestka, Petr and Latombe, Jean Claude and Overmars, Mark H.},
	doi = {10.1109/70.508439},
	issn = {1042296X},
	journal = {IEEE Transactions on Robotics and Automation},
	title = {{Probabilistic roadmaps for path planning in high-dimensional configuration spaces}},
	year = {1996},
	volume={12},
	number={4},
	pages={566-580}
}

@inproceedings{Faust2018,
	abstract = {We present PRM-RL, a hierarchical method for long-range navigation task completion that combines sampling-based path planning with reinforcement learning (RL). The RL agents learn short-range, point-to-point navigation policies that capture robot dynamics and task constraints without knowledge of the large-scale topology. Next, the sampling-based planners provide roadmaps which connect robot configurations that can be successfully navigated by the RL agent. The same RL agents are used to control the robot under the direction of the planning, enabling long-range navigation. We use the Probabilistic Roadmaps (PRMs) for the sampling-based planner. The RL agents are constructed using feature-based and deep neural net policies in continuous state and action spaces. We evaluate PRM-RL, both in simulation and on-robot, on two navigation tasks with non-trivial robot dynamics: end-to-end differential drive indoor navigation in office environments, and aerial cargo delivery in urban environments with load displacement constraints. Our results show improvement in task completion over both RL agents on their own and traditional sampling-based planners. In the indoor navigation task, PRM-RL successfully completes up to 215 m long trajectories under noisy sensor conditions, and the aerial cargo delivery completes flights over 1000 m without violating the task constraints in an environment 63 million times larger than used in training.},
	archivePrefix = {arXiv},
	arxivId = {1710.03937},
	author = {Faust, Aleksandra and Oslund, Kenneth and Ramirez, Oscar and Francis, Anthony and Tapia, Lydia and Fiser, Marek and Davidson, James},
	booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
	doi = {10.1109/ICRA.2018.8461096},
	eprint = {1710.03937},
	isbn = {9781538630815},
	issn = {10504729},
	title = {{PRM-RL: Long-range robotic navigation tasks by combining reinforcement learning and sampling-based planning}},
	month = {21-25 May},
	address = {Brisbane, QLD, Australia},
    pages={5113-5120},
	year = {2018}
}

@inproceedings{Lenz2016,
	abstract = {Human drivers use nonverbal communication and anticipation of other drivers' actions to master conflicts occurring in everyday driving situations. Without a high penetration of vehicle-to-vehicle communication an autonomous vehicle has to have the possibility to understand intentions of others and share own intentions with the surrounding traffic participants. This paper proposes a cooperative combinatorial motion planning algorithm without the need for inter vehicle communication based on Monte Carlo Tree Search (MCTS). We motivate why MCTS is particularly suited for the autonomous driving domain. Furthermore, adoptions to the MCTS algorithm are presented as for example simultaneous decisions, the usage of the Intelligent Driver Model as microscopic traffic simulation, and a cooperative cost function. We further show simulation results of merging scenarios in highway-like situations to underline the cooperative nature of the approach.},
	author = {Lenz, David and Kessler, Tobias and Knoll, Alois},
	booktitle = {2016 IEEE Intelligent Vehicles Symposium (IV)},
	doi = {10.1109/IVS.2016.7535424},
	month = {19-22 June},
	isbn = {9781509018215},
	title = {{Tactical cooperative planning for autonomous highway driving using Monte-Carlo Tree Search}},
	address = {Gothenburg, Sweden},
	year = {2016},
	pages={447-453},
}

@inproceedings{Latombe1991,
	author = {Latombe, Jean-Claude},
	title = {A Fast Path Planner for a Car-like Indoor Mobile Robot},
	year = {1991},
	isbn = {0262510596},
	publisher = {AAAI Press},
	booktitle = {Proceedings of the Ninth National Conference on Artificial Intelligence - Volume 2},
	pages = {659–665},
	numpages = {7},
	address = {Anaheim, California},
	series = {AAAI’91},
	month = jul
}

@InProceedings{Sanchez2002,
	author="S{\'a}nchez L., Abraham
	and Zapata, Ren{\'e}
	and Arenas B., J. Abraham",
	editor="Coello Coello, Carlos A.
	and de Albornoz, Alvaro
	and Sucar, Luis Enrique
	and Battistutti, Osvaldo Cair{\'o}",
	title="Motion Planning for Car-Like Robots Using Lazy Probabilistic Roadmap Method",
	booktitle="MICAI 2002: Advances in Artificial Intelligence",
	year="2002",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="1--10",
	abstract="In this paper we describe an approach to probabilistic roadmap method. Our algorithm builds initially a roadmap in the configuration space considering that all nodes and edges are collision-free, and searches the roadmap for the shortest path between start and goal nodes. If a collision with the obstacles occurs, the corresponding nodes and edges are removed from the roadmap or the planner updates the roadmap with new nodes and edges, and then searches for a shortest path. The procedure is repeated until a collision-free path is found. The goal of our approach is to minimize the number of collision checks and calls to the local method. Experimental results presented in this paper show that our approach is very efficient in practice.",
	isbn="978-3-540-46016-9"
}

@ARTICLE{Lamiraux2001,	
	author={F. {Lamiraux} and J. -. {Lammond}},
	journal={IEEE Transactions on Robotics and Automation}, 
	title={Smooth motion planning for car-like vehicles}, 
	year={2001},
	volume={17},
	number={4},
	pages={498-501},
}

@article{Reeds1990,
	author = "Reeds, J. A. and Shepp, L. A.",
	fjournal = "Pacific Journal of Mathematics",
	journal = "Pacific J. Math.",
	number = "2",
	pages = "367--393",
	publisher = "Pacific Journal of Mathematics, A Non-profit Corporation",
	title = "Optimal paths for a car that goes both forwards and backwards.",
	url = "https://projecteuclid.org:443/euclid.pjm/1102645450",
	volume = "145",
	year = "1990"
}

@inproceedings{Xu2012,
	abstract = {In this paper, an efficient real-time autonomous driving motion planner with trajectory optimization is proposed. The planner first discretizes the plan space and searches for the best trajectory based on a set of cost functions. Then an iterative optimization is applied to both the path and speed of the resultant trajectory. The post-optimization is of low computational complexity and is able to converge to a higher-quality solution within a few iterations. Compared with the planner without optimization, this framework can reduce the planning time by 52{\%} and improve the trajectory quality. The proposed motion planner is implemented and tested both in simulation and on a real autonomous vehicle in three different scenarios. Experiments show that the planner outputs high-quality trajectories and performs intelligent driving behaviors. {\textcopyright} 2012 IEEE.},
	author = {Xu, Wenda and Wei, Junqing and Dolan, John M. and Zhao, Huijing and Zha, Hongbin},
  	booktitle={2012 IEEE International Conference on Robotics and Automation (ICRA)},
	doi = {10.1109/ICRA.2012.6225063},
	isbn = {9781467314039},
	issn = {10504729},
	title = {{A real-time motion planner with trajectory optimization for autonomous vehicles}},
	year = {2012},
	month = {14-18 May},
	address = {Saint Paul, MN, USA},
	pages={2061-2067},
}

@inproceedings{Gonzalez2014,
	abstract = {This paper presents a continuous curvature planning algorithm with obstacle avoidance capabilities. The automated system generates a collision free path that considers vehicle's constraints, the road and different obstacles inside the horizon of view. The developed planning module was integrated in the RITS (former IMARA) autonomous vehicle architecture. The goal of this module is to obtain an accurate, continuous and safe path generation, by implementing parametric curves. To this end, a continuous curvature profile when calculating vehicle trajectory is introduced. It also permits to generate different speed profiles, improving the comfort by reducing lateral accelerations in the driving process. These algorithms have been implemented in simulated -ProSiVIC- and real platforms -Cybercars- showing good results in both cases. This approach is currently being implemented in the framework of the EU CityMobil2 project.},
	author = {Gonz{\'{a}}lez, David and P{\'{e}}rez, Joshue and Lattarulo, Ray and Milan{\'{e}}s, Vicente and Nashashibi, Fawzi},
	booktitle = {2014 17th IEEE International Conference on Intelligent Transportation Systems, ITSC 2014},
	doi = {10.1109/ITSC.2014.6957887},
	isbn = {9781479960781},
	title = {{Continuous curvature planning with obstacle avoidance capabilities in urban scenarios}},
	year = {2014}
}

@inproceedings{Funke2012,
	abstract = {This paper presents a novel approach to autonomous driving at the vehicle's handling limits. Such a system requires a high speed, consistent control signal as well as numerous safety features capable of monitoring and stopping the vehicle. When operating, the system's high level controller utilizes a highly accurate differential GPS and known friction values to drive a precomputed path at the friction limits of the vehicle. The system was tested in a variety of road conditions, including the challenging Pikes Peak Hill climb. Results from this work can be extended to improve driving safety and accident avoidance in vehicles. {\textcopyright} 2012 IEEE.},
	author = {Funke, Joseph and Theodosis, Paul and Hindiyeh, Rami and Stanek, Ganymed and Kritatakirana, Krisada and Gerdes, Chris and Langer, Dirk and Hernandez, Marcial and M{\"{u}}ller-Bessler, Bernhard and Huhnke, Burkhard},
	booktitle = {2012 IEEE Intelligent Vehicles Symposium},
	doi = {10.1109/IVS.2012.6232212},
	isbn = {9781467321198},
	title = {{Up to the limits: Autonomous Audi TTS}},
	year = {2012},
	month = {3-7 June},
	address = {Alcala de Henares, Spain},
	pages={541-547},
}

@INPROCEEDINGS{Pivtoraiko2005,
       author = {{Pivtoraiko}, M. and {Kelly}, A.},
        title = {Efficient Constrained Path Planning via Search in State Lattices},
    booktitle = {The 8th International Symposium on Artificial Intelligence, Robotics and Automation in Space (i-SAIRAS 2005)},
         year = {2005},
       series = {ESA Special Publication},
       volume = {603},
        month = {5-8 September},
      address = {Munich, Germany},
          eid = {33},
        pages = {33},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2005ESASP.603E..33P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{LaValle1998,
	author={S. M. {LaValle} and S. A. {Hutchinson}},
	journal={IEEE Transactions on Robotics and Automation}, 
	title={Optimal motion planning for multiple robots having independent goals}, 
	year={1998},
	volume={14},
	number={6},
	pages={912-925},
}


@INPROCEEDINGS{Svestka1995,
	author={P. {Svestka} and M. H. {Overmars}},
	booktitle={Proceedings of 1995 IEEE International Conference on Robotics and Automation}, 
	title={Coordinated motion planning for multiple car-like robots using probabilistic roadmaps}, 
	year={1995},
	volume={2},
	number={},
	pages={1631-1636 vol.2},
}

@inproceedings{Altche2016,
  author    = {Florent Altch{\'{e}} and
               Xiangjun Qian and
               Arnaud de La Fortelle},
  title     = {Time-optimal coordination of mobile robots along specified paths},
  booktitle = {2016 {IEEE/RSJ} International Conference on Intelligent Robots and
               Systems, {IROS} 2016, October 9-14, 2016},
  address  = {Daejeon, South Korea},
  pages     = {5020--5026},
  publisher = {{IEEE}},
  year      = {2016},
  url       = {https://doi.org/10.1109/IROS.2016.7759737},
  doi       = {10.1109/IROS.2016.7759737},
  timestamp = {Wed, 16 Oct 2019 14:14:51 +0200},
  biburl    = {https://dblp.org/rec/conf/iros/AltcheQF16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Altche2016b,
  author    = {Florent Altch{\'{e}} and
               Arnaud de La Fortelle},
  title     = {Analysis of optimal solutions to robot coordination problems to improve
               autonomous intersection management policies},
  booktitle = {2016 {IEEE} Intelligent Vehicles Symposium, {IV} 2016, June 19-22},
  address  = {Gotenburg, Sweden},
  pages     = {86--91},
  publisher = {{IEEE}},
  year      = {2016},
  doi       = {10.1109/IVS.2016.7535369},
  timestamp = {Wed, 16 Oct 2019 14:14:57 +0200},
  biburl    = {https://dblp.org/rec/conf/ivs/AltcheF16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{Altche2017,
  author={Altch{\'{e}}, Florent and Qian, Xiangjun and {de La Fortelle}, Arnaud},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={An Algorithm for Supervised Driving of Cooperative Semi-Autonomous Vehicles}, 
  year={2017},
  volume={18},
  number={12},
  pages={3527-3539},
}


@Comment{Chap2_1_3,
	title = {Imitation Learning},
}

@incollection{Pomerleau1989,
	title = {ALVINN: An Autonomous Land Vehicle in a Neural Network},
	author = {Pomerleau, Dean A.},
	booktitle = {Advances in Neural Information Processing Systems 1},
	editor = {D. S. Touretzky},
	pages = {305--313},
	year = {1989},
	publisher = {Morgan-Kaufmann},
	address = {Denver, Colorado, USA},
	month = dec,
	url = {http://papers.nips.cc/paper/95-alvinn-an-autonomous-land-vehicle-in-a-neural-network.pdf}
}

@article{Levine2016,
  author  = {Sergey Levine and Chelsea Finn and Trevor Darrell and Pieter Abbeel},
  title   = {End-to-End Training of Deep Visuomotor Policies},
  journal = {Journal of Machine Learning Research},
  year    = {2016},
  volume  = {17},
  number  = {39},
  pages   = {1-40},
  url     = {http://jmlr.org/papers/v17/15-522.html}
}

@inproceedings{Ross2011,
	abstract = {Sequential prediction problems such as imitation learning, where future observations depend on previous predictions (actions), violate the common i.i.d. assumptions made in statistical learning. This leads to poor performance in theory and often in practice. Some recent approaches (Daum{\'{e}} III et al., 2009; Ross and Bagnell, 2010) provide stronger guarantees in this setting, but remain somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations. In this paper, we propose a new iterative algorithm, which trains a stationary deterministic policy, that can be seen as a no regret algorithm in an online learning setting. We show that any such no regret algorithm, combined with additional reduction assumptions, must find a policy with good performance under the distribution of observations it induces in such sequential settings. We demonstrate that this new approach outperforms previous approaches on two challenging imitation learning problems and a benchmark sequence labeling problem. Copyright 2011 by the authors.},
	author = {Ross, St{\'{e}}phane and Gordon, Geoffrey J. and Bagnell, J. Andrew},
	booktitle = {Journal of Machine Learning Research},
	issn = {15324435},
	title = {{A reduction of imitation learning and structured prediction to no-regret online learning}},
	year = {2011}
}

@misc{Bojarski2016,
    title={End to End Learning for Self-Driving Cars},
    author={Mariusz Bojarski and Davide Del Testa and Daniel Dworakowski and Bernhard Firner and Beat Flepp and Prasoon Goyal and Lawrence D. Jackel and Mathew Monfort and Urs Muller and Jiakai Zhang and Xin Zhang and Jake Zhao and Karol Zieba},
    year={2016},
    eprint={1604.07316},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    note={preprint}
}

@inproceedings{Xu2017,
	abstract = {Robust perception-action models should be learned from training data with diverse visual appearances and realistic behaviors, yet current approaches to deep visuomotor policy learning have been generally limited to in-situ models learned from a single vehicle or simulation environment. We advocate learning a generic vehicle motion model from large scale crowd-sourced video data, and develop an endto-end trainable architecture for learning to predict a distribution over future vehicle egomotion from instantaneous monocular camera observations and previous vehicle state. Our model incorporates a novel FCN-LSTM architecture, which can be learned from large-scale crowd-sourced vehicle action data, and leverages available scene segmentation side tasks to improve performance under a privileged learning paradigm. We provide a novel large-scale dataset of crowd-sourced driving behavior suitable for training our model, and report results predicting the driver action on held out sequences across diverse conditions.},
	archivePrefix = {arXiv},
	arxivId = {1612.01079},
	author = {Xu, Huazhe and Gao, Yang and Yu, Fisher and Darrell, Trevor},
	booktitle = {30th IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	doi = {10.1109/CVPR.2017.376},
	eprint = {1612.01079},
	isbn = {9781538604571},
	title = {{End-to-end learning of driving models from large-scale video datasets}},
	year = {2017},
	pages = {3530-3538},
	month = {21-26 July},
	address = {Honolulu, HI, USA}
}

@inproceedings{Eraqi2017,
	title={End-to-End Deep Learning for Steering Autonomous Vehicles Considering Temporal Dependencies},
	author={Hesham M. Eraqi and Mohamed N. Moustafa and Jens Honer},
	booktitle={Machine Learning for Intelligent Transportation Systems Workshop in the 31st Conference on Neural Information Processing Systems (NIPS), December},
	month=dec,
	address = {Montreal, Canada},
	eprint={1710.03804},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	year={2017},
}

@incollection{Ho2016,
	title = {Generative Adversarial Imitation Learning},
	author = {Ho, Jonathan and Ermon, Stefano},
	booktitle = {Advances in Neural Information Processing Systems 29},
	editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
	pages = {4565--4573},
	year = {2016},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/6391-generative-adversarial-imitation-learning.pdf}
}

@inproceedings{Kuefler2017,
	author={A. {Kuefler} and J. {Morton} and T. {Wheeler} and M. {Kochenderfer}},
	booktitle={2017 IEEE Intelligent Vehicles Symposium (IV)},
	title={Imitating driver behavior with generative adversarial networks},
	month={11--14 June},
	address={Los Angeles, CA, USA},
	year={2017},
	pages={204-211},
}

@inproceedings{Bhattacharyya2018,
  author    = {Raunak P. Bhattacharyya and
               Derek J. Phillips and
               Blake Wulfe and
               Jeremy Morton and
               Alex Kuefler and
               Mykel J. Kochenderfer},
  title     = {Multi-Agent Imitation Learning for Driving Simulation},
  booktitle = {2018 {IEEE/RSJ} International Conference on Intelligent Robots and
               Systems, {IROS} 2018, October 1-5, 2018},
  address   = {Madrid, Spain},
  pages     = {1534--1539},
  publisher = {{IEEE}},
  year      = {2018},
  doi       = {10.1109/IROS.2018.8593758},
  timestamp = {Wed, 16 Oct 2019 14:14:51 +0200},
  biburl    = {https://dblp.org/rec/conf/iros/BhattacharyyaPW18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Codevilla2018,
	abstract = {Deep networks trained on demonstrations of human driving have learned to follow roads and avoid obstacles. However, driving policies trained via imitation learning cannot be controlled at test time. A vehicle trained end-to-end to imitate an expert cannot be guided to take a specific turn at an upcoming intersection. This limits the utility of such systems. We propose to condition imitation learning on high-level command input. At test time, the learned driving policy functions as a chauffeur that handles sensorimotor coordination but continues to respond to navigational commands. We evaluate different architectures for conditional imitation learning in vision-based driving. We conduct experiments in realistic three-dimensional simulations of urban driving and on a 1/5 scale robotic truck that is trained to drive in a residential area. Both systems drive based on visual input yet remain responsive to high-level navigational commands.},
	archivePrefix = {arXiv},
	arxivId = {1710.02410},
	author = {Codevilla, Felipe and Miiller, Matthias and Lopez, Antonio and Koltun, Vladlen and Dosovitskiy, Alexey},
	booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA), 21-25 May},
	month = may,
	address = {Brisbane, QLD, Australia},
	doi = {10.1109/ICRA.2018.8460487},
	eprint = {1710.02410},
	isbn = {9781538630815},
	issn = {10504729},
	title = {{End-to-End Driving Via Conditional Imitation Learning}},
	year = {2018},
    pages={4693-4700},
}

@inproceedings{Rhinehart2019,
	abstract = {For autonomous vehicles (AVs) to behave appropriately on roads populated by human-driven vehicles, they must be able to reason about the uncertain intentions and decisions of other drivers from rich perceptual information. Towards these capabilities, we present a probabilistic forecasting model of future interactions between a variable number of agents. We perform both standard forecasting and the novel task of conditional forecasting, which reasons about how all agents will likely respond to the goal of a controlled agent (here, the AV). We train models on real and simulated data to forecast vehicle trajectories given past positions and LIDAR. Our evaluation shows that our model is substantially more accurate in multi-agent driving scenarios compared to existing state-of-the-art. Beyond its general ability to perform conditional forecasting queries, we show that our model's predictions of all agents improve when conditioned on knowledge of the AV's goal, further illustrating its capability to model agent interactions.},
	author = {Rhinehart, Nicholas and McAllister, Rowan and Kitani, Kris and Levine, Sergey},
	booktitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
	doi = {10.1109/ICCV.2019.00291},
	month = {27 Oct.-2 Nov.},
	address = {Seoul, Korea (South)},
	isbn = {9781728148038},
	issn = {15505499},
	title = {{PRECOG: Prediction conditioned on goals in visual multi-agent settings}},
	year = {2019},
	pages={2821-2830},
}

@inproceedings{Rhinehart2020,
	title={Deep Imitative Models for Flexible Inference, Planning, and Control},
	author={Nicholas Rhinehart and Rowan McAllister and Sergey Levine},
	year={2020},
	booktitle={International Conference on Learning Representations (ICLR)},
	address={Virtual Conference, formerly Addis Ababa, Ethiopa},
	month = may,
}

@misc{Bansal2018,
	title={ChauffeurNet: Learning to Drive by Imitating the Best and Synthesizing the Worst},
	author={Mayank Bansal and Alex Krizhevsky and Abhijit Ogale},
	year={2018},
	eprint={1812.03079},
	archivePrefix={arXiv},
	primaryClass={cs.RO},
	note={preprint}
}

@Comment{Chap2_2,
	title =	{Partial Observability},
}

@misc{ObjCode2017,
	author = {{Editions Nationales du Permis de Conduire}},
	title = {{Objectif Code !}},
	year = {2017}
}

@article{Astrom1965,
	author       = {Åström, Karl Johan},
	issn         = {0022-247X},
	language     = {eng},
	pages        = {174--205},
	publisher    = {Elsevier},
	journal      = {Journal of Mathematical Analysis and Applications},
	title        = {Optimal Control of Markov Processes with Incomplete State Information I},
	url          = {https://lup.lub.lu.se/search/ws/files/5323668/8867085.pdf},
	doi          = {10.1016/0022-247X(65)90154-X},
	volume       = {10},
	number       = {1},
	year         = {1965},
}

@inproceedings{Ulbrich2013,
	abstract = {The Stadtpilot project aims at fully automated driving on Braunschweig's inner city ring road. The TU Braunschweig's research vehicle 'Leonie' is one of the first vehicles having the ability of fully automated driving in real urban traffic scenarios. This paper shows our decision making approach for performing lane changes while driving fully automated in urban environments. We apply an online Partially Observable Markov Decision Process (POMDP) to accommodate inevitable sensor noise to be faced in urban traffic scenarios. In this paper we propose a two step algorithm to keep the complexity of the POMDP low enough for real-time decision making while driving. The presented approach has been integrated in our vehicle and was evaluated in real urban traffic. {\textcopyright} 2013 IEEE.},
	author = {Ulbrich, Simon and Maurer, Markus},
	booktitle = {16th International IEEE Conference on Intelligent Transportation Systems (ITSC 2013)},
	doi = {10.1109/ITSC.2013.6728533},
	isbn = {9781479929146},
	title = {{Probabilistic online POMDP decision making for lane changes in fully automated driving}},
	year = {2013},
	pages = {2063-2067},
	month = {6-9 Oct},
	address = {The Hague, Netherlands}
}

@InProceedings{Du2010,
	abstract = {Motion planning in uncertain and dynamic environments is critical for reliable operation of autonomous robots. Partially observable Markov decision processes (POMDPs) provide a principled general framework for such planning tasks and have been successfully applied to several moderately complex robotic tasks, including navigation, manipulation, and target tracking. The challenge now is to scale up POMDP planning algorithms and handle more complex, realistic tasks. This paper outlines ideas aimed at overcoming two major obstacles to the efficiency of POMDP planning: the “curse of dimensionality” and the “curse of history”. Our main objective is to show that using these ideas along with others POMDP algorithms can be used successfully for motion planning under uncertainty for robotic tasks with a large number of states or a long time horizon. We implemented some of our algorithms as a software package Ap- proximate POMDP Planning Library (APPL), now available for download at http://motion.comp.nus.edu.sg/ projects/pomdp/pomdp.html. Introduction},
	author = {Du, Yanzhu and Hsu, David and Kurniawati, Hanna and Lee, Wee and Ong, Sylvie and Png, Shao},
	booktitle = {Workshop on Solving Real-World POMDP Problems at the Conference on Automated Planning and Scheduling},
	month=may,
	address={Toronto, Canada},
	title = {{A POMDP Approach to Robot Motion Planning Under Uncertainty}},
	year = {2010}
}

@InProceedings{Brechtel2013,
  title = 	 {Solving Continuous POMDPs: Value Iteration with Incremental Learning of an Efficient Space Representation},
  author = 	 {Sebastian Brechtel and Tobias Gindele and Rüdiger Dillmann},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {370--378},
  year = 	 {2013},
  editor = 	 {Sanjoy Dasgupta and David McAllester},
  volume = 	 {28},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/brechtel13.pdf},
  url = 	 {http://proceedings.mlr.press/v28/brechtel13.html},
  abstract = 	 {Discrete POMDPs of medium complexity can be approximately solved in reasonable time. However, most applications have a continuous and thus uncountably infinite state space. We propose the novel concept of learning a discrete representation of the continuous state space to solve the integrals in continuous POMDPs efficiently and generalize sparse calculations over the continuous space. The representation is iteratively refined as part of a novel Value Iteration step and does not depend on prior knowledge. Consistency for the learned generalization is asserted by a self-correction algorithm. The presented concept is implemented for continuous state and observation spaces based on Monte Carlo approximation to allow for arbitrary POMDP models. In an experimental comparison it yields higher values in significantly shorter time than state of the art algorithms and solves higher-dimensional problems.}
}


@inproceedings{Bouton2018,
	author={M. {Bouton} and A. {Nakhaei} and K. {Fujimura} and M. J. {Kochenderfer}},
	booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA), 21-25 May 2018},
	month = may,
	address = {Brisbane, QLD, Australia},
	title={Scalable Decision Making with Sensor Occlusions for Autonomous Driving}, 
	year={2018},
	volume={},
	number={},
	pages={2076-2081},
}

@article{VanDenBerg2011,
	author = {{van den Berg}, Jur and Abbee, Pieter and Goldberg, Ken},
	title ={LQG-MP: Optimized path planning for robots with motion uncertainty and imperfect state information},
	journal = {The International Journal of Robotics Research},
	volume = {30},
	number = {7},
	pages = {895-913},
	year = {2011},
	doi = {10.1177/0278364911406562},
}

@inproceedings{Brechtel2014,
	abstract = {This paper presents a generic approach for tactical decision-making under uncertainty in the context of driving. The complexity of this task mainly stems from the fact that rational decision-making in this context must consider several sources of uncertainty: The temporal evolution of situations cannot be predicted without uncertainty because other road users behave stochastically and their goals and plans cannot be measured. Even more important, road users are only able to perceive a tiny part of the current situation with their sensors because measurements are noisy and most of the environment is occluded. In order to anticipate the consequences of decisions a probabilistic approach, considering both forms of uncertainty, is necessary. We address this by formulating the task of driving as a continuous Partially Observable Markov Decision Process (POMDP) that can be automatically optimized for different scenarios. As driving is a continuous-space problem, the belief space is infinite-dimensional. We do not use a symbolic representation or discretize the state space a priori because there is no representation of the state space that is optimal for every situation. Instead, we employ a continuous POMDP solver that learns a good representation of the specific situation.},
	author = {Brechtel, Sebastian and Gindele, Tobias and Dillmann, Rudiger},
	booktitle = {17th International IEEE Conference on Intelligent Transportation Systems (ITSC), 8-11 Oct},
	month = oct,
	address = {Qingdao, China},
	doi = {10.1109/ITSC.2014.6957722},
	isbn = {9781479960781},
	title = {{Probabilistic decision-making under uncertainty for autonomous driving using continuous POMDPs}},
	year = {2014},
    pages={392-399}
}

@InProceedings{Bandyopadhyay2013,
author="Bandyopadhyay, Tirthankar
and Won, Kok Sung
and Frazzoli, Emilio
and Hsu, David
and Lee, Wee Sun
and Rus, Daniela",
editor="Frazzoli, Emilio
and Lozano-Perez, Tomas
and Roy, Nicholas
and Rus, Daniela",
title="Intention-Aware Motion Planning",
booktitle="Algorithmic Foundations of Robotics X",
year="2013",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="475--491",
abstract="As robots venture into new application domains as autonomous vehicles on the road or as domestic helpers at home, they must recognize human intentions and behaviors in order to operate effectively. This paper investigates a new class of motion planning problems with uncertainty in human intention. We propose a method for constructing a practical model by assuming a finite set of unknown intentions. We first construct a motion model for each intention in the set and then combine these models together into a single Mixed Observability Markov Decision Process (MOMDP), which is a structured variant of the more common Partially Observable Markov Decision Process (POMDP). By leveraging the latest advances in POMDP/MOMDP approximation algorithms, we can construct and solve moderately complex models for interesting robotic tasks. Experiments in simulation and with an autonomous vehicle show that the proposed method outperforms common alternatives because of its ability in recognizing intentions and using the information effectively for decision making.",
isbn="978-3-642-36279-8"
}

@inproceedings{Barbier2018,
	title = {{Probabilistic Decision-Making at Road Intersections: Formulation and Quantitative Evaluation}},
	author = {Barbier, Mathieu and Laugier, Christian and Simonin, Olivier and Iba{\~n}ez-Guzm{\'a}n, Javier},
	url = {https://hal.inria.fr/hal-01940392},
	booktitle = {{ICARCV 2018 - 15 th International Conference on Control, Automation, Robotics and Vision}},
	address = {Singapour, Singapore},
	pages = {1-8},
	year = {2018},
	month = Nov,
	pdf = {https://hal.inria.fr/hal-01940392/file/paper_icarcv_2018_submitted.pdf},
	hal_id = {hal-01940392},
	hal_version = {v1},
}

@inproceedings{Bouton2017,
  author={M. {Bouton} and A. {Cosgun} and M. J. {Kochenderfer}},
  booktitle={2017 IEEE Intelligent Vehicles Symposium (IV), 11-14 June 2017},
  month = jun,
  address = {Los Angeles, CA, USA},
  title={Belief state planning for autonomously navigating urban intersections},
  year={2017},
  volume={},
  number={},
  pages={825-830},
}

@inproceedings{Sunberg2017,
	abstract = {Safe interaction with human drivers is one of the primary challenges for autonomous vehicles. In order to plan driving maneuvers effectively, the vehicle's control system must infer and predict how humans will behave based on their latent internal state (e.g., intentions and aggressiveness). This research uses a simple model for human behavior with unknown parameters that make up the internal states of the traffic participants and presents a method for quantifying the value of estimating these states and planning with their uncertainty explicitly modeled. An upper performance bound is established by an omniscient Monte Carlo Tree Search (MCTS) planner that has perfect knowledge of the internal states. A baseline lower bound is established by planning with MCTS assuming that all drivers have the same internal state. MCTS variants are then used to solve a partially observable Markov decision process (POMDP) that models the internal state uncertainty to determine whether inferring the internal state offers an advantage over the baseline. Applying this method to a freeway lane changing scenario reveals that there is a significant performance gap between the upper bound and baseline. POMDP planning techniques come close to closing this gap, especially when important hidden model parameters are correlated with measurable parameters.},
	archivePrefix = {arXiv},
	arxivId = {1702.00858},
	author = {Sunberg, Zachary N. and Ho, Christopher J. and Kochenderfer, Mykel J.},
	booktitle = {2017 American Control Conference (ACC)},
	doi = {10.23919/ACC.2017.7963408},
	eprint = {1702.00858},
	isbn = {9781509059928},
	issn = {07431619},
	title = {{The value of inferring the internal state of traffic participants for autonomous freeway driving}},
	year = {2017},
	pages={3004-3010},
	month={24-26 May},
	address={Seattle, WA, USA}
}

@inproceedings{Bry2011,
	abstract = {In this paper we address the problem of motion planning in the presence of state uncertainty, also known as planning in belief space. The work is motivated by planning domains involving nontrivial dynamics, spatially varying measurement properties, and obstacle constraints. To make the problem tractable, we restrict the motion plan to a nominal trajectory stabilized with a linear estimator and controller. This allows us to predict distributions over future states given a candidate nominal trajectory. Using these distributions to ensure a bounded probability of collision, the algorithm incrementally constructs a graph of trajectories through state space, while efficiently searching over candidate paths through the graph at each iteration. This process results in a search tree in belief space that provably converges to the optimal path. We analyze the algorithm theoretically and also provide simulation results demonstrating its utility for balancing information gathering to reduce uncertainty and finding low cost paths. {\textcopyright} 2011 IEEE.},
	author = {Bry, Adam and Roy, Nicholas},
	booktitle = {2011 IEEE International Conference on Robotics and Automation, 9-13 May},
	month = may,
	address = {Shanghai, China},
	doi = {10.1109/ICRA.2011.5980508},
	isbn = {9781612843865},
	issn = {10504729},
	title = {{Rapidly-exploring random belief trees for motion planning under uncertainty}},
	year = {2011},
    pages={723-730},
}

@inproceedings{Xu2014,
	abstract = {We present a motion planning framework for autonomous on-road driving considering both the uncertainty caused by an autonomous vehicle and other traffic participants. The future motion of traffic participants is predicted using a local planner, and the uncertainty along the predicted trajectory is computed based on Gaussian propagation. For the autonomous vehicle, the uncertainty from localization and control is estimated based on a Linear-Quadratic Gaussian (LQG) framework. Compared with other safety assessment methods, our framework allows the planner to avoid unsafe situations more efficiently, thanks to the direct uncertainty information feedback to the planner. We also demonstrate our planner's ability to generate safer trajectories compared to planning only with a LQG framework.},
	author = {Xu, Wenda and Pan, Jia and Wei, Junqing and Dolan, John M.},
	booktitle = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
	doi = {10.1109/ICRA.2014.6907209},
	issn = {10504729},
	title = {{Motion planning under uncertainty for on-road autonomous driving}},
	year = {2014},
	pages = {2507-2512},
	month = {31 May--7 June},
	address = {Hong Kong, China}
}

@inproceedings{Pineau2003,
	author = {Pineau, Joelle and Gordon, Geoff and Thrun, Sebastian},
	title = {Point-Based Value Iteration: An Anytime Algorithm for POMDPs},
	year = {2003},
	publisher = {Morgan Kaufmann Publishers Inc.},
	address = {San Francisco, CA, USA},
	booktitle = {Proceedings of the 18th International Joint Conference on Artificial Intelligence},
	month=aug,
	pages = {1025–1030},
	numpages = {6},
	location = {Acapulco, Mexico},
	series = {IJCAI’03}
}

@article{Porta2006,
	author = {Porta, Josep M. and Vlassis, Nikos and Spaan, Matthijs T.J. and Poupart, Pascal},
	title = {Point-Based Value Iteration for Continuous POMDPs},
	year = {2006},
	issue_date = {12/1/2006},
	publisher = {JMLR.org},
	volume = {7},
	issn = {1532-4435},
	journal = {Journal of Machine Learning Research},
	month = dec,
	pages = {2329–2367},
	numpages = {39}
}

@incollection{Silver2010,
	title = {Monte-Carlo Planning in Large POMDPs},
	author = {Silver, David and Veness, Joel},
	booktitle = {Advances in Neural Information Processing Systems 23},
	editor = {J. D. Lafferty and C. K. I. Williams and J. Shawe-Taylor and R. S. Zemel and A. Culotta},
	pages = {2164--2172},
	year = {2010},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/4031-monte-carlo-planning-in-large-pomdps.pdf},
	month=dec,
	address = {Vancouver, Canada}
}

@article{Kalman1960,
  author = {Kalman, Rudolph Emil and Others},
  journal = {Journal of basic Engineering},
  number = {1},
  pages = {35--45},
  title = {A new approach to linear filtering and prediction problems},
  volume = {82},
  year = {1960}
}

@Inbook{VanDenBerg2017,
	author="van den Berg, Jur
	and Patil, Sachin
	and Alterovitz, Ron",
	title="Motion Planning Under Uncertainty Using Differential Dynamic Programming in Belief Space",
	bookTitle="Robotics Research : The 15th International Symposium ISRR",
	year="2017",
	publisher="Springer International Publishing",
	address="Cham",
	pages="473--490",
	abstract="We present an approach to motion planning under motion and sensing un-certainty, formally described as a continuous partially-observable Markov decision process (POMDP). Our approach is designed for non-linear dynamics and observation models, and follows the general POMDP solution framework in which we represent beliefs by Gaussian distributions, approximate the belief dynamics using an extended Kalman filter (EKF), and represent the value function by a quadratic function that is valid in the vicinity of a nominal trajectory through belief space. Using a variant of differential dynamic programming, our approach iterates with second-order convergence towards a linear control policy over the belief space that is locally-optimal with respect to a user-defined cost function. Unlike previous work, our approach does not assume maximum-likelihood observations, does not assume fixed estimator or control gains, takes into account obstacles in the environment, and does not require discretization of the belief space. The running time of the algorithm is polynomial in the dimension of the state space. We demonstrate the potential of our approach in several continuous partially-observable planning domains with obstacles for robots with non-linear dynamics and observation models.",
	isbn="978-3-319-29363-9",
	doi="10.1007/978-3-319-29363-9_27",
	url="https://doi.org/10.1007/978-3-319-29363-9_27"
}

@inproceedings{Brechtel2011,
	abstract = {This paper presents a method for high-level decision making in traffic environments. In contrast to the usual approach of modeling decision policies by hand, a Markov Decision Process (MDP) is employed to plan the optimal policy by assessing the outcomes of actions. Using probability theory, decisions are deduced automatically from the knowledge about how road users behave over time. This approach does neither depend on an explicit situation recognition nor is it limited to only a variety of situations or types of descriptions. Hence it is versatile and powerful. The contribution of this paper is a mathematical framework to derive abstract symbolic states from complex continuous temporal models encoded as Dynamic Bayesian Networks (DBN). For this purpose discrete MDP states are interpreted by random variables. To make computation feasible this space grows adaptively during planning and according to the problem to be solved. {\textcopyright} 2011 IEEE.},
	author = {Brechtel, Sebastian and Gindele, Tobias and Dillmann, Rudiger},
	booktitle = {IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC},
	doi = {10.1109/ITSC.2011.6082928},
	isbn = {9781457721984},
	title = {{Probabilistic MDP-behavior planning for cars}},
	year = {2011}
}

@inproceedings{Sun2019,
	abstract = {Autonomous cars have to navigate in dynamic environment which can be full of uncertainties. The uncertainties can come either from sensor limitations such as occlusions and limited sensor range, or from probabilistic prediction of other road participants, or from unknown social behavior in a new area. To safely and efficiently drive in the presence of these uncertainties, the decision-making and planning modules of autonomous cars should intelligently utilize all available information and appropriately tackle the uncertainties so that proper driving strategies can be generated. In this paper, we propose a social perception scheme which treats all road participants as distributed sensors in a sensor network. By observing the individual behaviors as well as the group behaviors, uncertainties of the three types can be updated uniformly in a belief space. The updated beliefs from the social perception are then explicitly incorporated into a probabilistic planning framework based on Model Predictive Control (MPC). The cost function of the MPC is learned via inverse reinforcement learning (IRL). Such an integrated probabilistic planning module with socially enhanced perception enables the autonomous vehicles to generate behaviors which are defensive but not overly conservative, and socially compatible. The effectiveness of the proposed framework is verified in simulation on an representative scenario with sensor occlusions.},
	archivePrefix = {arXiv},
	arxivId = {1905.00988},
	author = {Sun, Liting and Zhan, Wei and Chan, Ching Yao and Tomizuka, Masayoshi},
	booktitle = {2019 IEEE Intelligent Vehicles Symposium (IV)},
	doi = {10.1109/IVS.2019.8814223},
	eprint = {1905.00988},
	isbn = {9781728105604},
	title = {{Behavior Planning of Autonomous Cars with Social Perception}},
	year = {2019},
	pages={207-213},
	month={9-12 June},
	address={Paris, France},
}

@Comment{Chap2_3,
	title =	{Temporal abstraction},
}

@article{Sutton1999,
	title = "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning",
	journal = "Artificial Intelligence",
	volume = "112",
	number = "1",
	pages = "181 - 211",
	year = "1999",
	issn = "0004-3702",
	doi = "https://doi.org/10.1016/S0004-3702(99)00052-1",
	url = "http://www.sciencedirect.com/science/article/pii/S0004370299000521",
	author = "Richard S. Sutton and Doina Precup and Satinder Singh",
	keywords = "Temporal abstraction, Reinforcement learning, Markov decision processes, Options, Macros, Macroactions, Subgoals, Intra-option learning, Hierarchical planning, Semi-Markov decision processes",
	abstract = "Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key, longstanding challenges for AI. In this paper we consider how these challenges can be addressed within the mathematical framework of reinforcement learning and Markov decision processes (MDPs). We extend the usual notion of action in this framework to include options—closed-loop policies for taking action over a period of time. Examples of options include picking up an object, going to lunch, and traveling to a distant city, as well as primitive actions such as muscle twitches and joint torques. Overall, we show that options enable temporally abstract knowledge and action to be included in the reinforcement learning framework in a natural and general way. In particular, we show that options may be used interchangeably with primitive actions in planning methods such as dynamic programming and in learning methods such as Q-learning. Formally, a set of options defined over an MDP constitutes a semi-Markov decision process (SMDP), and the theory of SMDPs provides the foundation for the theory of options. However, the most interesting issues concern the interplay between the underlying MDP and the SMDP and are thus beyond SMDP theory. We present results for three such cases: (1) we show that the results of planning with options can be used during execution to interrupt options and thereby perform even better than planned, (2) we introduce new intra-option methods that are able to learn about an option from fragments of its execution, and (3) we propose a notion of subgoal that can be used to improve the options themselves. All of these results have precursors in the existing literature; the contribution of this paper is to establish them in a simpler and more general setting with fewer changes to the existing reinforcement learning framework. In particular, we show that these results can be obtained without committing to (or ruling out) any particular approach to state abstraction, hierarchy, function approximation, or the macro-utility problem."
}

@misc{ShalevShwartz2017,
    title={On a Formal Model of Safe and Scalable Self-driving Cars},
    author={Shai Shalev-Shwartz and Shaked Shammah and Amnon Shashua},
    year={2017},
    eprint={1708.06374},
    archivePrefix={arXiv},
    primaryClass={cs.RO},
    note={preprint}
}

@misc{ShalevShwartz2016,
    title={Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving},
    author={Shai Shalev-Shwartz and Shaked Shammah and Amnon Shashua},
    year={2016},
    eprint={1610.03295},
    archivePrefix={arXiv},
    primaryClass={cs.AI},
    note={preprint}
}

@inproceedings{Paxton2017,
	abstract = {Task and motion planning subject to Linear Temporal Logic (LTL) specifications in complex, dynamic environments requires efficient exploration of many possible future worlds. Model-free reinforcement learning has proven successful in a number of challenging tasks, but shows poor performance on tasks that require long-term planning. In this work, we integrate Monte Carlo Tree Search with hierarchical neural net policies trained on expressive LTL specifications. We use reinforcement learning to find deep neural networks representing both low-level control policies and task-level 'option policies' that achieve high-level goals. Our combined architecture generates safe and responsive motion plans that respect the LTL constraints. We demonstrate our approach in a simulated autonomous driving setting, where a vehicle must drive down a road in traffic, avoid collisions, and navigate an intersection, all while obeying rules of the road.},
	archivePrefix = {arXiv},
	arxivId = {1703.07887},
	author = {Paxton, Chris and Raman, Vasumathi and Hager, Gregory D. and Kobilarov, Marin},
  	booktitle={2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
	doi = {10.1109/IROS.2017.8206505},
	eprint = {1703.07887},
	isbn = {9781538626825},
	issn = {21530866},
	title = {{Combining neural networks and tree search for task and motion planning in challenging environments}},
	year = {2017},
	pages={6059-6066},
	month={24-28 Sept},
	address = {Vancouver, BC, Canada}
}

@inproceedings{Bacon2017,
	author = {Bacon, Pierre-Luc and Harb, Jean and Precup, Doina},
	title = {The Option-Critic Architecture},
	year = {2017},
	publisher = {AAAI Press},
	booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
	pages = {1726–1734},
	numpages = {9},
	address = {San Francisco, California, USA},
	month = feb,
	series = {AAAI’17}
}

@inproceedings{Vezhnevets2017,
	author = {Vezhnevets, Alexander Sasha and Osindero, Simon and Schaul, Tom and Heess, Nicolas and Jaderberg, Max and Silver, David and Kavukcuoglu, Koray},
	title = {FeUdal Networks for Hierarchical Reinforcement Learning},
	year = {2017},
	publisher = {JMLR},
	booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
	month = aug,
	pages = {3540–3549},
	numpages = {10},
	address = {Sydney, NSW, Australia},
	series = {ICML’17}
}

@misc{Heess2016,
    title={Learning and Transfer of Modulated Locomotor Controllers},
    author={Nicolas Heess and Greg Wayne and Yuval Tassa and Timothy Lillicrap and Martin Riedmiller and David Silver},
    year={2016},
    eprint={1610.05182},
    archivePrefix={arXiv},
    primaryClass={cs.RO},
    note={preprint}
}

@Comment{Chap2_4,
	title = {Inverse Reinforcement Learning},
}

@inproceedings{Kuderer2015,
	abstract = {It is expected that autonomous vehicles capable of driving without human supervision will be released to market within the next decade. For user acceptance, such vehicles should not only be safe and reliable, they should also provide a comfortable user experience. However, individual perception of comfort may vary considerably among users. Whereas some users might prefer sporty driving with high accelerations, others might prefer a more relaxed style. Typically, a large number of parameters such as acceleration profiles, distances to other cars, speed during lane changes, etc., characterize a human driver's style. Manual tuning of these parameters may be a tedious and error-prone task. Therefore, we propose a learning from demonstration approach that allows the user to simply demonstrate the desired style by driving the car manually. We model the individual style in terms of a cost function and use feature-based inverse reinforcement learning to find the model parameters that fit the observed style best. Once the model has been learned, it can be used to efficiently compute trajectories for the vehicle in autonomous mode. We show that our approach is capable of learning cost functions and reproducing different driving styles using data from real drivers.},
	author = {Kuderer, Markus and Gulati, Shilpa and Burgard, Wolfram},
	booktitle = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
	month = {26-30 May},
	address = {Seattle, WA, USA},
	doi = {10.1109/ICRA.2015.7139555},
	issn = {10504729},
	title = {{Learning driving styles for autonomous vehicles from demonstration}},
	pages={2641-2646},
	year = {2015}
}

@inbook{Ziebart2008,
	author = {Ziebart, Brian D. and Maas, Andrew L. and Dey, Anind K. and Bagnell, J. Andrew},
	title = {Navigate like a Cabbie: Probabilistic Reasoning from Observed Context-Aware Behavior},
	year = {2008},
	isbn = {9781605581361},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/1409635.1409678},
	booktitle = {Proceedings of the 10th International Conference on Ubiquitous Computing (UbiComp)},
	pages = {322–331},
	month = sep,
	numpages = {10}
}

@inproceedings{Ziebart2009,
	abstract = {We present a novel approach for determining robot movements that efficiently accomplish the robot's tasks while not hindering the movements of people within the environment. Our approach models the goal-directed trajectories of pedestrians using maximum entropy inverse optimal control. The advantage of this modeling approach is the generality of its learned cost function to changes in the environment and to entirely different environments. We employ the predictions of this model of pedestrian trajectories in a novel incremental planner and quantitatively show the improvement in hindrance-sensitive robot trajectory planning provided by our approach. {\textcopyright} 2009 IEEE.},
	author = {Ziebart, Brian D. and Ratliff, Nathan and Gallagher, Garratt and Mertz, Christoph and Peterson, Kevin and Bagnell, J. Andrew and Hebert, Martial and Dey, Anind K. and Srinivasa, Siddhartha},
	booktitle = {2009 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
	doi = {10.1109/IROS.2009.5354147},
	isbn = {9781424438044},
	title = {{Planning-based prediction for pedestrians}},
	year = {2009},
	pages = {3931-3936},
	month = {10-15 Oct.},
	address = {St. Louis, MO, USA}
}

@inproceedings{Sadigh2016,
	abstract = {Traditionally, autonomous cars make predictions about other drivers' future trajectories, and plan to stay out of their way. This tends to result in defensive and opaque behaviors. Our key insight is that an autonomous car's actions will actually affect what other cars will do in response, whether the car is aware of it or not. Our thesis is that we can leverage these responses to plan more efficient and communicative behaviors. We model the interaction between an autonomous car and a human driver as a dynamical system, in which the robot's actions have immediate consequences on the state of the car, but also on human actions. We model these consequences by approximating the human as an optimal planner, with a reward function that we acquire through Inverse Reinforcement Learning. When the robot plans with this reward function in this dynamical system, it comes up with actions that purposefully change human state: it merges in front of a human to get them to slow down or to reach its own goal faster; it blocks two lanes to get them to switch to a third lane; or it backs up slightly at an intersection to get them to proceed first. Such behaviors arise from the optimization, without relying on hand-coded signaling strategies and without ever explicitly modeling communication. Our user study results suggest that the robot is indeed capable of eliciting desired changes in human state by planning using this dynamical system.},
	author = {Sadigh, Dorsa and Sastry, Shankar and Seshia, Sanjit A. and Dragan, Anca D.},
    booktitle = {Proceedings of Robotics: Science and Systems},
	doi = {10.15607/rss.2016.xii.029},
	isbn = {9780992374723},
	issn = {2330765X},
	title = {{Planning for autonomous cars that leverage effects on human actions}},
	year = {2016},
    address = {AnnArbor, Michigan},
    month = {June},
}

@Comment{Chap2_5,
	title = {Transfer},
}

@misc{levine2020offline,
	title={Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems},
	author={Sergey Levine and Aviral Kumar and George Tucker and Justin Fu},
	year={2020},
	eprint={2005.01643},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	note={preprint}
}

@InProceedings{Laroche2019,
  title = 	 {Safe Policy Improvement with Baseline Bootstrapping},
  author = 	 {Laroche, Romain and Trichelair, Paul and Combes, Remi Tachet Des},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {3652--3661},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/laroche19a/laroche19a.pdf},
  url = 	 {http://proceedings.mlr.press/v97/laroche19a.html},
  abstract = 	 {This paper considers Safe Policy Improvement (SPI) in Batch Reinforcement Learning (Batch RL): from a fixed dataset and without direct access to the true environment, train a policy that is guaranteed to perform at least as well as the baseline policy used to collect the data. 	 Our approach, called SPI with Baseline Bootstrapping (SPIBB), is inspired by the knows-what-it-knows paradigm: it bootstraps the trained policy with the baseline when the uncertainty is high. 	 Our first algorithm, $\Pi_b$-SPIBB, comes with SPI theoretical guarantees. 	 We also implement a variant, $\Pi_{\leq b}$-SPIBB, that is even more efficient in practice. 	 We apply our algorithms to a motivational stochastic gridworld domain and further demonstrate on randomly generated MDPs the superiority of SPIBB with respect to existing algorithms, not only in safety but also in mean performance. 	 Finally, we implement a model-free version of SPIBB and show its benefits on a navigation task with deep RL implementation called SPIBB-DQN, which is, to the best of our knowledge, the first RL algorithm relying on a neural network representation able to train efficiently and reliably from batch data, without any interaction with the environment.}
}

@InProceedings{Nadjahi2019,
	author="Nadjahi, Kimia
	and Laroche, Romain
	and Tachet des Combes, R{\'e}mi",
	editor="Brefeld, Ulf
	and Fromont, Elisa
	and Hotho, Andreas
	and Knobbe, Arno
	and Maathuis, Marloes
	and Robardet, C{\'e}line",
	title="Safe Policy Improvement with Soft Baseline Bootstrapping",
	booktitle="European Conference on Machine Learning and Knowledge Discovery in Databases",
	year="2020",
	publisher="Springer International Publishing",
	address="Würzburg, Germany",
	month="16-20 Sep",
	pages="53--68",
	abstract="Batch Reinforcement Learning (Batch RL) consists in training a policy using trajectories collected with another policy, called the behavioural policy. Safe policy improvement (SPI) provides guarantees with high probability that the trained policy performs better than the behavioural policy, also called baseline in this setting. Previous work shows that the SPI objective improves mean performance as compared to using the basic RL objective, which boils down to solving the MDP with maximum likelihood (Laroche et al. 2019). Here, we build on that work and improve more precisely the SPI with Baseline Bootstrapping algorithm (SPIBB) by allowing the policy search over a wider set of policies. Instead of binarily classifying the state-action pairs into two sets (the uncertain and the safe-to-train-on ones), we adopt a softer strategy that controls the error in the value estimates by constraining the policy change according to the local model uncertainty. The method can take more risks on uncertain actions all the while remaining provably-safe, and is therefore less conservative than the state-of-the-art methods. We propose two algorithms (one optimal and one approximate) to solve this constrained optimization problem and empirically show a significant improvement over existing SPI algorithms both on finite MDPS and on infinite MDPs with a neural network function approximation.",
	isbn="978-3-030-46133-1"
}

@inproceedings{Kakade2002,
	author = {Kakade, Sham and Langford, John},
	title = {Approximately Optimal Approximate Reinforcement Learning},
	year = {2002},
	isbn = {1558608737},
	publisher = {Morgan Kaufmann Publishers Inc.},
	month = jul,
	address = {San Francisco, CA, USA},
	booktitle = {Proceedings of the Nineteenth International Conference on Machine Learning},
	pages = {267–274},
	numpages = {8},
	series = {ICML ’02}
}

@InProceedings{Schulman2015,
  title = 	 {Trust Region Policy Optimization},
  author = 	 {John Schulman and Sergey Levine and Pieter Abbeel and Michael Jordan and Philipp Moritz},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1889--1897},
  year = 	 {2015},
  editor = 	 {Francis Bach and David Blei},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/schulman15.pdf},
  url = 	 {http://proceedings.mlr.press/v37/schulman15.html},
  abstract = 	 {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.}
}

@inproceedings{Pan2017,
    title={Virtual to Real Reinforcement Learning for Autonomous Driving},
    author={Xinlei Pan, Yurong You, Ziyan Wang and Cewu Lu},
    year={2017},
    month=sep,
    pages={11.1-11.13},
    articleno={11},
    numpages={13},
    booktitle={Proceedings of the British Machine Vision Conference (BMVC)},
    address={London, United Kingdom},
    publisher={BMVA Press},
    editor={Tae-Kyun Kim, Stefanos Zafeiriou, Gabriel Brostow and Krystian Mikolajczyk},
    doi={10.5244/C.31.11},
    isbn={1-901725-60-X},
    url={https://dx.doi.org/10.5244/C.31.11}
}

@misc{Liang2019,
	title={Federated Transfer Reinforcement Learning for Autonomous Driving},
	author={Xinle Liang and Yang Liu and Tianjian Chen and Ming Liu and Qiang Yang},
	year={2019},
	eprint={1910.06001},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	note={preprint}
}

@inproceedings{Tobin2017,
	abstract = {Bridging the 'reality gap' that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to 1.5 cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control.},
	archivePrefix = {arXiv},
	arxivId = {1703.06907},
	author = {Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
	booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
	doi = {10.1109/IROS.2017.8202133},
	eprint = {1703.06907},
	isbn = {9781538626825},
	issn = {21530866},
	title = {{Domain randomization for transferring deep neural networks from simulation to the real world}},
	year = {2017},
	pages={23-30},
	month={24-28 Sept},
	address={Vancouver, BC, Canada}
}

@misc{openai2019solving,
	title={Solving Rubik's Cube with a Robot Hand},
	author={OpenAI and Ilge Akkaya and Marcin Andrychowicz and Maciek Chociej and Mateusz Litwin and Bob McGrew and Arthur Petron and Alex Paino and Matthias Plappert and Glenn Powell and Raphael Ribas and Jonas Schneider and Nikolas Tezak and Jerry Tworek and Peter Welinder and Lilian Weng and Qiming Yuan and Wojciech Zaremba and Lei Zhang},
	year={2019},
	eprint={1910.07113},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	note={preprint}
}

@inproceedings{Prakash2019,
	abstract = {We present structured domain randomization (SDR), a variant of domain randomization (DR) that takes into account the structure of the scene in order to add context to the generated data. In contrast to DR, which places objects and distractors randomly according to a uniform probability distribution, SDR places objects and distractors randomly according to probability distributions that arise from the specific problem at hand. In this manner, SDR-generated imagery enables the neural network to take the context around an object into consideration during detection. We demonstrate the power of SDR for the problem of 2D bounding box car detection, achieving competitive results on real data after training only on synthetic data. On the KITTI easy, moderate, and hard tasks, we show that SDR outperforms other approaches to generating synthetic data (VKITTI, Sim 200k, or DR), as well as real data collected in a different domain (BDD100K). Moreover, synthetic SDR data combined with real KITTI data outperforms real KITTI data alone.11Video is at http://youtu.be/1WdjWJYx9AY.},
	archivePrefix = {arXiv},
	arxivId = {1810.10093},
	author = {Prakash, Aayush and Boochoon, Shaad and Brophy, Mark and Acuna, David and Cameracci, Eric and State, Gavriel and Shapira, Omer and Birchfield, Stan},
	booktitle = {2019 International Conference on Robotics and Automation (ICRA)},
	month = {20-24 May},
	address = {Montreal, QC, Canada},
	doi = {10.1109/ICRA.2019.8794443},
	eprint = {1810.10093},
	isbn = {9781538660263},
	issn = {10504729},
	title = {{Structured domain randomization: Bridging the reality gap by context-aware synthetic data}},
	year = {2019},
	pages={7249-7255},
}

@inproceedings{Pouyanfar2019,
	abstract = {End-to-end deep learning has emerged as a simple and promising approach for autonomous driving recently. However, collecting large-scale real-world data representing the full spectrum of road scenarios and rare events remains the main hurdle in this area. For this purpose, this paper addresses the problem of end-to-end collision-free deep driving using only simulation data. It extends the idea of domain randomization to bridge the reality gap between simulation and the real world. Using a range of domain randomization flavors in a primitive simulation, it is shown that a model can learn to drive in realistic environments without seeing any real or photo-realistic images. The proposed work dramatically reduces the need for collecting large real-world or high-fidelity simulated datasets, along with allowing for the creation of rare events in the simulation. Finally, this is the first time domain randomization is used for the application of 'deep driving' which can avoid obstacles. The effectiveness of the proposed method is demonstrated with extensive experiments on both simulation and real-world datasets.},
	author = {Pouyanfar, Samira and Saleem, Muneeb and George, Nikhil and Chen, Shu Ching},
	booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
	doi = {10.1109/CVPRW.2019.00166},
	isbn = {9781728125060},
	issn = {21607516},
	title = {{ROADS: Randomization for obstacle avoidance and driving in simulation}},
	year = {2019},
	pages={1267-1276},
	month = {16-17 June},
	address = {Long Beach, CA, USA}
}

@InProceedings{Mueller2018,
	title = 	 {Driving Policy Transfer via Modularity and Abstraction},
	author = 	 {Mueller, Matthias and Dosovitskiy, Alexey and Ghanem, Bernard and Koltun, Vladlen},
	booktitle = 	 {Proceedings of The 2nd Conference on Robot Learning},
	pages = 	 {1--15},
	year = 	 {2018},
	editor = 	 {Billard, Aude and Dragan, Anca and Peters, Jan and Morimoto, Jun},
	volume = 	 {87},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Zürich, Switzerland},
	month = 	 {29--31 Oct},
	publisher =  {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v87/mueller18a/mueller18a.pdf},
	url = 	 {http://proceedings.mlr.press/v87/mueller18a.html},
	abstract = 	 {End-to-end approaches to autonomous driving have high sample complexity and are difficult to scale to realistic urban driving. Simulation can help end-to-end driving systems by providing a cheap, safe, and diverse training environment. Yet training driving policies in simulation brings up the problem of transferring such policies to the real world. We present an approach to transferring driving policies from simulation to reality via modularity and abstraction. Our approach is inspired by classic driving systems and aims to combine the benefits of modular architectures and end-to-end deep learning approaches. The key idea is to encapsulate the driving policy such that it is not directly exposed to raw perceptual input or low-level vehicle dynamics. We evaluate the presented approach in simulated urban environments and in the real world. In particular, we transfer a driving policy trained in simulation to a 1/5-scale robotic truck that is deployed in a variety of conditions, with no finetuning, on two continents.}
}

@incollection{Liu2017,
	title = {Unsupervised Image-to-Image Translation Networks},
	author = {Liu, Ming-Yu and Breuel, Thomas and Kautz, Jan},
	booktitle = {Advances in Neural Information Processing Systems 30},
	editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	pages = {700--708},
	year = {2017},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/6672-unsupervised-image-to-image-translation-networks.pdf}
}

@INPROCEEDINGS{Ros2016,
	author={G. {Ros} and L. {Sellart} and J. {Materzynska} and D. {Vazquez} and A. M. {Lopez}},
	booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
	title={The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes}, 
	year={2016},
	month={27-30 June},
	address={Las Vegas, NV, USA},
	pages={3234-3243},
}

@inproceedings{Cordts2016,
	abstract = {Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations, 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.},
	archivePrefix = {arXiv},
	arxivId = {1604.01685},
	author = {Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
	booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 27-30 June},
	month = jun,
	address = {Las Vegas, NV, USA},
	doi = {10.1109/CVPR.2016.350},
	eprint = {1604.01685},
	isbn = {9781467388504},
	issn = {10636919},
	title = {{The Cityscapes Dataset for Semantic Urban Scene Understanding}},
	year = {2016},
    pages={3213-3223},
}


@Comment{Chap2_6,
	title = {Safety},
}

@article{Garcia2015,
  author  = {Javier Garc{{\'i}}a and Fern and o Fern{{\'a}}ndez},
  title   = {A Comprehensive Survey on Safe Reinforcement Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2015},
  volume  = {16},
  number  = {42},
  pages   = {1437-1480},
  url     = {http://jmlr.org/papers/v16/garcia15a.html}
}

@book{Markowitz59,
	ISBN = {9780300013726},
	URL = {http://www.jstor.org/stable/j.ctt1bh4c8h},
	abstract = {Applies modern techniques of analysis and computation to the problem of finding combinations of securities that best meet the needs of the private institutional investor. Written primarily with the nonmathematician in mind, although it contains mathematical development of the subject in appendixes.},
	author = {Harry M. Markowitz},
	publisher = {Yale University Press},
	title = {Portfolio Selection: Efficient Diversification of Investments},
	year = {1959}
}

@article{Moody2001,
	author={J. {Moody} and M. {Saffell}},
	journal={IEEE Transactions on Neural Networks},
	title={Learning to trade via direct reinforcement},
	year={2001},
	volume={12},
	number={4},
	pages={875-889},
}

@inproceedings{Tamar2012,
	author = {Tamar, Aviv and Di Castro, Dotan and Mannor, Shie},
	title = {Policy Gradients with Variance Related Risk Criteria},
	year = {2012},
	isbn = {9781450312851},
	publisher = {Omnipress},
	booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning (ICML 2012)},
	pages = {1651–1658},
	numpages = {8},
	address = {Edinburgh, Scotland},
	month = jun,
	series = {ICML’12}
}

@incollection{Prashanth2013,
	title = {Actor-Critic Algorithms for Risk-Sensitive MDPs},
	author = {L.A., Prashanth and Ghavamzadeh, Mohammad},
	booktitle = {Advances in Neural Information Processing Systems 26},
	editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
	pages = {252--260},
	year = {2013},
	publisher = {Curran Associates, Inc.},
	address = {Lake Tahoe, NV/CA, USA},
	url = {http://papers.nips.cc/paper/4917-actor-critic-algorithms-for-risk-sensitive-mdps.pdf}
}

@article{Chow2018,
	author = {Chow, Yinlam and Ghavamzadeh, Mohammad and Janson, Lucas and Pavone, Marco},
	title = {Risk-Constrained Reinforcement Learning with Percentile Risk Criteria},
	year = {2017},
	publisher = {JMLR.org},
	volume = {18},
	number = {1},
	issn = {1532-4435},
	journal = {J. Mach. Learn. Res.},
	month = jan,
	pages = {6070–6120},
	numpages = {51},
	keywords = {policy gradient algorithms, actor-critic algorithms, Markov decision process, chance-constrained optimization, reinforcement learning, conditional value-at-risk}
}

@ARTICLE{Artzner1999,
	title = {Coherent Measures of Risk},
	author = {Artzner, Philippe and Delbaen, Freddy and Eber, Jean‐Marc and Heath, David},
	year = {1999},
	journal = {Mathematical Finance},
	volume = {9},
	number = {3},
	pages = {203-228},
	abstract = {In this paper we study both market risks and nonmarket risks, without complete markets assumption, and discuss methods of measurement of these risks. We present and justify a set of four desirable properties for measures of risk, and call the measures satisfying these properties “coherent.” We examine the measures of risk provided and the related actions required by SPAN, by the SEC/NASD rules, and by quantile‐based methods. We demonstrate the universality of scenario‐based methods for providing coherent measures. We offer suggestions concerning the SEC method. We also suggest a method to repair the failure of subadditivity of quantile‐based methods.},
}

@article{Delage2010,
	author = {Delage, Erick and Mannor, Shie},
	title = {Percentile Optimization for Markov Decision Processes with Parameter Uncertainty},
	year = {2010},
	issue_date = {January 2010},
	publisher = {INFORMS},
	address = {Linthicum, MD, USA},
	volume = {58},
	number = {1},
	issn = {0030-364X},
	url = {https://doi.org/10.1287/opre.1080.0685},
	doi = {10.1287/opre.1080.0685},
	journal = {Oper. Res.},
	month = jan,
	pages = {203–213},
	numpages = {11},
	keywords = {stochastic model applications, parameter uncertainty, value at risk, stochastic programming, finite state, chance-constrained optimization, Markov decision processes}
}


@incollection{Tamar2015,
	title = {Policy Gradient for Coherent Risk Measures},
	author = {Tamar, Aviv and Chow, Yinlam and Ghavamzadeh, Mohammad and Mannor, Shie},
	booktitle = {Advances in Neural Information Processing Systems 28},
	editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
	pages = {1468--1476},
	year = {2015},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/5923-policy-gradient-for-coherent-risk-measures.pdf}
}

@article{Geibel2005,
	author = {Geibel, Peter and Wysotzki, Fritz},
	title = {Risk-Sensitive Reinforcement Learning Applied to Control under Constraints},
	year = {2005},
	issue_date = {July 2005},
	publisher = {AI Access Foundation},
	address = {El Segundo, CA, USA},
	volume = {24},
	number = {1},
	issn = {1076-9757},
	journal = {Journal of Artificial Intelligence Research},
	month = jul,
	pages = {81–108},
	numpages = {28}
}

@inproceedings{Berkenkamp2015,
	abstract = {This paper introduces a learning-based robust control algorithm that provides robust stability and performance guarantees during learning. The approach uses Gaussian process (GP) regression based on data gathered during operation to update an initial model of the system and to gradually decrease the uncertainty related to this model. Embedding this data-based update scheme in a robust control framework guarantees stability during the learning process. Traditional robust control approaches have not considered online adaptation of the model and its uncertainty before. As a result, their controllers do not improve performance during operation. Typical machine learning algorithms that have achieved similar high-performance behavior by adapting the model and controller online do not provide the guarantees presented in this paper. In particular, this paper considers a stabilization task, linearizes the nonlinear, GP-based model around a desired operating point, and solves a convex optimization problem to obtain a linear robust controller. The resulting performance improvements due to the learning-based controller are demonstrated in experiments on a quadrotor vehicle.},
	author = {Berkenkamp, Felix and Schoellig, Angela P.},
	booktitle = {2015 European Control Conference, ECC 2015, 15-17 July 2015},
	month = jul,
	location = {Linz, Austria},
	address = {Linz, Austria},
	doi = {10.1109/ECC.2015.7330913},
	isbn = {9783952426937},
	title = {{Safe and robust learning control with Gaussian processes}},
    pages={2496-2501},
	year = {2015}
}

@inproceedings{Tamar2014,
	abstract = {We consider large-scale Markov decision processes (MDPs) with parameter uncertainty, un-der the robust MDP paradigm. Previous studies showed that robust MDPs, based on a minimax approach to handling uncertainty, can be solved using dynamic programming for small to medium sized problems. However, due to the "curse of dimensionality", MDPs that model real-life problems are typically prohibitively large for such approaches. In this work we employ a reinforcement learning approach to tackle this planning problem: we develop a robust approximate dynamic programming method based on a projected fixed point equation to approximately solve large scale robust MDPs. We show that the proposed method provably succeeds under certain technical conditions, and demonstrate its effectiveness through simulation of an option pricing problem. To the best of our knowledge, this is the first attempt to scale up the robust MDP paradigm.},
	author = {Tamar, Aviv and Mannor, Shie and Xu, Huan},
	booktitle = {31st International Conference on Machine Learning, ICML 2014},
	isbn = {9781634393973},
	title = {{Scaling up robust MDPs using function approximation}},
	year = {2014}
}

@inproceedings{Bouton2019,
  author={Bouton, Maxime and Nakhaei, Alireza and Fujimura, Kikuo and Kochenderfer, Mykel J.},
  booktitle={2019 IEEE Intelligent Vehicles Symposium (IV), 9-12 June},
  month = jun,
  address = {Paris, France},
  title={Safe Reinforcement Learning with Scene Decomposition for Navigating Complex Urban Environments},
  year={2019},
  archivePrefix = {arXiv},
  arxivId = {1904.11483},
  doi = {10.1109/IVS.2019.8813803},
  eprint = {1904.11483},
  isbn = {9781728105604},
  volume={},
  number={},
  pages={1469-1476},
}

@inproceedings{Bouton2019workshop,
	title={Reinforcement learning with probabilistic guarantees for autonomous driving},
	author={Bouton, Maxime and Karlsson, Jesper and Nakhaei, Alireza and Fujimura, Kikuo and Kochenderfer, Mykel J and Tumova, Jana},
	booktitle={Workshop on Safety Risk and Uncertainty in Reinforcement Learning, Conference on Uncertainty in Artificial Intelligence (UAI)},
	address = {Tel Aviv, Israel},
	month = jul,
	year={2019}
}

@inproceedings{Koller2019,
	abstract = {Learning-based methods have been successful in solving complex control tasks without significant prior knowledge about the system. However, these methods typically do not provide any safety guarantees, which prevents their use in safety-critical, real-world applications. In this paper, we present a learning-based model predictive control scheme that can provide provable high-probability safety guarantees. To this end, we exploit regularity assumptions on the dynamics in terms of a Gaussian process prior to construct provably accurate confidence intervals on predicted trajectories. Unlike previous approaches, we do not assume that model uncertainties are independent. Based on these predictions, we guarantee that trajectories satisfy safety constraints. Moreover, we use a terminal set constraint to recursively guarantee the existence of safe control actions at every iteration. In our experiments, we show that the resulting algorithm can be used to safely and efficiently explore and learn about dynamic systems.},
	archivePrefix = {arXiv},
	arxivId = {1803.08287},
	author = {Koller, Torsten and Berkenkamp, Felix and Turchetta, Matteo and Krause, Andreas},
	booktitle = {Proceedings of the IEEE Conference on Decision and Control},
	doi = {10.1109/CDC.2018.8619572},
	eprint = {1803.08287},
	isbn = {9781538613955},
	issn = {07431546},
	title = {{Learning-Based Model Predictive Control for Safe Exploration}},
	year = {2019}
}

@techreport{Williams2018,
	abstract = {We present an algorithmic framework for stochastic model predictive control that is able to optimize non-linear systems with cost functions that have sparse, discontinuous gradient information. The proposed framework combines the benefits of sampling-based model predictive control with linearization-based trajectory optimization methods. The resulting algorithm consists of a novel utilization of Tube-based model predictive control. We demonstrate robust algorithmic performance on a variety of simulated tasks, and on a real-world fast autonomous driving task.},
	author = {Williams, Grady and Goldfain, Brian and Drews, Paul and Saigol, Kamil and Rehg, James M and Theodorou, Evangelos A},
	title = {{Robust Sampling Based Model Predictive Control with Sparse Objective Information}},
	year = {2018}
}

@misc{Naghshvar2018,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1812.01254v1},
	author = {Naghshvar, Mohammad and Sadek, Ahmed K and Wiggers, Auke J},
	eprint = {arXiv:1812.01254v1},
	title = {{Risk-averse Behavior Planning for Autonomous Driving under Uncertainty}},
	year = {2018},
	note = {preprint}
}

@inproceedings{Berkenkamp2016,
	abstract = {One of the most fundamental problems when designing controllers for dynamic systems is the tuning of the controller parameters. Typically, a model of the system is used to obtain an initial controller, but ultimately the controller parameters must be tuned manually on the real system to achieve the best performance. To avoid this manual tuning step, methods from machine learning, such as Bayesian optimization, have been used. However, as these methods evaluate different controller parameters on the real system, safety-critical system failures may happen. In this paper, we overcome this problem by applying, for the first time, a recently developed safe optimization algorithm, SafeOpt, to the problem of automatic controller parameter tuning. Given an initial, low-performance controller, SafeOpt automatically optimizes the parameters of a control law while guaranteeing safety. It models the underlying performance measure as a Gaussian process and only explores new controller parameters whose performance lies above a safe performance threshold with high probability. Experimental results on a quadrotor vehicle indicate that the proposed method enables fast, automatic, and safe optimization of controller parameters without human intervention.},
	archivePrefix = {arXiv},
	arxivId = {1509.01066},
	author = {Berkenkamp, Felix and Schoellig, Angela P. and Krause, Andreas},
	booktitle = {2016 IEEE International Conference on Robotics and Automation (ICRA), 16-21 May 2016},
	doi = {10.1109/ICRA.2016.7487170},
	eprint = {1509.01066},
	isbn = {9781467380263},
	issn = {10504729},
	title = {{Safe controller optimization for quadrotors with Gaussian processes}},
	year = {2016},
	month = may,
	address = {Stockholm, Sweden},
	pages={491-496},
}

@incollection{Turchetta2016,
	title = {Safe Exploration in Finite Markov Decision Processes with Gaussian Processes},
	author = {Turchetta, Matteo and Berkenkamp, Felix and Krause, Andreas},
	booktitle = {Advances in Neural Information Processing Systems 29},
	editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
	pages = {4312--4320},
	year = {2016},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/6358-safe-exploration-in-finite-markov-decision-processes-with-gaussian-processes.pdf},
	month=dec,
	address={Barcelona, Spain}
}

@book{Altman1999,
	title = {Constrained Markov Decision Processes},
	author = {Altman, Eitan},
	year = {1999},
	publisher = {Chapman and Hall/CRC},
}

@InProceedings{Achiam2017,
  title = 	 {Constrained Policy Optimization},
  author = 	 {Joshua Achiam and David Held and Aviv Tamar and Pieter Abbeel},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {22--31},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/achiam17a/achiam17a.pdf},
  url = 	 {http://proceedings.mlr.press/v70/achiam17a.html},
  abstract = 	 {For many applications of reinforcement learning it can be more convenient to specify both a reward function and constraints, rather than trying to design behavior through the reward function. For example, systems that physically interact with or around humans should satisfy safety constraints. Recent advances in policy search algorithms (Mnih et al., 2016, Schulman et al., 2015, Lillicrap et al., 2016, Levine et al., 2016) have enabled new capabilities in high-dimensional control, but do not consider the constrained setting. We propose Constrained Policy Optimization (CPO), the first general-purpose policy search algorithm for constrained reinforcement learning with guarantees for near-constraint satisfaction at each iteration. Our method allows us to train neural network policies for high-dimensional control while making guarantees about policy behavior all throughout training. Our guarantees are based on a new theoretical result, which is of independent interest: we prove a bound relating the expected returns of two policies to an average divergence between them. We demonstrate the effectiveness of our approach on simulated robot locomotion tasks where the agent must satisfy constraints motivated by safety.}
}

@inproceedings{Tessler2019,
	abstract = {Solving tasks in Reinforcement Learning is no easy feat. As the goal of the agent is to maximize the accumulated reward, it often learns to exploit loopholes and misspecifications in the reward signal resulting in unwanted behavior. While constraints may solve this issue, there is no closed form solution for general constraints. In this work, we present a novel multi-timescale approach for constrained policy optimization, called 'Reward Constrained Policy Optimization' (RCPO), which uses an alternative penalty signal to guide the policy towards a constraint satisfying one. We prove the convergence of our approach and provide empirical evidence of its ability to train constraint satisfying policies.},
	archivePrefix = {arXiv},
	arxivId = {1805.11074},
	author = {Tessler, Chen and Mankowitz, Daniel J. and Mannor, Shie},
	booktitle = {7th International Conference on Learning Representations, ICLR 2019},
	eprint = {1805.11074},
	title = {{Reward constrained policy optimization}},
	address={New Orleans, LA, USA},
	month=may,
	year = {2019}
}

@InProceedings{leung2018infusing,
	author="Leung, Karen
	and Schmerling, Edward
	and Chen, Mo
	and Talbot, John
	and Gerdes, J. Christian
	and Pavone, Marco",
	editor="Xiao, Jing
	and Kr{\"o}ger, Torsten
	and Khatib, Oussama",
	title="On Infusing Reachability-Based Safety Assurance Within Probabilistic Planning Frameworks for Human-Robot Vehicle Interactions",
	booktitle="Proceedings of the 2018 International Symposium on Experimental Robotics (ISER 2018)",
	year="2020",
	publisher="Springer International Publishing",
	address="Buenos Aires, Argentina",
	month=nov,
	pages="561--574",
	abstract="Action anticipation, intent prediction, and proactive behavior are all desirable characteristics for autonomous driving policies in interactive scenarios. Paramount, however, is ensuring safety on the road---a key challenge in doing so is accounting for uncertainty in human driver actions without unduly impacting planner performance. This paper introduces a minimally-interventional safety controller operating within an autonomous vehicle control stack with the role of ensuring collision-free interaction with an externally controlled (e.g., human-driven) counterpart. We leverage reachability analysis to construct a real-time (100 Hz) controller that serves the dual role of (1) tracking an input trajectory from a higher-level planning algorithm using model predictive control, and (2) assuring safety through maintaining the availability of a collision-free escape maneuver as a persistent constraint regardless of whatever future actions the other car takes. A full-scale steer-by-wire platform is used to conduct traffic weaving experiments wherein the two cars, initially side-by-side, must swap lanes in a limited amount of time and distance, emulating cars merging onto/off of a highway. We demonstrate that, with our control stack, the autonomous vehicle is able to avoid collision even when the other car defies the planner's expectations and takes dangerous actions, either carelessly or with the intent to collide, and otherwise deviates minimally from the planned trajectory to the extent required to maintain safety.",
	isbn="978-3-030-33950-0"
}

@article{Fisac2019,
	abstract = {The proven efficacy of learning-based control schemes strongly motivates their application to robotic systems operating in the physical world. However, guaranteeing correct operation during the learning process is currently an unresolved issue, which is of vital importance in safety-critical systems. We propose a general safety framework based on Hamilton-Jacobi reachability methods that can work in conjunction with an arbitrary learning algorithm. The method exploits approximate knowledge of the system dynamics to guarantee constraint satisfaction while minimally interfering with the learning process. We further introduce a Bayesian mechanism that refines the safety analysis as the system acquires new evidence, reducing initial conservativeness when appropriate while strengthening guarantees through real-Time validation. The result is a least-restrictive, safety-preserving control law that intervenes only when the computed safety guarantees require it, or confidence in the computed guarantees decays in light of new observations. We prove theoretical safety guarantees combining probabilistic and worst-case analysis and demonstrate the proposed framework experimentally on a quadrotor vehicle. Even though safety analysis is based on a simple point-mass model, the quadrotor successfully arrives at a suitable controller by policy-gradient reinforcement learning without ever crashing, and safely retracts away from a strong external disturbance introduced during flight.},
	archivePrefix = {arXiv},
	arxivId = {1705.01292},
	author = {Fisac, Jaime F. and Akametalu, Anayo K. and Zeilinger, Melanie N. and Kaynama, Shahab and Gillula, Jeremy and Tomlin, Claire J.},
	doi = {10.1109/TAC.2018.2876389},
	eprint = {1705.01292},
	issn = {15582523},
	journal = {IEEE Transactions on Automatic Control},
	keywords = {Gaussian processes,Safety,autonomous systems,robot learning,robust optimal control},
	title = {{A General Safety Framework for Learning-Based Control in Uncertain Robotic Systems}},
	year = {2019},
	volume={64},
    number={7},
    pages={2737-2752},
}

@InProceedings{Le2019,
  title = 	 {Batch Policy Learning under Constraints},
  author = 	 {Le, Hoang and Voloshin, Cameron and Yue, Yisong},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {3703--3712},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/le19a/le19a.pdf},
  url = 	 {http://proceedings.mlr.press/v97/le19a.html},
  abstract = 	 {When learning policies for real-world domains, two important questions arise: (i) how to efficiently use pre-collected off-policy, non-optimal behavior data; and (ii) how to mediate among different competing objectives and constraints. We thus study the problem of batch policy learning under multiple constraints, and offer a systematic solution. We first propose a flexible meta-algorithm that admits any batch reinforcement learning and online learning procedure as subroutines. We then present a specific algorithmic instantiation and provide performance guarantees for the main objective and all constraints. As part of off-policy learning, we propose a simple method for off-policy policy evaluation (OPE) and derive PAC-style bounds. Our algorithm achieves strong empirical results in different domains, including in a challenging problem of simulated car driving subject to multiple constraints such as lane keeping and smooth driving. We also show experimentally that our OPE method outperforms other popular OPE techniques on a standalone basis, especially in a high-dimensional setting.}
}

@inproceedings{Torossian19a,
	title = 	 {$\mathcal{X}$-Armed Bandits: Optimizing Quantiles, CVaR and Other Risks},
	author = 	 {Torossian, L\'eonard and Garivier, Aur\'elien and Picheny, Victor},
	booktitle = 	 {Proceedings of The Eleventh Asian Conference on Machine Learning},
	pages = 	 {252--267},
	year = 	 {2019},
	editor = 	 {Lee, Wee Sun and Suzuki, Taiji},
	volume = 	 {101},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Nagoya, Japan},
	month = 	 {17--19 Nov},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v101/torossian19a/torossian19a.pdf},
	url = 	 {http://proceedings.mlr.press/v101/torossian19a.html},
	abstract = 	 {We propose and analyze StoROO, an algorithm for risk optimization on stochastic black-box functions derived from StoOO. Motivated by risk-averse decision making fields like agriculture, medicine, biology or finance, we do not focus on the mean payoff but on generic functionals of the return distribution. We provide a generic regret analysis of StoROO and illustrate its applicability with two examples: the optimization of quantiles and CVaR. Inspired by the bandit literature and black-box mean optimizers, StoROO relies on the possibility to construct confidence intervals for the targeted functional based on random-size samples. We detail their construction in the case of quantiles, providing tight bounds based on Kullback-Leibler divergence. We finally present numerical experiments that show a dramatic impact of tight bounds for the optimization of quantiles and CVaR.}
}

@techreport{Fraichard2014,
	TITLE = {{Will the Driver Seat Ever Be Empty?}},
	AUTHOR = {Fraichard, Thierry},
	URL = {https://hal.inria.fr/hal-00965176},
	TYPE = {Research Report},
	NUMBER = {RR-8493},
	INSTITUTION = {{INRIA}},
	YEAR = {2014},
	MONTH = Mar,
	PDF = {https://hal.inria.fr/hal-00965176/file/14-rr-fraichard.pdf},
	HAL_ID = {hal-00965176},
	HAL_VERSION = {v2},
}

@article{Fukushima2007,
	title = "Adaptive model predictive control for a class of constrained linear systems based on the comparison model",
	journal = "Automatica",
	volume = "43",
	number = "2",
	pages = "301 - 308",
	year = "2007",
	issn = "0005-1098",
	doi = "https://doi.org/10.1016/j.automatica.2006.08.026",
	url = "http://www.sciencedirect.com/science/article/pii/S000510980600375X",
	author = "Hiroaki Fukushima and Tae-Hyoung Kim and Toshiharu Sugie",
	keywords = "Model predictive control, Adaptive estimation, Constrained systems, Robust stability, Comparison principle",
	abstract = "This paper proposes an adaptive model predictive control (MPC) algorithm for a class of constrained linear systems, which estimates system parameters on-line and produces the control input satisfying input/state constraints for possible parameter estimation errors. The key idea is to combine the robust MPC method based on the comparison model with an adaptive parameter estimation method suitable for MPC. To this end, first, a new parameter update method based on the moving horizon estimation is proposed, which allows to predict an estimation error bound over the prediction horizon. Second, an adaptive MPC algorithm is developed by combining the on-line parameter estimation with an MPC method based on the comparison model, suitably modified to cope with the time-varying case. This method guarantees feasibility and stability of the closed-loop system in the presence of state/input constraints. A numerical example is given to demonstrate its effectiveness."
}

@article{Adetola2009,
	title = "Adaptive Model Predictive Control for Constrained Nonlinear Systems",
	journal = "IFAC Proceedings Volumes",
	volume = "41",
	number = "2",
	pages = "1946 - 1951",
	year = "2008",
	note = "17th IFAC World Congress",
	issn = "1474-6670",
	doi = "https://doi.org/10.3182/20080706-5-KR-1001.00331",
	url = "http://www.sciencedirect.com/science/article/pii/S1474667016392369",
	author = "Veronica Adetola and Martin Guay",
	abstract = "A true adaptive nonlinear model predictive control (MPC) algorithm must address the issue of robustness to model uncertainty while the estimator is evolving. Unfortunately, this may not be achieved without introducing extra degree of conservativeness and/or computational complexity in the controller calculations. To attenuate this problem, we employ a finite time identifier and propose an adaptive predictive control structure that reduces to a nominal MPC problem when exact parameter estimates are obtained. The adaptive MPC is formulated in such a way that useful excitation is automatically injected into the closed loop system to decrease the identification period."
}


@article{Aswani2013,
	abstract = {Controller design faces a trade-off between robustness and performance, and the reliability of linear controllers has caused many practitioners to focus on the former. However, there is renewed interest in improving system performance to deal with growing energy constraints. This paper describes a learning-based model predictive control (LBMPC) scheme that provides deterministic guarantees on robustness, while statistical identification tools are used to identify richer models of the system in order to improve performance; the benefits of this framework are that it handles state and input constraints, optimizes system performance with respect to a cost function, and can be designed to use a wide variety of parametric or nonparametric statistical tools. The main insight of LBMPC is that safety and performance can be decoupled under reasonable conditions in an optimization framework by maintaining two models of the system. The first is an approximate model with bounds on its uncertainty, and the second model is updated by statistical methods. LBMPC improves performance by choosing inputs that minimize a cost subject to the learned dynamics, and it ensures safety and robustness by checking whether these same inputs keep the approximate model stable when it is subject to uncertainty. Furthermore, we show that if the system is sufficiently excited, then the LBMPC control action probabilistically converges to that of an MPC computed using the true dynamics. {\textcopyright} 2013 Elsevier Ltd. All rights reserved.},
	author = {Aswani, Anil and Gonzalez, Humberto and Sastry, S. Shankar and Tomlin, Claire},
	doi = {10.1016/j.automatica.2013.02.003},
	issn = {00051098},
	journal = {Automatica},
	keywords = {Learning control,Predictive control,Robustness,Safety analysis,Statistics},
	title = {{Provably safe and robust learning-based model predictive control}},
	year = {2013},
	volume = {49},
	number = {5},
	pages = {1216-1226}
}

@Article{Lorenzen2017,
	author={Lorenzen, Matthias
	and Allg{\"o}wer, Frank
	and Cannon, Mark},
	title={Adaptive Model Predictive Control with Robust Constraint Satisfaction},
	journal={IFAC-PapersOnLine},
	year={2017},
	month={Jul},
	day={01},
	volume={50},
	number={1},
	pages={3313-3318},
	keywords={Model Predictive Control; Adaptive Control; Constraint Satisfaction Problems; Uncertain Linear Systems; System Identification},
	abstract={Adaptive control for constrained, linear systems is addressed and a solution based on Model Predictive Control (MPC) and set-membership system identification is presented. The paper introduces a computationally tractable solution which uses observations of past state and input trajectories to update the model and improve control performance while maintaining guaranteed constraint satisfaction and recursive feasibility. The developed approach is applied to a stabilizing MPC scheme and practical stability under persistent, additive disturbance is proved. A numerical example and brief comparison with non-adaptive MPC is provided.},
	issn={2405-8963}
}

@inproceedings{Lu2019,
	abstract = {An adaptive Model Predictive Control (adaptive MPC) strategy is proposed for linear systems with constant unknown model parameters, bounded additive disturbances and state and control constraints. By combining online set-based identification and robust tube MPC, the proposed controller reduces the conservativeness of constraint handling, guarantees recursive feasibility and provides asymptotic bounds on the closed loop system state that depend explicitly on the the identified parameter set. Computational tractability is ensured by using fixed complexity polytopic sets to bound the model parameters and predicted states. Convex conditions for persistence of excitation are considered. The results are illustrated by a numerical example.},
	author = {Lu, Xiaonan and Cannon, Mark},
	booktitle = {2019 American Control Conference (ACC)},
	month={10-12 July},
	address={ Philadelphia, PA, USA},
	isbn = {9781538679265},
	issn = {07431619},
	title = {{Robust adaptive tube model predictive control}},
	year = {2019},
	pages={3695-3701},
}

@INPROCEEDINGS{Kohler2019,
	author={J. {Köhler} and E. {Andina} and R. {Soloperto} and M. A. {Müller} and F. {Allgöwer}},
	booktitle={2019 IEEE 58th Conference on Decision and Control (CDC)},
	title={Linear robust adaptive model predictive control: Computational complexity and conservatism},
	year={2019},
	month=dec,
	address={Nice, France},
	pages={1383-1388},
}

@INPROCEEDINGS{Macek2009,
  author = {Ma{\^{c}}ek, Kristijan and Vasquez, Dizan and Fraichard, Thierry and Siegwart, Roland},
  booktitle={2008 11th International IEEE Conference on Intelligent Transportation Systems (ITSC)},
  title={Safe Vehicle Navigation in Dynamic Urban Scenarios},
  month={12-15 Oct},
  address={Beijing, China},
  year={2008},
  pages={482-489},
}

﻿@article{Bouraine2012,
	author={Bouraine, Sara
	and Fraichard, Thierry
	and Salhi, Hassen},
	title={Provably safe navigation for mobile robots with limited field-of-views in dynamic environments},
	journal={Autonomous Robots},
	year={2012},
	month=apr,
	day={01},
	volume={32},
	number={3},
	pages={267-283},
	abstract={This paper addresses the problem of navigating in a provably safe manner a mobile robot with a limited field-of-view placed in a unknown dynamic environment. In such a situation, absolute motion safety (in the sense that no collision will ever take place whatever happens in the environment) is impossible to guarantee in general. It is therefore settled for a weaker level of motion safety dubbed passive motion safety: it guarantees that, if a collision takes place, the robot will be at rest.},
	issn={1573-7527},
	doi={10.1007/s10514-011-9258-8},
}

@inproceedings{Bouraine2014,
	abstract = {This paper addresses the problem of planning the motion of a mobile robot with a limited sensory field-of-view in an unknown dynamic environment. In such a situation, the upper-bounded planning time prevents from computing a complete motion to the goal, partial motion planning is in order. Besides the presence of moving obstacles whose future behaviour is unknown precludes absolute motion safety (in the sense that no collision will ever take place whatever happens) is impossible to guarantee. The stance taken herein is to settle for a weaker level of motion safety called passive motion safety: it guarantees that, if a collision takes place, the robot will be at rest. The primary contribution of this paper is PassPMP, a partial motion planner enforcing passive motion safety. PassPMP periodically computes a passively safe partial trajectory designed to drive the robot towards its goal state. Passive motion safety is handled using a variant of the Inevitable Collision State (ICS) concept called Braking ICS, i.e. states such that, whatever the future braking trajectory of the robot, a collision occurs before it is at rest. Simulation results demonstrate how PassPMP operates and handles limited sensory field-of-views, occlusions and moving obstacles with unknown future behaviour. More importantly, PassPMP is provably passively safe.},
	author = {Bouraine, S. and Fraichard, Th and Azouaoui, O. and Salhi, Hassen},
	booktitle = {2014 IEEE International Conference on Robotics and Automation (ICRA), 31 May--7 June},
	month = jun,
	address = {Hong Kong, China},
	doi = {10.1109/ICRA.2014.6907375},
	issn = {10504729},
	title = {{Passively safe partial motion planning for mobile robots with limited field-of-views in unknown dynamic environments}},
	year = {2014},
    pages={3576-3582}
}

@article{Mitsch2017,
	author = {Stefan Mitsch and Khalil Ghorbal and David Vogelbacher and André Platzer},
	title ={Formal verification of obstacle avoidance and navigation of ground robots},
	journal = {The International Journal of Robotics Research},
	volume = {36},
	number = {12},
	pages = {1312-1340},
	year = {2017},
	doi = {10.1177/0278364917733549},
}

@Comment{Chap3,
	title = {Problem Statement},
}

@article{Kesting2007,
	author = {Arne Kesting and Martin Treiber and Dirk Helbing},
	title ={General Lane-Changing Model MOBIL for Car-Following Models},
	journal = {Transportation Research Record},
	volume = {1999},
	number = {1},
	pages = {86-94},
	year = {2007},
	doi = {10.3141/1999-10},
}



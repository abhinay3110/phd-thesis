%!TEX root = ../../PhD_thesis__Edouard_Leurent.tex

\graphicspath{{2-Chapters/4-Chapter/}}

\tikzset{
	state/.style={
		rectangle,
		draw=black, very thick,
		minimum height=2em,
		inner sep=2pt,
		text centered,
	},
	name plot/.style={every path/.style={name path global=#1}}
}

\chapter{Considering Social Interactions}
\label{chapter:4}

\begin{flushright}
	\begin{tabular}{@{}l@{}}
		\emph{O what a strange parcel of creatures are we,}\\
		\emph{Scarce ever to quarrel, or even agree;}\\
		\dots\dots\dots\dots\dots\dots\dots\dots\dots\dots\\
		\emph{Like social companions we never fall out,}\\
		\emph{Nor ever care what one anotherâ€™s about;}\\
	\end{tabular}
	
	Elizabeth Hands, \href{https://eleurent.github.io/sisyphe/texts/unsociable_family.html}{\enquote{On An Unsociable Family}}. \emph{The Death of Amnon}.
\end{flushright}

\abstractStartChapter{}%
Having detailed the \gls{MDP} model in \Cref{part:1}, we now study in \Cref{part:2} how model-free \glsxtrlong{RL} algorithms can learn an optimal behavioural planning policy. In this chapter, we focus on the design of \emph{sample-efficient} learning architectures, tailored for dense traffic situations. Such architectures should deal with a varying number of nearby vehicles, be invariant to the ordering chosen to describe them, while staying accurate and compact. We observe that the two most popular representations in the literature do not fit these criteria, and perform badly on an complex negotiation task. We propose an attention-based architecture that satisfies all these properties and explicitly accounts for interactions between the traffic participants. We empirically show that this architecture enjoys significant performance gains, and is able to capture interactions patterns that can be visualised and qualitatively interpreted.\footnote{This chapter is based on a preprint \citep{Leurent2019social} presented at the \emph{Machine Learning for Autonomous Driving workshop} at the NeurIPS 2019 conference. It is a collaboration with my friend and colleague Jean Mercat, who pursued this idea in further work on trajectory forecasting, which was published at the \emph{2020 International Conference on Robotics and Automation} \citep{Mercat2020} and won two international competitions.}
\minitocStartChapterNoNewPage{}

\section{Motivation}

Value-based \glsxtrlong{RL} algorithms such as Q-Learning \citep{Watkins1992} and its variants rely on estimating the optimal state-action value function $\Q^\star$. Since the state space $\cS$ chosen in \Cref{chapter:3} is continuous, we must resort to function approximation. Thus, independently of how $\cS$ was defined, we now have to specify how a state $s\in\cS$ will be \textit{represented} as an input to parametrised model $\gls+{Q}_\params$. The choice of both the model class and state representation will strongly influence the system performances. In particular, we claim that the two most-widely used representations both suffer from different drawbacks: on the one hand, the \emph{list of features} representation is compact and accurate but has a varying-size and depends on the choice of ordering. On the other hand, the \emph{spatial grid} representation addresses these concerns but in return suffers from an accuracy-size trade-off.

Our contributions are the following: first, we propose an attention-based architecture for decision-making involving social interactions. This architecture allows to satisfy the variable-size and permutation invariance requirements even when using a \emph{list of features} representation. It also naturally accounts for interactions between the ego-vehicle and any other traffic participant.
Second, we evaluate our model on a challenging intersection-crossing task involving up to 15 vehicles perceived simultaneously. We show that our proposed method provides significant quantitative improvements, and that it enables to capture interaction patterns in a way that is visually interpretable.

\subsection{Background}

\label{sec:background}

We start by giving some background on standard model-free learning algorithms --with a focus on value-based methods--, on usual state representations used for behavioural planning, and on attention mechanisms for \glspl{NN}.

\subsubsection{Value-based deep \glsxtrlong{RL}}

Recall from \Cref{def:optimality} that $Q^\star$ can be computed from an optimal policy $\optimalpolicy$ since $\Q^\star = \Q^{\optimalpolicy}$. However, the converse that $\optimalpolicy$ can be obtained from knowing $\Q^\star$ is also true since
\begin{proposition}[Optimality of the greedy policy, \citealp{Bellman1957}]
	\begin{leftbar}[propositionbar]
	A greedy policy defined as
	\begin{equation*}
	\forall s\in\cS, \optimalpolicy\parentheses{\cdot \mid s} = \dirac_{a^\star},\,\text{where } {a^\star}\in \argmax_a \Q^\star(s, a),
	\end{equation*}
	and $\dirac_x = \dirac(x-\cdot)$ denotes the Dirac distribution in $x$, is optimal.
	\end{leftbar}
\end{proposition}

Finding an optimal policy thus reduces to computing the optimal value function $Q^\star$. Fortunately,
\begin{theorem}[Bellman Optimality Equation, \citealp{Bellman1957}]
\label{thm:bellman-optimality-mdp}
\begin{leftbar}[theorembar]
The optimal action-value function $Q^\star$ satisfies the Bellman Optimality Equation:
\begin{equation*}
\Q^\star(s, a) = (\bo \Q^{\star}) (s, a) \eqdef \expectedvalueover{s'\sim \Ps(s'|s, a)} \max_{a'\in \cA} \left[\R(s, a) + \discount \Q^\star(s', a')\right].
\end{equation*}
Moreover, $\bo$ is a $\discount$-contraction for the $\|\cdot\|_\infty$ norm:
\begin{equation*}
\forall Q_1, \Q_2\in \Real^{\cS\times\cA},\; \|\bo Q_1 - \bo Q_2\|_\infty \leq \gamma \|Q_1 - Q_2\|_\infty.
\end{equation*}
\end{leftbar}
\end{theorem}

Thus, since $Q^{\star}$ is a fixed-point of a contracting operator, it can be computed by iteratively applying $\bo$ in a fixed-point iteration fashion. The \emph{Q-learning} algorithm \citep{Watkins1992} follows this procedure by applying a sampling version $\bo$ to a batch of collected experience. When dealing with a continuous state space $S$, we need to employ function approximation in order to generalise to nearby states. The  \gls{DQN} algorithm \citep{Mnih2015humanlevel} implements this idea by using a neural network model to represent the action-value function $Q$.

\subsubsection{Common traffic state representations}

In order to apply a reinforcement learning algorithm such as \gls{DQN} to an autonomous driving problem, a state space $S$ must first be chosen, that is, a representation of the scene. The state should at least contain a description of every nearby vehicle, when social interactions are relevant to the decision. We recall our definition \eqref{eq:state-space} of $\cS$ from \Cref{chapter:3}, in which a vehicle driving on a road is described by it's continuous position, heading and velocity, and the joint state of a road traffic with one ego-vehicle denoted $s_0$ and $N_v$ other vehicles can be described by a list of individual vehicle states:
\begin{equation*}
s = \begin{bmatrix}
p^x_0 & p^y_0 & v_0 & \psi_0\\
\vdots & \vdots & \vdots & \vdots\\
p^x_{N_v} & p^y_{N_v} & v_{N_v} & \psi_{N_v}\\
\end{bmatrix}
\in\gls+{cS} \eqdef\Real^{(N_v+1) \times 4}.
\end{equation*}

This description was appropriate to simply describe the system dynamics. However, it has several drawbacks when used for function approximation: because of its $2\pi$-periodicity, the heading $\psi_i$ is either clipped to $(-\pi, \pi]$ which causes a discontinuity at $\pm\pi$, or unclipped which causes several inputs to correspond to the same state. Likewise, the forward velocity $v_i$ needs to be combined with the heading $\psi_i$ and projected to inform the future positions $p^x_i,\,p^y_i$ of the vehicle $i$. Consequently, we slightly modify the features describing the vehicle states as
\begin{equation}
s = \left(s_i \right)_{i \in [0, N_v]}\qquad
\text{where}\qquad
s_i = \begin{bmatrix}
p^x_i & p^y_i & \dot{p}^x_i & \dot{p}^y_i & \cos\psi_i & \sin \psi_i
\end{bmatrix}
\label{eq:coordinates}
\end{equation}


\begin{figure}[tp]
	\centering
	\includegraphics[width=0.25\textwidth]{img/coordinates}
	\includegraphics[width=0.25\textwidth]{img/map}
	\caption{The \emph{list of features} (left) and \emph{spatial grid} (right) representations}
	\label{fig:representation}
\end{figure}

This representation, that we call \emph{list of features}, is illustrated in \Cref{fig:representation} (left) and was used for instance in \citep{Bai2015, Gindele2015, Song2016, Sunberg2017, Paxton2017, Galceran2017, Chen2017}.


This encoding is efficient in the sense that it uses the smallest quantity of information necessary to represent the scene. However, it lacks two important properties. First, its size varies with the number of vehicles which can be problematic for the sake of function approximation which often expects constant-sized inputs. Second, we expect a driving policy $\policy$ to be \emph{permutation invariant}, i.e. not to be dependent on the order in which other traffic participants are listed. Ideally, this property should be enforced and not approximated by relying on the coverage of the $N_v!$ possible permutations $\tau$ of any given traffic state in the dataset. Formally, we require that
\begin{equation}
\label{eq:permutation}
\policy(\cdot|(s_0, s_1,\dotsc,s_{N_v})) = \policy(\cdot|(s_0, s_{\tau(1)},\dotsc,s_{\tau(N_v)})), \quad\quad\quad \forall\tau \in \mathfrak{S}_{N_v},
\end{equation}
where $\mathfrak{S}_{N_v}$ is the \emph{symmetric} group of permutations of the integer range $[1,N_v]$.
A popular way to address this limitations is to use a \emph{spatial grid} representation. Instead of explicitly representing spatial information as variables $x, y$ along with other features $f$ directly inside a state $\{s_i=(p^x_i,p^y_i,f_i)\}_{i\in[0,N]}$ indexed on the vehicles, they are instead represented implicitly through the layout of several feature variables $f_{ij}$ organised in a tensor structure, where the $(i,j)$ indexes refer to a quantisation of the 2D-space. This representation is illustrated in \Cref{fig:representation} (right). Note that the size of this tensor is related to the area covered divided by the quantisation step, which reflects a trade-off between accuracy and dimensionality.
In an occupancy grid, the $f$ features contains presence information (0-1) and additional channels such as velocity and heading, as in \citep[e.g.][]{Isele2018, Fridman2018, Bansal2018, Rehder2017c}. Another example is the use of top-view RGB images \citep[e.g.][]{Bagnell2010, Rehder2017, Rehder2017c, Liu2018}.


This permutation invariance property \eqref{eq:permutation} can also be implemented within the architecture of the policy $\pi$. A general technique to achieve this is to treat each entity similarly in the early stages -- e.g. through weight sharing -- before reducing them with a projection operator that is itself invariant to permutations, for instance a max-pooling as in \citep{Chen2017,Hoel2018} or an average as in \citep{Qi2016}. A particular instance of this idea is attention mechanisms.


\subsubsection{Attention mechanisms} 

The attention architecture was introduced to enable \glspl*{NN}  to discover interdepen\-dencies within a variable number of inputs.
It has been used for pedestrian trajectory forecasting in~\citep{Vemula2018} with spatiotemporal graphs and in~\citep{Sadeghian2019CVPR} with spatial and social attention using a generative \glsxtrlong{NN}. In~\citep{Sadeghian2018ECCV}, attention over top-view road scene images for car trajectory forecasting is used. Multi-head attention mechanism has been developed in~\citep{Vaswani2017} for sentence translation. In~\citep{Messaoud2019} a mechanism called non-local multi-head attention is developed. However, this is a spatial attention that does not allow vehicle-to-vehicle attention. In the present chapter, we use a multi-head social attention mechanism to capture vehicle-to-ego dependencies and build varying input size and permutation invariance into the policy model.

\section{A social attention architecture}
\label{sec:architecture}

Out of a complex scene description, the model should be able to filter information and consider only what is relevant for decision. In other words, the agent should \emph{pay attention} to vehicles that are close or conflict with the planned route.

The proposed architecture is presented in \Cref{fig:ego-architecture}. It is used to represent the $\Q$-function that will be optimized by the \gls{DQN} algorithm. It is composed of a first linear encoding layer whose weights are shared between all vehicles. At that point, the embeddings only contain individual features of size $d_x$. They are then fed to an ego-attention layer, composed of several heads stacked together. The \emph{ego} prefix highlights that it similar to a multi-head self-attention layer \citep{Vaswani2017} but with only a single output corresponding to the ego-vehicle. Such an ego-attention head is illustrated in \Cref{fig:ego-attention} and works in the following way: in order to select a subset of vehicles depending on the context, the ego-vehicle  first emits a single query $Q = [q_0]\in\Real^{1 \times d_k}$, computed with a linear projection $L_q\in\Real^{d_x \times d_k}$ of its embedding. This query is then compared to a set of keys $K = [k_0, \dots, k_{N_v}]\in\Real^{(N_v+1) \times d_k}$ containing descriptive features $k_i$ for each vehicle, again computed with a shared linear projection $L_k\in\Real^{d_x \times d_k}$. The similarity between the query $q_0$ and any key $k_i$ is assessed by their dot product $q_0 k_i^T$. These similarities are then scaled by the inverse-square-root-dimension $1/\sqrt{d_k}$\footnote{This scaling is due to the fact that the dot-product of two independent random vectors with mean 0,  variance 1, and dimension $d_k$, is a random variable with mean 0 and variance $d_k$} and normalised with a softmax function $\sigma$ across vehicles. We obtain a stochastic matrix called the \emph{attention matrix}, which is finally used to gather a set of output value $V = [v_0, \dots, v_{N_v}]$, where each value $v_i$ is a feature computed with a shared linear projection $L_v\in\Real^{d_x \times d_v}$. Overall, the attention computation for each head can be written as
\begin{equation}
\text{output}=\underbrace{\sigma\left(\frac{QK^T}{\sqrt{d_k}}\right)}_{\text{attention matrix}}V.
\label{eq:selfattention}
\end{equation}
The outputs from all heads are finally combined with a linear layer, and the resulting tensor is then added to the ego encoding as in residual networks. We can easily see that this process is permutation invariant: indeed, a permutation $\tau$ will change the order of the rows in keys $K$ and values $V$ in \eqref{eq:selfattention} but will keep their correspondence. The final result is a dot product of values and key-similarities, which is independent of the ordering.


\begin{figure}[tp]
	\centering
	\begin{tikzpicture}
	\node(X1){ego};
	\node[below of=X1, node distance=0.7cm](X2){vehicle$_{1}$};
	\node[below of=X2, node distance=0.6cm](X3){$\vdots$};
	\node[below of=X3, node distance=0.7cm](X4){vehicle$_{N_v}$};
	
	\node[draw, right of=X1, node distance=1.8cm, rectangle](ENC1){Encoder};
	\node[draw, right of=X2, node distance=1.8cm, rectangle](ENC2){Encoder};
	\node[below of=ENC2, node distance=0.6cm](ENC3){$\vdots$};
	\node[draw, right of=X4, node distance=1.8cm, rectangle](ENC4){Encoder};
	
	\path (X1) edge (ENC1);
	\path (X2) edge (ENC2);
	\path (X4) edge (ENC4);
	
	
	\node[draw, rectangle, right of=ENC1, node distance=2.0cm, below=-0.4cm, fill=white](TRANS3){\rotatebox{90}{ Ego-attention }};
	\node[draw, rectangle, right of=ENC1, node distance=1.9cm, below=-0.3cm, fill=white](TRANS2){\rotatebox{90}{ Ego-attention }};
	\node[draw, rectangle, right of=ENC1, node distance=1.8cm, below=-0.2cm, fill=white](TRANS1){\rotatebox{90}{ Ego-attention }};
	
	\draw (ENC1.east) -| (TRANS1.west);
	\draw (ENC2.east) -| (TRANS1.west);
	\draw (ENC4.east) -| (TRANS1.west);
	
	
	\node[draw, right of=ENC1, node distance=3.6cm, rectangle](DEC1){Decoder};
	
	\draw (TRANS1.east) |- (DEC1.west);
	
	\node[right of=DEC1, node distance=2.3cm](Y1){action values};
	
	\draw (DEC1.east) -- (Y1.west);
	\end{tikzpicture}
	\caption{Block diagram of our model architecture. It is composed of several identical linear encoders, a stack of ego-attention heads, and a linear decoder.}
	\label{fig:ego-architecture}	
\end{figure}

\begin{figure}[tp]
	\centering
%	\begin{tikzpicture}[scale=1, every node/.style={scale=1}]
%	\node(X1){ego encoding};
%	\node[below of=X1, node distance=2cm](X2){$\vdots$};
%	\node[below of=X2, node distance=2cm](X3){vehicle$_{N_v}$ encoding};
%	
%	\coordinate[right of= X1, node distance=2cm](X1b){};
%	
%	\draw (X1) -- (X1b);
%	
%	\node[draw, right of=X1b, node distance=1cm](LK1){$L_{k}$};
%	\node[draw, below of=LK1, node distance=1cm](LV1){$L_{v}$};
%	\node[draw, above of=LK1, node distance=1cm](LQ1){$L_{q}$};
%	
%	\draw (X1b) -- (LQ1);
%	\draw (X1b) -- (LK1);
%	\draw (X1b) -- (LV1);
%	
%	
%	\node[right of=LQ1, node distance=1cm](Q1){$\mathbf{q}_0$};
%	\node[right of=LK1, node distance=1cm](K1){$\mathbf{k}_0$};
%	\node[right of=LV1, node distance=1cm](V1){$\mathbf{v}_0$};
%	
%	\draw (LQ1) -- (Q1);
%	\draw (LK1) -- (K1);
%	\draw (LV1) -- (V1);
%	
%	\coordinate[right of= X3, node distance=2cm](X3b){};
%	
%	\draw (X3) -- (X3b);
%	
%	\node[draw, right of=X3b, node distance=1cm](LK3){$L_{k}$};
%	\node[draw, below of=LK3, node distance=1cm](LV3){$L_{v}$};
%	
%	\draw (X3b) -- (LK3);	
%	\draw (X3b) -- (LV3);
%	
%	\node[right of=LK3, node distance=1cm](K3){$\mathbf{k}_{n}$};
%	\node[right of=LV3, node distance=1cm](V3){$\mathbf{v}_{n}$};
%	
%	\draw (LK3) -- (K3);
%	\draw (LV3) -- (V3);
%	
%	\coordinate[right of=Q1, node distance=0.3cm](TOP){};
%	\coordinate[right of=V3, node distance=0.3cm](BOT){};
%	\draw[decorate,decoration={brace}] (TOP) -- node[left=5pt]{} (BOT);
%	
%	\node[right of=X2, text width=3cm, node distance=5.5cm](EQ){
%		\footnotesize \[Q = \left( \begin{matrix}
%		\mathbf{q}_0
%		\end{matrix} \right)\]
%		\\
%		\footnotesize \[K = \left( \begin{matrix}
%		\mathbf{k}_0 \\
%		\vdots \\
%		\mathbf{k}_{n}
%		\end{matrix} \right)\]
%		\\
%		\footnotesize \[ V = \left( \begin{matrix}
%		\mathbf{v}_0 \\
%		\vdots \\
%		\mathbf{v}_{n}
%		\end{matrix}\right) \]
%	};
%	
%	\node[draw, right of=X2, node distance=8.3cm](EQ2){
%		\footnotesize $\sigma\left(\frac{QK^T}{\sqrt{d_k}}\right)V$};
%	
%	\coordinate[left of=EQ2, node distance=1.6cm, above=2.2cm](TOP2){};
%	\coordinate[left of=EQ2, node distance=1.5cm, below=0.0cm](MID2){};
%	\coordinate[left of=EQ2, node distance=1.6cm, below=2.2cm](BOT2){};
%	\draw[decorate,decoration={brace}] (TOP2) -- node[left=5pt]{} (BOT2);
%	\draw (MID2) -- (EQ2);
%	
%	\node[right of=EQ2, node distance=2cm](OUT){output};
%	\draw (EQ2) -- (OUT);
%	
%	\draw[draw=black, dashed] (1.75cm, -5.5cm) rectangle (9.5cm,1.5cm);
%	\end{tikzpicture}
	\includegraphics[width=\linewidth]{img/ego_attention}
	\caption{Architecture of an ego-attention head. After received the encoded vehicle states, the query $q_0$, keys $k_i$ and values $v_i$ are produced by three linear projections. Note that only the ego-vehicle produces a query $q$, while the keys $K$ and values $V$ are concatenated from all vehicles. Then, the attention matrix is computed by matching keys $K$ to the ego query $q_0$, and the corresponding values are retrieved. The resulting embedding is finally forwarded to a decoder to obtain the $Q$-values as an output.}
	\label{fig:ego-attention}
\end{figure}
% TODO: revoir lÃ©gende

\section{Experiments}
\paragraph{Environment}

In this experiment, we use the \href{https://github.com/eleurent/highway-env}{highway-env} environment \citep{highway-env} presented in \Cref{chapter:a}. We consider a task where vehicle-to-vehicle interaction plays a significant part: crossing a four-way intersection.
The scene -- composed of two roads crossing perpendicularly -- is populated with several traffic participants initialised with random positions, velocities, and destinations. As described in \Cref{chapter:3}, these vehicle are simulated with the Kinematic Bicycle Model, their lateral control is achieved by a low-level steering controller tracking a target route, and their longitudinal behaviour follows the \gls+{IDM} model \citep{Treiber2000}. However, this model only considers same-lane interactions and special care was required to prevent lateral collisions at the intersection. To that end, I implemented the following simplistic behaviour: each vehicle predicts the future positions of its neighbours over a three-seconds horizon by using a constant velocity model. In case of predicted collision with a neighbour, the yielding vehicle is determined based on road priorities and brakes until the collision prediction ceases. 

In this context, the agent must drive a vehicle by controlling its speed chosen from a finite set of actions $\gls+{cA} \eqdef \{\text{drive faster},\, \text{drive slower},\, \text{maintain speed}\}$. Note that the lane change actions have been removed from the general definition \eqref{eq:action-space} of \Cref{chapter:3} since the roads are all single-lane in this example. The lateral control is performed automatically by a low-level controller, such that the problem complexity is focused on the high-level interactions with other vehicles, namely the decision to either give or take way. The reward function $\gls+{reward}$ is defined as in \eqref{eq:reward-function}.

\paragraph{Agents}

We evaluate three different agents, whose characteristics are summarised in \Cref{tab:agents}.
\begin{itemize}
	\item \MLPL: a \emph{list of features} state representation is used, as described in \Cref{sec:background}. The model is a simple \gls{FCN}. Because this architecture requires a fixed-size input, we use zero-padding to fill the input tensor up to a maximum number $N=14$ of observed vehicles, and add an additional \emph{presence} feature to the coordinates described in \eqref{eq:coordinates} so as to identify active rows.
	\item \CNNG: a \emph{spatial grid} representation is used, as described in \Cref{sec:background}, with a $32 \times 32$ grid where each cell represents a $2\text{m}\times 2$m square. The model is a \gls{CNN}.
	\item \EgoAtt: a \emph{list of features} state representation is used along with the Ego-Attention architecture described in \Cref{sec:architecture}. As this model supports varying-size inputs, zero-padding is not required.
\end{itemize}

\begin{table}[tp]
	\centering
	\begin{threeparttable}
		\caption{Characteristics of the agents}
		\label{tab:agents}
		\begin{tabular}{lccc}
			\toprule
			Architecture & \MLPL & \CNNG & \EgoAtt \\
			\midrule 
			Input sizes & [15, 7] & [32, 32, 7] & [~$\boldsymbol{\cdot}$~, 7] \\
			Layers sizes & [128, 128] &  \makecell[tc]{Convolutional layers: 3 \\ Kernel Size: 2 \\
				Stride: 2 \\ Head: [20]} & \makecell[tl]{Encoder: [64, 64] \\Attention: 2 heads\\\phantom{Attention: }$d_k=32$ \\ Decoder: [64, 64]} \\
			Number of parameters & 3.0e4 & 3.2e4 & 3.4e4 \\
			Variable input size & No & No &  {Yes}  \\
			Permutation invariant & No & {Yes} &  {Yes} \\
			\bottomrule
		\end{tabular}
	\end{threeparttable}
\end{table}

These agents are all trained with the \gls{DQN} algorithm using the same hyperparameters, and their architectures are scaled to admit about the same number of trainable parameters for fair comparison.

\paragraph{Performances}

We plot in \Cref{fig:attention-results} the evolution of the total reward, episode length and average velocity during training, over 4000 episodes and repeated across 120 random seeds.
The \MLPL agent learns to accelerate to earn short-term rewards, as shown by its high average velocity, but fails to exploit the information of other vehicles and crashes often, leading to short episodes. We obtain a risky and blind policy that is the worst performing.
Conversely, the \CNNG architecture benefits from its invariance to permutations and manages to learn to brake upon arrival at the intersection to avoid collisions, as we can see from its higher episode length. However, it only proceeds when the intersection has been fully cleared, as reflected by its low average velocity. This results in an overly cautious policy -- a common trait colloquially known as the \emph{freezing robot problem} \citep{Trautman2010} -- with a slight increase in performance.
In stark contrast, the \EgoAtt policy quickly learns both when it must slow down at the intersection (see the high episode length), but also when it can exploit the gaps in the traffic and take way to vehicles that are far or slow enough (see the higher average velocity than \CNNG). This translates as a significant performance improvement, and the overall resulting behaviour is qualitatively more nuanced and human-like.

\begin{figure}[htp]
	\centering
	\begin{subfigure}[t]{0.6\linewidth}
		\includegraphics[width=\linewidth]{img/total_reward}
		\caption{Average return $\return=\sum_t\gamma^t\R(s_t,a_t)$ (higher is better)}
		\label{fig:attention-reward}
	\end{subfigure}
	\begin{subfigure}[t]{0.49\linewidth}
		\includegraphics[width=\linewidth]{img/length}
		\caption{Average episode length. Higher is better, since episodes are terminated at collisions (or after 13 steps).}
		\label{fig:attention-length}
	\end{subfigure}
	\begin{subfigure}[t]{0.49\linewidth}
		\includegraphics[width=\linewidth]{img/velocity}
		\caption{Average speed $v_0$ of the ego-vehicle (higher is better)}
		\label{fig:attention-velocity}
	\end{subfigure}
	\caption{Performances of the tree agents according to various measures. We display the mean values -- along with their 95\% confidence interval -- averaged over 120 random seeds.}
	\label{fig:attention-results}
\end{figure}

\paragraph{Attention interpretation}

In any given state, the attention matrix can be visualised in the following way: we connect the ego-vehicle to every vehicle by a line of width proportional to the corresponding attention weight. Since the architecture can contain several ego-attention heads, we use different colours to distinguish them. In our experiments, two attention heads were used and will be represented in green and blue. We observe in \Cref{fig:heads} that they specialised to focus on different areas: the green head is only watching the vehicles coming from the left, while the blue head restricts itself to vehicles in the front and right directions. However, we notice that both heads exhibit a common behaviour: they direct their attention to incoming vehicles that are likely to collide with the ego-vehicle, depending on their current position, heading, velocity, and ignore those that are too far or in a conflict-less situation. In particular, the attention tends to increase when vehicles get closer, as shown in \Cref{fig:distance}; also available in video \footnote{\href{https://eleurent.github.io/social-attention/assets/2\_distance.mp4}{https://eleurent.github.io/social-attention/assets/2\_distance.mp4}}. It can also be very sensitive to small variations in the traffic state, as reflected in \Cref{fig:sensitivity}. A full episode showcasing interactions with several vehicles is shown in \Cref{fig:episode}; a video is also available\footnote{\href{https://eleurent.github.io/social-attention/assets/1\_episode.mp4}{https://eleurent.github.io/social-attention/assets/1\_episode.mp4}}.


\begin{figure}[tp]
	\centering
	\begin{minipage}{.44\textwidth}
		\centering
		\includegraphics[width=\linewidth]{img/head_specialization}
		\captionof{figure}{The attention heads specialised in different areas: left and front/right.}
		\label{fig:heads}
	\end{minipage}
	\hfill
	\begin{minipage}{.545\textwidth}
		\centering
		\includegraphics[width=\linewidth]{img/distances}
		\captionof{figure}{The attention paid to a vehicle tends to increase as it gets closer.}
		\label{fig:distance}
	\end{minipage}
\end{figure}

\begin{figure}[tp]
	\centering
	\begin{subfigure}[t]{0.4\linewidth}
		\includegraphics[width=\linewidth]{img/watch1}
		\caption{The agent has stopped at the intersection, its attention is focused on an incoming vehicle whose destination is still uncertain.}
	\end{subfigure}
	\begin{subfigure}[t]{0.4\linewidth}
		\includegraphics[width=\linewidth]{img/watch2}
		\caption{As soon as the vehicle orientation changes, revealing its intention of turning right, the attention drops and the agent starts accelerating right away.}
	\end{subfigure}
	\caption{Sensitivity to uncertainty.}
	\label{fig:sensitivity}
\end{figure}

\begin{figure}[tp]
	\centering
	\includegraphics[width=.32\linewidth]{img/episode1}
	\includegraphics[width=.32\linewidth]{img/episode2}
	\includegraphics[width=.32\linewidth]{img/episode3}
	\\[0.12em]
	\includegraphics[width=.32\linewidth]{img/episode4}
	\includegraphics[width=.32\linewidth]{img/episode5}
	\includegraphics[width=.32\linewidth]{img/episode6}
	\caption[A complete episode.]{A complete episode. \emph{From left to right, top to bottom}: 
		\begin{enumerate*}
			\item The green and blue heads direct their attentions to the left and front vehicles, respectively;
			\item The left-vehicle is passing and is no longer a threat;
			\item Immediately, the green attention head switches to the next vehicle coming from the left;
			\item The front vehicle has now passed, and the blue attention head is now focused on the ego-vehicle;
			\item The ego-vehicle waits for one last vehicle coming from the left
			\item The ego-vehicle can finally proceed, and its attention is focused on itself.
		\end{enumerate*}
}
	\label{fig:episode}
\end{figure}

\paragraph{Exploiting interaction patterns}

The agent decisions regarding right of way are not enforced through rewards but interactions: based on the defined road priorities, some vehicles will take way to the ego-vehicle while others will not. By changing which is a priority road, we can influence the rules of interactions which affects the learnt behaviour. In \Cref{fig:priority}, we compare two policies placed in the exact same initial state and observe how their decisions are affected by their internal model of how incoming vehicles interact with them. This difference showcases the ability of our proposed architecture to discover and exploit such interaction patterns.

\begin{figure}[htp]
	\centering
	\begin{subfigure}[t]{0.48\linewidth}
		\includegraphics[width=\linewidth]{img/priority1}
		\caption{When trained on a non-priority road, the agent learns to yield to incoming vehicles.}
	\end{subfigure}
	\begin{subfigure}[t]{0.48\linewidth}
		\includegraphics[width=\linewidth]{img/priority2}
		\caption{When trained on a priority road, the agent expects other vehicles to give way and is consequently more aggressive.}
	\end{subfigure}
	\caption{Effect of the right of way.}
	\label{fig:priority}
\end{figure}

\paragraph{Goal conditioning}

In the previous examples, we trained a policy tailored for left-turns only because it is the hardest direction with the most conflict points and the lowest priority level. Two individual policies tailored for right turns and driving straight can be trained as well, with similar results. Training a generic intersection policy would be less efficient without any prior information on where the ego-vehicle is headed. To remedy this problem, the destination could be added as additional features in \eqref{eq:coordinates}, for instance encoded as a desired direction $(d_x, d_y)$. This destination feature could also be used for other traffic participants to encode blinker information when available. This should result in a more efficient and generic policy.

\section*{Chapter conclusion}

In this chapter, we showed that the \emph{list of features} representation, commonly used to describe vehicles in autonomous driving literature, is not tailored for use in a function approximation setting, in particular with neural networks. These concerns can be addressed by the \emph{spatial grid} representation, but at the price of an increased input size and loss of accuracy. In contrast, we proposed an attention-based neural network architecture to tackle the aforementioned issues of the \emph{list of features} representation without compromising either size or accuracy. This architecture enjoys a better performance on a simulated negotiation and intersection crossing task, and is also more interpretable thanks to the visualisation of the attention matrix. The resulting policy successfully learns to recognise and exploit the interaction patterns that govern the nearby traffic.

Let us back up a moment and reflect again on the behaviours exhibited by \Cref{fig:attention-results}. Though \EgoAtt performs better than \CNNG, it also has shorter episodes, which means it collides more often with other vehicles. Though this {aggressive} behaviour is deemed better by our choice of reward function, it may not be desirable in practice. A straightforward way to make the optimal policy more {conservative} is to manually tune the reward function and increase the weight of collisions. However, setting too high a penalty may also result in overly {cautious} behaviours. Finding the sweet spot between these two extremes can be difficult and demanding since changing the reward requires retraining the policy entirely. In the next chapter, we study a way to learn not a single policy but rather a whole range of policies that exhibit different levels of risk.


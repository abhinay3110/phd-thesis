%!TEX root = ../../PhD_thesis__Edouard_Leurent.tex

\makeatletter
\def\toclevel@chapter{-1}
\makeatother

%TODO: Discussion ?
\chapter{General Conclusion and Perspectives}
\label{chapter:conclusion}

\begin{flushright}
	\begin{tabular}{@{}l@{}}
		\emph{Nos Ã©quipiers}\\
		\emph{\hspace*{1.0cm}Sur les voies}\\
		\emph{\hspace*{0.5cm}Ralentissez}\\
	\end{tabular}

	Vinci Autoroutes\footnote{Writings collected by \href{https://twitter.com/pooredward/status/1273249408231124994}
		{@pooredward}.}\hspace*{1cm}
\end{flushright}

\section{Conclusion on our contributions}
In this thesis, we proposed a learning-based approach to the problem of behavioural planning for autonomous vehicles, with a focus on situations where several drivers are interacting. Following an in-breadth (\Cref{chapter:2}) as well as in-depth (\Cref{chapter:3}) initial investigation, we identified a set of key issues which make this problem challenging. We now recall each of these subjects, and precise how we strived to address them both in the model-free approach of \Cref{part:2} and the model-based approach of \Cref{part:3}.

%TODO highlight our most significant contributions

\paragraph{Coupled social dynamics}
In dense traffic, the dynamics of distinct vehicles are locally coupled, due to how drivers react and adapt to their surroundings. Consequently, predicting the course or acting in a driving scene requires a \emph{social awareness} skill: the ability to accurately understand and exploit these couplings.
In \Cref{chapter:4}, this skill was implemented through a \emph{social attention} mechanism in the policy architecture, which enables the agent to filter out irrelevant objects from a complex driving scene and retain only those that represent a risk of collision. In \Cref{chapter:5}, this coupling was made even more implicit by describing the motion of a vehicle $i$ through a dynamical model $\dot{x_i} = f_i(x)$ hat accepts the whole traffic state $x$ as an input.

\paragraph{Uncertainty due to human drivers}
Another key difficulty lies in the uncertainty of human behaviours. In \gls{RL}, the traditional approach to account for uncertainty is to incorporate stochasticity in the system dynamics, as we did in \Cref{chapter:5} where the objectives are formulated in terms of \emph{expected} rewards and costs. Incidentally, we also observed in \Cref{chapter:4} that our attention-based architecture is highly sensitive to ambiguous and disambiguated information, such as vehicles' destinations. In \Cref{chapter:7} however, we adopted another view and assumed that the dynamics were (close to) deterministic, but dependent on some unknown parameters --both continuous and discrete-- that could be estimated along the way.

\paragraph{Safety}
To deal with this uncertainty, three models of safety have been studied. In \Cref{chapter:5}, following the \gls{CMDP} framework, we formalised risk as the expected discounted sum of an additional cost signal $\constraint(s,a)$ --separate from the rewards $\reward(s,a)$-- constrained to remain below a threshold $\budget$. In \Cref{chapter:7}, we introduced a novel interval predictor allowing us to bound the set of reachable trajectories given the current parametric uncertainty over dynamics. This enabled us to cast safety as a robust stabilisation and constraint satisfaction problem, ensuring that the systems stays at all time within a safe space $\safestates\times\safecontrols$. Finally, to go beyond stabilisation problems, we considered a third formalisation of safety as a worst-case outcome, and proposed an algorithm for the minimax control of a generic reward function $\R(s,a)$.

\paragraph{Trade-off between safety and efficiency}
As we've just seen, safety is always defined with respect to some \emph{admissible} uncertainty. The larger the set of scenarios one is willing to consider and protect against, the more conservative they need to be to ensure safety. In particular, situations that require interacting with other agents are always susceptible to lead to accidents, when considering unlikely adversarial scenarios. In that sense, \emph{absolute safety is not achievable}, or only at the cost of usability. To strike the right level of safety, we need to consider the right level of uncertainty, the right set of outcomes. In \Cref{chapter:7}, the size of this ambiguity set to protect against can be controlled by adjusting the confidence level $\delta$ for continuous parameters (\eg driving style), and by adding or removing $(A,\phi)$-modelling assumptions from the multi-model extension, for discrete parameters (\eg potential destinations or lanes for a vehicle). In \Cref{chapter:5}, we embrace this trade-off even more explicitly. Rather than trying to adjust the scope and size of the uncertainty at its source, we instead directly control its effects on both the efficiency (rewards) of the policy and its safety (costs), by training a \emph{budgeted} policy $\budgetedpolicy$ that takes as input the desired level of risk $\confidence$.

\paragraph{Sample efficiency}
As for most reinforcement learning problems, we were concerned by minimising the number of samples required to reach optimality. To that end, we exploited the specificities and structures of the behavioural planning problem in several ways. In \Cref{chapter:4}, we embedded an inductive bias into the policy architecture by enforcing its invariance to permutations of the scene description, and observed that this fosters faster learning. In \Cref{chapter:7}, some structure was similarly imposed, on the dynamics model this time, in the form of a parametrised linear model which allowed to significantly reduce the dimension of the hypothesis space. We were also able to provide a bound on the simple regret relating the agent performance to the number of observed transition samples. In \Cref{chapter:6}, we looked into the sample efficiency of the planning procedure, and specifically tree-based planning algorithms. First, in the case of stochastic dynamics representing human behaviours, we proposed a modification of the \OLOP algorithm that improves its empirical sample complexity by an order on magnitude, while retaining its theoretical guarantees. Second, we showed that merging similar nodes in the lookahead tree enables to decrease the near-optimal branching factor featured in the regret bound of the algorithm. This translated as substantial empirical improvements in simple path planning tasks, where distinct sequences of actions lead to overlapping trajectories.


\section{Outstanding issues and perspectives}

In this section, I will adopt a more personal and subjective standpoint, and discuss which are the main barriers between research and industrialisation. Indeed, though we never intended for this thesis to lead directly to products, practical applications remains the long haul goal that motivates our work, and I must now examine our contributions again in this light.

A first and general concern of mine is that, beside the warm comfort of the well-behaved theoretical frameworks in which we place ourselves, the sheer complexity of the real world can be overwhelming. While any single aspect --partial observability, temporal abstraction, non-stationarity, risk aversion, you name it-- can be isolated and studied independently, the question of how to merge all these approaches into one single integrated product seems arduous, if not hopeless. Yet, any candidate algorithm not addressing any of these issues would be unfit.
In the sequel, I will not be so ambitious but reflect instead on a more reasonable question: are the methods that we developed likely to be suitable for a real-world application?

\paragraph{Reinforcement Learning in continuous states}

Let us start by our work in \Cref{part:2}. Following a model-free perspective in continuous states, we resorted function approximation using neural networks. Unfortunately, Deep Learning interacts with Reinforcement Learning algorithms in ways that are yet to be understood, but already infamous for their brittleness. In \Cref{chapter:4}, even our best policies after training still suffer a prohibitive rate of collisions of \SI{5}{\per}, considerably higher than the required performances. 
As we discussed, this may be attributed to the reward function that would not penalise collisions enough, but reward engineering is tedious and might in turn lead to over-conservative policies. The budgeted approaches of \Cref{chapter:5} were meant to address this issue, but at the price of an increased complexity, and our negative result of \Cref{thm:contraction} raises concerns over convergence in the general case. 
Another concern lies in our use of a very naive exploration policy --the $\epsilon$-greedy-- which takes random actions at a fixed frequency, a strategy widely considered as inefficient and damaging, yet which we had no choice but resorting to in the absence of a better solution. Guided exploration strategies tailored for regret minimisation, and especially following the \gls{OFU} principle, have been studied in the context of finite state-action spaces \citep{Auer2009,Azar2017}. 
These methods typically require the ability of counting the number of state visits, the question of how they can be extended to continuous spaces constitutes a promising research perspective. First steps have recently been made in that direction, by either relying on approximated \emph{pseudo-counts} \citep{Guyon2017}, or by deriving similar regret bounds under linear function approximation \citep{Jin2020}.

\paragraph{Trial without error?}

% In this thesis, we tried to solve behavioural planning in simulation, as a necessary step to solve it in real world. Thus treated simulation as our target task. 
Assume for a moment that the research community was able to solve the aforementioned problem and came up with exciting new algorithms for continuous state space with promising regret bounds. There remains an inevitable and fatal limitation: \emph{the foundations of \glsxtrlong{RL} are intrinsically based on trial and error}. Unfortunately, this is not an acceptable paradigm for the development of safety-critical problems such as \glsxtrlong{AD}. For reference, when we applied the model-free methods of \Cref{part:2} to very simple tasks, they converged in about 50k interaction samples, which represents about \SI{15}{\hour} of driving. Throughout training, the agents experienced about two thousand collisions. More generally, having vehicles \emph{exploring} on the roads among human drivers is morally inconceivable. Is there any chance at all to come up with learning algorithms that do not require actually causing accidents while training?

\paragraph{Safety guarantees}

As a first candidate, the line of work of Safe Control is committed to develop algorithms that are guaranteed never to reach an unsafe state, or with a provably bounded probability of failure. 
Likewise, in \Cref{chapter:7}, we managed to obtain some theoretical guarantees: a robust constraint satisfaction result, and a lower-bound on the worst-case outcome, that increases towards near-optimal performance with the number of samples. However, it is evident that these results are worth as much as their underlying assumptions, which could be: not much. Indeed, it seems very dubious that the complexity of human behaviours can be accurately described by our linear dynamics \Cref{assumpt:structure}, and our own proposed system largely overlooks large parts of the driving task. With \Cref{assumpt:feasible-constr}, the safe region $\safestates$ cannot be chosen freely, but must contain the basin of attraction $\safestates_{f}$ whose size grows with uncertainty. Finally, the assumptions of \Cref{thm:control-error} do not hold even in our simple simulations: the behaviours of observed vehicles are not persistently excited (\ie constantly braking before their leading vehicle and changing lanes). More generally, safety analyses can never protect against unmodeled events: a tree or a package falling down the road. Yet, having to model the world is daunting, especially since theoretical analysis imposes an additional constraint on the modelling effort, often at the expense of empirical performance. This may explain why practical achievements often precede their formal analysis. In the end, do we prefer relying on a simpler model that we can analyse under some assumptions unlikely to hold in practice, or more complex black-box models which tend to perform better on experimental benchmarks? Fortunately, all is not bleak and it has been observed in countless occasions that even when the guarantees do not hold, the founding principles of an algorithm can lead to designs that still exhibit the desired properties, and can robustly generalise. An example is the reliable of control systems in the aerospace industry, which even though aircraft dynamics are not \emph{really} linear and measurement noises are not \emph{really} Gaussian.

% TODO: still, current safety frameworks are not well suited to AD. Stabilisation: meh. Safe states: meh.
% Perspective: investigate new ones (which ?)
% But always the same problem: guaranteed protection against rare events = conservativeness.

\paragraph{Simulation and beyond?}

Another obvious way to avoid trial-and-error in the real world is to rely on simulation. Of course, the effort of modelling a complex world remains, but dropping the analysability requirement significantly relaxes the constraints on how this description can be expressed. We can safely expect that simulations will continue to play an increasingly significant part in \gls{AD} technologies, for both offline pre-training and online planning. This can be the occasion to divert our research efforts from the traditional regret minimisation objective, which does not make much sense in a simulated environments where rewards are free but samples are costly. In contrast, a promising research direction is the study of the more appropriate Pure Exploration setting, which aims at relating the policy suboptimality to the number of samples. I already took part in collaborations exploring this direction \citep{Jonsson2020planning,Kaufmann2020adaptive,Menard2020Fast} and hope to pursue this path further. 
Finally, relying on simulation introduces the additional question of how to adapt knowledge from simulation to the real world. A fine-tuning training process in real conditions would involve experiencing real failures again, though hopefully in reduced numbers, which could be realistic under human interventions \citep{Saunders2018,Kendall2019}. Another path of interest to me could be to leverage offline \gls{RL} methods \citep{Thomas2015,Laroche2019}, that could enable to safely improve pre-trained policies around nominal states in a dataset of real driving data.

@book{Howard1960,
author = {R. A. Howard},
title = {Dynamic Programming and Markov Processes},
publisher = {MIT},
year = {1960}
}

@article{Chaslot2008,
	author = {Chaslot, Guillaume and Winands, Mark and Herik, H. and Uiterwijk, Jos and Bouzy, Bruno},
	year = {2008},
	month = {11},
	pages = {343-357},
	title = {Progressive Strategies for Monte-Carlo Tree Search},
	volume = {04},
	journal = {New Mathematics and Natural Computation},
	doi = {10.1142/S1793005708001094}
}

@inproceedings{Coulom2007,
	TITLE = {{Computing Elo Ratings of Move Patterns in the Game of Go}},
	AUTHOR = {Coulom, R{\'e}mi},
	URL = {https://hal.inria.fr/inria-00149859},
	BOOKTITLE = {{Computer Games Workshop}},
	ADDRESS = {Amsterdam, Netherlands},
	EDITOR = {van den Herik, H. Jaap and Mark Winands and Jos Uiterwijk and Maarten Schadd},
	YEAR = {2007},
	MONTH = Jun,
	PDF = {https://hal.inria.fr/inria-00149859/file/MMGoPatterns.pdf},
	HAL_ID = {inria-00149859},
	HAL_VERSION = {v1},
}

@incollection{Wang2009,
	title = {Algorithms for Infinitely Many-Armed Bandits},
	author = {Yizao Wang and Jean-yves Audibert and R\'{e}mi Munos},
	booktitle = {Advances in Neural Information Processing Systems 21},
	editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
	pages = {1729--1736},
	year = {2009},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/3452-algorithms-for-infinitely-many-armed-bandits.pdf}
}

@inproceedings{Bubeck2010,
	author = {Bubeck, S{\'{e}}bastien and Munos, R{\'{e}}mi},
	booktitle = {COLT 2010 - The 23rd Conference on Learning Theory, June 27--29},
	title = {{Open Loop Optimistic Planning}},
	year = {2010},
	pages = {477-489},
	month = jun,
	address = {Haifa, Israel},
}

@inproceedings{Garivier2011,
  title={The KL-UCB algorithm for bounded stochastic bandits and beyond},
  author={Garivier, Aur{\'e}lien and Capp{\'e}, Olivier},
  booktitle={Proc. of COLT},
  year={2011}
}

@article{Cappe2013,
author = {Capp{\'{e}}, Olivier and Garivier, Aur{\'{e}}lien and Maillard, Odalric-Ambrym and Munos, R{\'{e}}mi and Stoltz, Gilles},
journal = {The Annals of Statistics},
number = {3},
pages = {1516--1541},
title = {{Kullback-Leibler Upper Confidence Bounds for Optimal Sequential Allocation}},
volume = {41},
year = {2013}
}

@book{Bellman1957,
author = {Bellman, Richard},
title = {Dynamic Programming},
year = {2010},
isbn = {0691146683},
publisher = {Princeton University Press},
address = {USA}
}

@inproceedings{Kearns2002,
author = {Kearns, Michael and Mansour, Yishay and Ng, Andrew Y.},
booktitle = {Proc. of IJCAI},
title = {{A sparse sampling algorithm for near-optimal planning in large Markov decision processes}},
year = {2002}
}

@article{Hren2008,
author = {Hren, Jean Fran{\c{c}}ois and Munos, R{\'{e}}mi},
journal = {Lecture Notes in Computer Science},
title = {{Optimistic planning of deterministic systems}},
year = {2008}
}

@inproceedings{Szorenyi2014,
author = {Szorenyi, B. and Kedenburg, G. and Munos, R.},
booktitle = {Proc. of NeurIPS},
title = {{Optimistic planning in Markov decision processes using a generative model}},
year = {2014}
}

@inproceedings{Grill2016,
author = {Grill, Jean-Bastien and Valko, Michal and Munos, R{\'{e}}mi},
booktitle = {Proc. of NeurIPS},
title = {{Blazing the trails before beating the path: Sample-efficient Monte-Carlo planning}},
year = {2016}
}

@misc{gym_minigrid,
  author = {Chevalier-Boisvert, Maxime and Willems, Lucas and Pal, Suman},
  title = {Minimalistic Gridworld Environment for OpenAI Gym},
  year = {2018},
  howpublished = {\url{https://github.com/maximecb/gym-minigrid}},
}

@article{Munos2014,
author = {Munos, Remi},
journal = {Foundations and Trends{\textregistered} in Machine Learning},
title = {{From Bandits to Monte-Carlo Tree Search: The Optimistic Principle Applied to Optimization and Planning}},
year = {2014}
}

@article{Busoniu2013,
author = {Buşoniu, Lucian and Daniels, Alexander and Munos, Remi and Babuska, Robert},
journal = {2013 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL),  16-19 April},
month = apr,
address = {Singapore},
title = {{Optimistic planning for continuous-action deterministic systems}},
year = {2013},
pages={69-76},
}

@article{Busoniu2018,
title = "Continuous-action planning for discounted infinite-horizon nonlinear optimal control with Lipschitz values",
journal = "Automatica",
volume = "92",
pages = "100 - 108",
year = "2018",
issn = "0005-1098",
doi = "https://doi.org/10.1016/j.automatica.2018.03.009",
url = "http://www.sciencedirect.com/science/article/pii/S000510981830089X",
author = "Lucian Buşoniu and Előd Páll and Rémi Munos",
keywords = "Optimal control, Planning, Nonlinear systems, Near-optimality analysis",
abstract = "We consider discrete-time, infinite-horizon optimal control problems with discounted rewards. The value function must be Lipschitz continuous over action (input) sequences, the actions are in a scalar interval, while the dynamics and rewards can be nonlinear/nonquadratic. Exploiting ideas from artificial intelligence, we propose two optimistic planning methods that perform an adaptive-horizon search over the infinite-dimensional space of action sequences. The first method optimistically refines regions with the largest upper bound on the optimal value, using the Lipschitz constant to find the bounds. The second method simultaneously refines all potentially optimistic regions, without explicitly using the bounds. Our analysis proves convergence rates to the global infinite-horizon optimum for both algorithms, as a function of computation invested and of a measure of problem complexity. It turns out that the second, simultaneous algorithm works nearly as well as the first, despite not needing to know the (usually difficult to find) Lipschitz constant. We provide simulations showing the algorithms are useful in practice, compare them with value iteration and model predictive control, and give a real-time example."
}

@article{Silver2016,
title	= {Mastering the game of Go with deep neural networks and tree search},
author	= {David Silver and Aja Huang and Christopher J. Maddison and Arthur Guez and Laurent Sifre and George van den Driessche and Julian Schrittwieser and Ioannis Antonoglou and Veda Panneershelvam and Marc Lanctot and Sander Dieleman and Dominik Grewe and John Nham and Nal Kalchbrenner and Ilya Sutskever and Timothy Lillicrap and Madeleine Leach and Koray Kavukcuoglu and Thore Graepel and Demis Hassabis},
year	= {2016},
journal	= {Nature},
pages	= {484--503},
volume	= {529}
}

@article{Silver2017,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={Nature},
  volume={550},
  number={7676},
  pages={354},
  year={2017},
  publisher={Nature Publishing Group}
}

@article {Silver2018,
	author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
	title = {A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
	volume = {362},
	number = {6419},
	pages = {1140--1144},
	year = {2018},
	publisher = {American Association for the Advancement of Science},
	issn = {0036-8075},
	journal = {Science}
}

@article{Kearns02SS,
	author    = {Michael J. Kearns and
	Yishay Mansour and
	Andrew Y. Ng},
	title     = {A Sparse Sampling Algorithm for Near-Optimal Planning in Large Markov
	Decision Processes},
	journal   = {Machine Learning},
	volume    = {49},
	number    = {2-3},
	pages     = {193--208},
	year      = {2002}
}

@inproceedings{Coulom2006,
	TITLE = {{Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search}},
	AUTHOR = {Coulom, R{\'e}mi},
	BOOKTITLE = {{Proc. of ICCG}},
	YEAR = {2006},
}

@inproceedings{Hren2008optimistic,
	title = {{Optimistic planning of deterministic systems}},
	year = {2008},
	booktitle = {Proc. of EWRL},
	author = {Hren, Jean-Francois and Munos, Rémi},
}

@inproceedings{Kocsis06UCT,
	title                    = {Bandit Based Monte-carlo Planning},
	author                   = {Kocsis, Levente and Szepesv\'{a}ri, Csaba},
	booktitle                = {Proc. of ECML},
	year                     = {2006}
}

@article{Coquelin2007,
	author = {Coquelin, Pierre-Arnaud and Munos, R{\'{e}}mi},
	title = {{Bandit Algorithms for Tree Search}},
	journal = {Proc. of UAI},
	year = {2007}
}

@book{Munos14,
	title                    = {From bandits to Monte-Carlo Tree Search: The optimistic principle applied to optimization and planning.},
	author                   = {Munos, R.},
	publisher                = {Foundations and Trends in Machine Learning},
	year                     = {2014},
	volume                   = {7}
}

@article{Browne12,
	title                    = {A Survey of Monte Carlo Tree Search Methods},
	author                   = {Browne, C. and Powley, E. and Whitehouse, D. and Lucas, S. and Cowling, P. and Rohlfshagen, P. and Tavener, S. and Perez, D. and Samothrakis, S. and Colton, S.},
	journal                  = {IEEE Transactions on Computational Intelligence and AI in games,},
	year                     = {2012},
	number                   = {1},
	pages                    = {1-49},
	volume                   = {4}
}


@InProceedings{Szorenyi14,
	Title                    = {Optimistic Planning in Markov Decision Processes using a generative model},
	Author                   = {Szorenyi, B. and Kedenburg, G. and Munos, R.},
	Booktitle                = {Advances in NIPS},
	Year                     = {2014}
}

@article{Feldman14BRUE,
	author    = {Zohar Feldman and Carmel Domshlak},
	title     = {Simple Regret Optimization in Online Planning for Markov Decision Processes},
	journal   = {Journal of Artifial Intelligence Research},
	volume    = {51},
	pages     = {165--205},
	year      = {2014}
}

@InProceedings{Grill16,
	Title                    = {Blazing the trails before beating the path: Sample-efficient Monte-Carlo planning},
	Author                   = {Grill, J.-B. and Valko, M. and Munos, R.},
	Booktitle                = {Advances in NIPS},
	Year                     = {2016}
}

@inproceedings{Grill19,
	author    = {Jean{-}Bastien Grill and
	Omar Darwiche Domingues and
	Pierre M{\'{e}}nard and
	R{\'{e}}mi Munos and
	Michal Valko},
	title     = {Planning in entropy-regularized Markov decision processes and games},
	booktitle = {Advances in NeurIPS},
	year      = {2019}
}

@article{Silver18,
	author    = {David Silver and
	Thomas Hubert and
	Julian Schrittwieser and
	Ioannis Antonoglou and
	Matthew Lai and
	Arthur Guez and
	Marc Lanctot and
	Laurent Sifre and
	Dharshan Kumaran and
	Thore Graepel and
	Timothy P. Lillicrap and
	Karen Simonyan and
	Demis Hassabis},
	title     = {A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
	journal   = {Science},
	volume    = {362},
	issue = {6419},
	page = {1140-1144},
	year      = {2018},
}

@inproceedings{Kaufmann2017,
	title = {Monte-Carlo Tree Search by Best Arm Identification},
	author = {Kaufmann, Emilie and Koolen, Wouter M},
	booktitle = {Advances in NIPS},
	pages = {4897--4906},
	year = {2017},
}

@inproceedings{Leurent2019practical,
	title={Practical Open-Loop Optimistic Planning},
	author={Edouard Leurent and Odalric-Ambrym Maillard},
	year={2019},
	booktitle={Proc. of ECML-PKDD}
}

@InProceedings{Busoniu2012optimistic,
  title = 	 {Optimistic planning for Markov decision processes},
  author = 	 {Lucian Buşoniu and Remi Munos},
  booktitle = 	 {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {182--189},
  year = 	 {2012},
  editor = 	 {Neil D. Lawrence and Mark Girolami},
  volume = 	 {22},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {La Palma, Canary Islands},
  month = 	 {21--23 Apr},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v22/busoniu12/busoniu12.pdf},
  url = 	 {http://proceedings.mlr.press/v22/busoniu12.html},
  abstract = 	 {The reinforcement learning community has recently intensified its interest in online planning methods, due to their relative independence on the state space size. However, tight near-optimality guarantees are not yet available for the general case of stochastic Markov decision processes and closed-loop, state-dependent planning policies. We therefore consider an algorithm related to AO* that optimistically explores a tree representation of the space of closed-loop policies, and we analyze the near-optimality of the action it returns after n tree node expansions. While this optimistic planning requires a finite number of actions and possible next states for each transition, its asymptotic performance does not depend directly on these numbers, but only on the subset of nodes that significantly impact near-optimal policies. We characterize this set by introducing a novel measure of problem complexity, called the near-optimality exponent. Specializing the exponent and performance bound for some interesting classes of MDPs illustrates the algorithm works better when there are fewer near-optimal policies and less uniform transition probabilities.}
}

@inproceedings{Filippi2010optimism,
	Title                    = {{Optimism in Reinforcement Learning and {K}ullback-{L}eibler Divergence}},
	Author                   = {Filippi, S. and Capp{\'e}, O. and Garivier, A.},
	Booktitle                = {{Allerton Conference on Communication, Control, and Computing}},
	Year                     = {2010},
}

@misc{Vanderbei1996,
	author = {Vanderbei, Robert},
	title = {Optimal sailing strategies, statistics and operations research program},
	year = {1996},
	publisher={University of Princeton},
	howpublished = {\url{https://vanderbei.princeton.edu/sail/sail.html}}
}

@inproceedings{Bubeck2009,
	title = {Online Optimization in X-Armed Bandits},
	author = {S\'{e}bastien Bubeck and Gilles Stoltz and Csaba Szepesv\'{a}ri and R\'{e}mi Munos},
	booktitle = {Advances in Neural Information Processing Systems 21},
	editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
	pages = {201--208},
	year = {2009},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/3605-online-optimization-in-x-armed-bandits.pdf}
}

@inproceedings{Munos2011,
	title = {Optimistic Optimization of a Deterministic Function without the Knowledge of its Smoothness},
	author = {R\'{e}mi Munos},
	booktitle = {Advances in NIPS},
	year = {2011},
}

@InProceedings{Huang2017,
	title = 	 {Structured Best Arm Identification with Fixed Confidence},
	author = 	 {Ruitong Huang and Mohammad M. Ajallooeian and Csaba Szepesvári and Martin Müller},
	booktitle = {Proc. of ALT},
	pages = 	 {593--616},
	year = 	 {2017},
	volume = 	 {76},
	publisher = 	 {PMLR},
}

@inproceedings{Grill2019,
	author    = {Jean{-}Bastien Grill and
	Omar Darwiche Domingues and
	Pierre M{\'{e}}nard and
	R{\'{e}}mi Munos and
	Michal Valko},
	title     = "{Planning in entropy-regularized Markov decision processes and games}",
	booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
	year      = {2019}
}


@inproceedings{Hostetler14,
	author = {Hostetler, Jesse and Fern, Alan and Dietterich, Tom},
	title = {State Aggregation in Monte Carlo Tree Search},
	year = {2014},
	booktitle = {Proceedings of AAAI},
}

@inproceedings{Ballesteros2013,
	abstract = {In this paper, we introduce an approach called FSBS (Forward Search in Belief Space) for online planning in POMDPs. The approach is based on the RTBSS (Real-Time Belief Space Search) algorithm of [1]. The main departure from the algorithm is the introduction of similarity measures in the belief space. By considering statistical divergence measures, the similarity between belief points in the forward search tree can be computed. Therefore, it is possible to determine if a certain belief point (or one very similar) has been already visited. This way, it is possible to reduce the complexity of the search by not expanding similar nodes already visited in the same depth. This reduction of complexity makes possible the real-time implementation of more complex problems in robots. The paper describes the algorithm, and analyzes different divergence measures. Benchmark problems are used to show how the approach can obtain a ten-fold reduction in the computation time for similar obtained rewards when compared to the original RTBSS. The paper also presents experiments with a quadrotor in a search application. {\textcopyright} 2013 IEEE.},
	author = {Ballesteros, Joaquin and Merino, Luis and Trujillo, Miguel Angel and Viguria, Antidio and Ollero, Anibal},
	booktitle = {Proceedings of ICRA},
	title = {{Improving the efficiency of online POMDPs by using belief similarity measures}},
	year = {2013}
}
%!TEX root = ../../PhD_thesis__Edouard_Leurent.tex

\makeatletter
\def\toclevel@chapter{-1}
\makeatother

\chapter{General Conclusion and Perspectives}
\label{chapter:conclusion}

\begin{flushright}
	\begin{tabular}{@{}l@{}}
		\emph{Nos Ã©quipiers}\\
		\emph{\hspace*{1.0cm}Sur les voies}\\
		\emph{\hspace*{0.5cm}Ralentissez}\\
	\end{tabular}

	Vinci Autoroutes\footnote{Writings collected by \href{https://twitter.com/pooredward/status/1273249408231124994}
		{@pooredward}.}\hspace*{1cm}
\end{flushright}

\section{Conclusion on our contributions}
In this thesis, we proposed a learning-based approach to the problem of behavioural planning for autonomous vehicles, with a focus on scenes with several drivers interacting. Following an in-breadth (\Cref{chapter:2}), as well as in-depth (\Cref{chapter:3}), initial investigation, we identified a set of key issues which make this problem challenging. We now recall these subjects, and precise how we strived to address them both in the model-free approach of \Cref{part:2} and the model-based approach of \Cref{part:3}.

\paragraph{Coupled social dynamics}
In dense traffic, the dynamics of distinct vehicles are locally coupled, due to how drivers react and adapt to their surroundings. Consequently, predicting the course or acting in a driving scene requires a \emph{social awareness} skill: the ability to accurately understand and exploit these couplings.
In \Cref{chapter:4}, this function was performed by a \emph{social attention} mechanism in the policy architecture, which enables the agent to filter out irrelevant objects from a complex driving scene and retain only those that represent a risk of collision. In \Cref{chapter:5}, this coupling was made explicit by describing the motion of a vehicle $i$ through a dynamical model $\dot{x_i} = f_i(x)$ taking the whole traffic state $x$ as input.

\paragraph{Uncertainty due to human drivers}
Another critical difficulty lies in the uncertainty of human behaviours. In \gls{RL}, the traditional approach to account for uncertainty is to incorporate stochasticity in the system dynamics, as we did in \Cref{chapter:5} where the objectives are formulated in terms of \emph{expected} rewards and costs. Incidentally, we also observed in \Cref{chapter:4} that our attention-based architecture is highly sensitive to ambiguous and disambiguated information, such as vehicles' destinations. In \Cref{chapter:7} however, we adopted another view and assumed that the dynamics were (close to) deterministic, but dependent on some \emph{unknown} parameters --both continuous and discrete-- that could be estimated along the way.

\paragraph{Safety}
To deal with this uncertainty, we studied three models of safety. In \Cref{chapter:5}, following the \gls{CMDP} framework, we formalised risk as the expected discounted sum of an additional \emph{cost signal} $\constraint(s,a)$ --separate from the rewards $\reward(s,a)$-- \emph{constrained} to remain below a threshold $\budget$. In \Cref{chapter:7}, we introduced a novel \emph{interval predictor} allowing us to bound the set of reachable trajectories given the current parametric uncertainty over the dynamics, which required circumventing the instability of previous methods. This enabled us to cast safety as a \emph{robust stabilisation} and \emph{constraint satisfaction} problem, ensuring that the system stays at all time within a safe space $\safestates\times\safecontrols$. Finally, to go beyond stabilisation problems, we considered a third formalisation of safety as a \emph{worst-case} outcome, and proposed an algorithm for the minimax control of a \emph{generic} reward function $\R(s,a)$. In addition to being tractable, each component of this algorithm is theoretically grounded, allowing us to obtain an end-to-end guarantee that the best performance predicted during planning is achievable on the underlying system, as well as the first regret bound in this setting, extending the state-of-the-art so far limited to quadratic costs.

\paragraph{Trade-off between safety and efficiency}
As we just saw, safety is always defined with respect to some \emph{admissible} uncertainty. The larger the set of scenarios one is willing to consider and protect against, the more conservative they need to be to ensure safety. In particular, situations that require interacting with other agents are always susceptible to lead to accidents, when considering (unlikely) adversarial scenarios. In that sense, \emph{absolute safety is not achievable}, or only at the cost of usability. To strike the right level of safety, we need to consider the right level of uncertainty, the right set of possible outcomes. In \Cref{chapter:7}, the size of this ambiguity set is controlled by adjusting the confidence level $\confidence$ for continuous parameters (\eg driving style), and by adding or removing $(A,\features)$-modelling assumptions from the multi-model extension, for discrete parameters (\eg potential destinations or lanes for a vehicle). In \Cref{chapter:5}, we embrace this trade-off even more explicitly. Rather than trying to adjust the scope and size of the uncertainty, we instead directly control its influence on both the efficiency of the policy (rewards) and its safety (costs), by training a \emph{budgeted} policy $\budgetedpolicy$ that takes as input the desired level of risk $\confidence$. While this setting was previously studied for finite states and known dynamics only, we extended it to continuous states and unknown dynamics.

\paragraph{Sample efficiency}
As for most reinforcement learning problems, we were also concerned about minimising the number of samples required to reach optimality. To that end, we exploited the specificities and structures of the behavioural planning problem in several ways. In \Cref{chapter:4}, we embedded an inductive bias into the policy architecture by enforcing its invariance to permutations of the scene description, and observed that this fastens learning. In \Cref{chapter:7}, some structure was similarly imposed --on the dynamics model this time-- in the form of a parametrised linear model, which allowed to reduce the dimension of the hypothesis space significantly. We were also able to provide a bound on the simple regret relating the agent performance to the number of observed transition samples. In \Cref{chapter:6}, we looked into the sample efficiency of the planning procedure, and specifically tree-based planning algorithms. First, in the case of stochastic dynamics representing human behaviours, we proposed a modification of the \OLOP algorithm that improves its empirical sample complexity by an order on magnitude, while retaining its theoretical guarantees. Second, we showed that merging similar nodes in the lookahead tree enables to decrease the near-optimal branching factor featured in the regret bound of the algorithm. This translated into substantial empirical improvements in simple path planning tasks, where distinct sequences of actions lead to overlapping trajectories.


\section{Outstanding issues and perspectives}

In this section, I will adopt a more personal and subjective standpoint, and discuss which are the main barriers between research and industrialisation. Indeed, though we never intended for this thesis to lead directly to practical applications, they remain the long term goal that motivates our work, and I must now examine our contributions again in this light.

A first and general concern of mine is that, beside the warm comfort of the well-behaved theoretical frameworks in which we place ourselves, the sheer complexity of the real world can be overwhelming. While any single aspect --\emph{partial observability, temporal abstraction, non-stationarity, risk aversion}, you name it-- can be isolated and studied independently, the question of how to merge all these approaches into one single integrated product seems arduous, if not hopeless. Yet, any candidate algorithm not addressing any of these issues would be unfit for deployment.
In the sequel, I will not be so ambitious but reflect instead on a more reasonable question: are the methods that we developed suitable for a real-world application?

\paragraph{Reinforcement Learning in continuous states}

Let us start with our work in \Cref{part:2}. Following a model-free perspective with continuous states, we resorted to function approximation using neural networks. Unfortunately, Deep Learning interacts with Reinforcement Learning algorithms in ways that are yet to be understood, but already infamous for their brittleness. In \Cref{chapter:4}, even our best policies still suffer a prohibitive rate of collisions of \SI{6}{\percent}, considerably higher than the required performances. 
As we discussed, this may be attributed to the reward function that would not penalise collisions enough, but reward engineering is tedious and might in turn lead to over-conservative policies. The budgeted approaches of \Cref{chapter:5} were meant to address this issue, but at the price of increased complexity, and our negative result of \Cref{thm:contraction} raises concerns about convergence in the general case. 
Another weakness lies in our use of a very naive exploration policy: the $\epsilon$-greedy, which takes random actions at a fixed frequency, a strategy widely considered inefficient and damaging, yet one that we had no choice but to resort to in the absence of a better solution. Guided exploration strategies tailored for regret minimisation, and especially following the \gls{OFU} principle, have been studied in the context of finite state-action spaces \citep{Auer2009,Azar2017}. 
These methods typically require the ability to count the number of state visits, which is not suitable for continuous states. The question of how they can be extended thus constitutes a promising research perspective. First steps have recently been made in that direction, by either relying on approximated \emph{pseudo-counts} \citep{Guyon2017}, or by deriving similar regret bounds under linear function approximation \citep{Jin2020}.

\paragraph{Trial without error?}

% In this thesis, we tried to solve behavioural planning in simulation, as a necessary step to solve it in the real world. Thus treated simulation as our target task. 
Assume for a moment that the research community was able to solve the aforementioned problem and came up with exciting new algorithms for continuous state space with promising regret bounds. There remains an inevitable and fatal limitation: \emph{the foundations of \glsxtrlong{RL} are intrinsically based on trial and error}. Unfortunately, this is not an acceptable paradigm for the development of safety-critical problems such as \glsxtrlong{AD}. For reference, when we applied the model-free methods of \Cref{part:2} to very simple tasks, they converged in about 50k interaction samples, which represents about \SI{15}{\hour} of driving, throughout which the agents experienced about two thousand collisions. More generally, having vehicles \emph{exploring} on the roads among human drivers is morally inconceivable. Is there any chance at all to come up with a learning algorithm that does not require causing accidents while training?

\paragraph{Safety guarantees}

As a first candidate, the line of work on \emph{safe} control is committed to developing algorithms that are guaranteed never to reach an unsafe state, or with a provably bounded probability of failure. 
Likewise, in \Cref{chapter:7}, we managed to obtain some theoretical guarantees: a robust constraint satisfaction result, and a lower-bound on the worst-case outcome, that increases towards near-optimal performance with the number of samples. However, it is evident that these results are only worth as much as their underlying assumptions, which may turn out to be: very little. Indeed, it seems dubious that the complexity of human behaviours can be accurately described by linear dynamics \Cref{assumpt:structure}, and our own proposed system largely overlooks vast areas of the driving task. With \Cref{assumpt:feasible-constr}, the safe region $\safestates$ cannot be chosen freely, but must contain the basin of attraction $\safestates_{f}$ whose size grows with uncertainty. Finally, the assumptions of \Cref{thm:minimax-regret-bound} do not hold even in our simple simulations: the behaviours of observed vehicles are not persistently excited (\ie continually changing lanes, or braking behind some vehicle), and the reward function is discontinuous. More generally, safety analyses can never protect against unmodeled events, such as a tree or a package falling down the road. Yet, having to model the world in its full complexity is daunting, especially since theoretical analysis imposes an additional constraint on the modelling effort, often at the expense of empirical performance. Fortunately, all is not bleak and it has been observed in numerous occasions that even when the guarantees do not hold, the founding principles of an algorithm can lead to designs that still exhibit the desired properties.
Beside this issue of the practical validity of theoretical assumptions, it seems to me that none of the most popular safety frameworks is really suitable for \gls{AD}. First, the ubiquitous concept of stabilisation does not really apply to a task of motion planning amidst other vehicles, for which there is no obvious equilibrium state. Second, the robust constraint satisfaction paradigm may be more relevant if the constraint space $\safestates$ could be defined as the collision-free space, but that space is non-convex which is not handled by most algorithms, \eg in the Tube \gls{MPC} family. Third, the minimax setting where worst-case outcomes are considered can become worthless in situations of large uncertainty where any decision can possibly result in a collision. In that case, the loss of sensitivity to probabilities caused to the $\min$ formulation means that decisions that are \emph{less likely} to lead to a collision will not even be preferred to those that are \emph{more likely} to do so. This ability requires to replace the worst-case evaluation by a more sophisticated measure of the outcomes distribution, such as the \gls{VaR} or \gls{CVaR}. This perspective seems a promising research direction to me, since to my knowledge no algorithm achieves guaranteed performance for these risk measures with continuous states, but it is also presumably a very demanding one.

%This may explain why practical achievements often precede their formal analysis. At the end of the day, do we prefer relying on a simpler model that we can analyse under some restrictive assumptions, or a black-box model which tends to perform better? Fortunately, all is not bleak and it has been observed in numerous occasions that even when the guarantees do not hold, the founding principles of an algorithm can lead to designs that still exhibit the desired properties. An example is the reliability of control systems used in the aerospace industry, despite aircraft dynamics not being \emph{really} linear and measurement noises not \emph{really} Gaussian.

\paragraph{Simulation and beyond?}

Another enticing way to avoid trial-and-error in the real world is to rely on simulation. Of course, the effort of modelling a complex world remains, but dropping the analysability requirement relaxes the modelling constraints. We can safely expect that simulations will continue to play an increasingly significant part in \gls{AD} technologies, for both offline pre-training and online planning. This can be the occasion to divert our research efforts from the traditional regret minimisation objective, which is groundless in a simulated environment where failures are free but samples are costly. In contrast, a promising research direction is the study of the more appropriate \emph{pure exploration} setting, which aims at relating the policy suboptimality to the number of samples used. I already took part in collaborations exploring this direction \citep{Jonsson2020planning,Kaufmann2020adaptive,Menard2020Fast} and hope to pursue this path further. 
Finally, relying on simulation introduces the additional question of how to adapt knowledge from simulation to the real world. A fine-tuning training process in real conditions would involve experiencing real failures again, though hopefully in reduced numbers, which could be realistic under human interventions \citep{Saunders2018,Kendall2019}. Another path of interest to me could be to leverage offline \gls{RL} methods \citep{Thomas2015,Laroche2019}, that could enable to safely improve pre-trained policies using real driving data, especially around nominal states that can be confidently estimated.

@Comment{Chap1,
	title =	{Introduction},
}

@article{Foot1967,
	volume = {5},
	pages = {5--15},
	title = {The Problem of Abortion and the Doctrine of Double Effect},
	year = {1967},
	journal = {Oxford Review},
	author = {Philippa Foot}
}

@Inbook{Lin2015,
	author={Lin, Patrick},
	title={Why Ethics Matters for Autonomous Cars},
	bookTitle={Autonomes Fahren: Technische, rechtliche und gesellschaftliche Aspekte},
	year={2015},
	publisher={Springer Berlin Heidelberg},
	address={Berlin, Heidelberg},
	pages={69--85},
	abstract={If motor vehicles are to be truly autonomous and able to operate responsibly on our roads, they will need to replicate -- or do better than -- the human decision-making process. But some decisions are more than just a mechanical application of traffic laws and plotting a safe path. They seem to require a sense of ethics, and this is a notoriously difficult capability to reduce into algorithms for a computer to follow.},
	isbn={978-3-662-45854-9},
	doi={10.1007/978-3-662-45854-9_4},
	url={https://doi.org/10.1007/978-3-662-45854-9_4}
	}

@article{Bonnefon2016,
	title={The social dilemma of autonomous vehicles},
	volume={352},
	ISSN={1095-9203},
	url={http://dx.doi.org/10.1126/science.aaf2654},
	DOI={10.1126/science.aaf2654},
	number={6293},
	journal={Science},
	publisher={American Association for the Advancement of Science (AAAS)},
	author={Bonnefon, J.-F. and Shariff, A. and Rahwan, I.},
	year={2016},
	month={Jun},
	pages={1573–1576}
}

@article{Gogoll2017,
	abstract = {The recent progress in the development of autonomous cars has seen ethical questions come to the forefront. In particular, life and death decisions regarding the behavior of self-driving cars in trolley dilemma situations are attracting widespread interest in the recent debate. In this essay we want to ask whether we should implement a mandatory ethics setting (MES) for the whole of society or, whether every driver should have the choice to select his own personal ethics setting (PES). While the consensus view seems to be that people would not be willing to use an automated car that might sacrifice themselves in a dilemma situation, we will defend the somewhat contra-intuitive claim that this would be nevertheless in their best interest. The reason is, simply put, that a PES regime would most likely result in a prisoner's dilemma.},
	author = {Gogoll, Jan and M{\"{u}}ller, Julian F.},
	doi = {10.1007/s11948-016-9806-x},
	issn = {14715546},
	journal = {Science and Engineering Ethics},
	keywords = {Automation,Autonomous driving,Dilemma,Ethics,Morality},
	title = {{Autonomous Cars: In Favor of a Mandatory Ethics Setting}},
	year = {2017}
}

@article{Awad2018,
	author={Awad, Edmond
	and Dsouza, Sohan
	and Kim, Richard
	and Schulz, Jonathan
	and Henrich, Joseph
	and Shariff, Azim
	and Bonnefon, Jean-Fran{\c{c}}ois
	and Rahwan, Iyad},
	title={The Moral Machine experiment},
	journal={Nature},
	year={2018},
	month={Nov},
	day={01},
	volume={563},
	number={7729},
	pages={59-64},
	abstract={With the rapid development of artificial intelligence have come concerns about how machines will make moral decisions, and the major challenge of quantifying societal expectations about the ethical principles that should guide machine behaviour. To address this challenge, we deployed the Moral Machine, an online experimental platform designed to explore the moral dilemmas faced by autonomous vehicles. This platform gathered 40 million decisions in ten languages from millions of people in 233 countries and territories. Here we describe the results of this experiment. First, we summarize global moral preferences. Second, we document individual variations in preferences, based on respondents' demographics. Third, we report cross-cultural ethical variation, and uncover three major clusters of countries. Fourth, we show that these differences correlate with modern institutions and deep cultural traits. We discuss how these preferences can contribute to developing global, socially acceptable principles for machine ethics. All data used in this article are publicly available.},
	issn={1476-4687},
	doi={10.1038/s41586-018-0637-6},
	url={https://doi.org/10.1038/s41586-018-0637-6}
}

@inproceedings{Noothigattu2018,
	title={A voting-based system for ethical decision making},
	author={Noothigattu, Ritesh and Gaikwad, Snehalkumar S and Awad, Edmond and Dsouza, Sohan and Rahwan, Iyad and Ravikumar, Pradeep and Procaccia, Ariel D},
	booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
	year={2018}
}

@misc{DeFreitas2019,
	title={Doubting Driverless Dilemmas},
	url={psyarxiv.com/a36e5},
	DOI={10.31234/osf.io/a36e5},
	publisher={PsyArXiv},
	author={De Freitas, Julian and Anthony, Sam E and Alvarez, George and Censi, Andrea},
	year={2019},
	month={Jan}
}

@misc{trolley2009,
	author = {Jesse Prinz},
	title = {Subcortex.com},
	year = 2009,
	howpublished={\url{http://subcortex.com/}},
	urldate = {2010-09-30}
}


@article{Abraham1960,
	title={Le prix d'une vie humaine dans les d{\'e}cisions {\'e}conomiques},
	author={Abraham, C. and Th{\'e}di{\'e}, J.},
	journal={Revue Française de Recherche Op{\'e}rationnelle},
	number={6},
	pages={157-168},
	year={1960}
}

@article{Dreze1962,
	title={L'utilit{\'e} sociale d'une vie humaine},
	author={Dr{\`e}ze, Jacques},
	journal={Revue Française de Recherche Op{\'e}rationnelle},
	volume={23},
	pages={93-118},
	issn={0556-7815},
	year={1962}
}

@article{Schelling1968life,
	title={The life you save may be your own},
	author={Schelling, Thomas C},
	journal={Problems in public expenditure},
	pages={127--162},
	year={1968}
}

@article{Banzhaf2014,
	Author = {Banzhaf, H. Spencer},
	Title = {Retrospectives: The Cold-War Origins of the Value of Statistical Life},
	Journal = {Journal of Economic Perspectives},
	Volume = {28},
	Number = {4},
	Year = {2014},
	Month = {November},
	Pages = {213-26},
	DOI = {10.1257/jep.28.4.213},
	URL = {https://www.aeaweb.org/articles?id=10.1257/jep.28.4.213}
}

@book{Tirole2017,
	ISBN = {9780691175164},
	URL = {http://www.jstor.org/stable/j.ctvc77hng},
	abstract = {From Nobel Prize-winning economist Jean Tirole, a bold new agenda for the role of economics in society When Jean Tirole won the 2014 Nobel Prize in Economics, he suddenly found himself being stopped in the street by complete strangers and asked to comment on issues of the day, no matter how distant from his own areas of research. His transformation from academic economist to public intellectual prompted him to reflect further on the role economists and their discipline play in society. The result is Economics for the Common Good, a passionate manifesto for a world in which economics, far from being a "dismal science," is a positive force for the common good. Economists are rewarded for writing technical papers in scholarly journals, not joining in public debates. But Tirole says we urgently need economists to engage with the many challenges facing society, helping to identify our key objectives and the tools needed to meet them. To show how economics can help us realize the common good, Tirole shares his insights on a broad array of questions affecting our everyday lives and the future of our society, including global warming, unemployment, the post-2008 global financial order, the euro crisis, the digital revolution, innovation, and the proper balance between the free market and regulation. Providing a rich account of how economics can benefit everyone, Economics for the Common Good sets a new agenda for the role of economics in society. },
	author = {Jean Tirole and Steven Rendall},
	publisher = {Princeton University Press},
	title = {Economics for the Common Good},
	year = {2017}
}

@article{Charpentier2019,
	title={La valeur de la vie humaine},
	author={Arthur Charpentier and Béatrice Cherrier},
	journal={Risques},
	number={118},
	pages={107--111},
	year={2019},
	month={June}
}

@book{Buehler2009,
	author = {Buehler, Martin and Iagnemma, Karl and Singh, Sanjiv},
	title = {The DARPA Urban Challenge: Autonomous Vehicles in City Traffic},
	year = {2009},
	isbn = {3642039901},
	publisher = {Springer Publishing Company, Incorporated},
	edition = {1st}
}

@phdthesis{Polack2018,
	title = {{Consistency and stability of hierarchical planning and control systems for autonomous driving}},
	author = {Polack, Philip},
	url = {https://pastel.archives-ouvertes.fr/tel-02096788},
	number = {2018PSLEM025},
	school = {{PSL Research University}},
	year = {2018},
	month = Oct,
	keywords = {Vehicle dynamics ; Motion planning ; Autonomous driving ; Model predictive control ; Model-free control ; Longitudinal and lateral control ; Planification de trajectoire ; Conduite autonome ; Commande pr{\'e}dictive {\`a} mod{\`e}le ; Contr{\^o}le longitudinal et lat{\'e}ral ; Dynamique du v{\'e}hicule},
	type = {Theses},
	pdf = {https://pastel.archives-ouvertes.fr/tel-02096788/file/2018PSLEM025_archivage.pdf},
	hal_id = {tel-02096788},
	hal_version = {v1},
}

@inproceedings{Baker2008,
	abstract = {We describe an autonomous robotic software subsystem for managing mission execution and discrete traffic interaction in the 2007 DARPA Urban Challenge. Its role is reviewed in the context of the software system that controls "Boss", Tartan Racing's winning entry in the competition. Design criteria are presented, followed by the application of software design principles to derive an architecture well suited to the rigors of developing complex robotic systems. Combined with a discussion of robust behavioral algorithms, the design's effectiveness is highlighted in its ability to manage complex autonomous driving behaviors while remaining adaptable to the system's evolving capabilities. {\textcopyright}2008 IEEE.},
	author = {Baker, Christopher R. and Dolan, John M.},
	booktitle = {2008 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS},
	doi = {10.1109/IROS.2008.4651211},
	isbn = {9781424420582},
	title = {{Traffic interaction in the urban challenge: Putting boss on its best behavior}},
	year = {2008}
}


@Comment{Chap2_1,
	title =	{Motion Planning},
}

@article{Gonzalez2016,
	author={D. {González} and J. {Pérez} and V. {Milanés} and F. {Nashashibi}},
	journal={IEEE Transactions on Intelligent Transportation Systems}, 
	title={A Review of Motion Planning Techniques for Automated Vehicles}, 
	year={2016},
	volume={17},
	number={4},
	pages={1135-1145},
}

@article{Paden2016,
	author    = {Brian Paden and
	Michal C{\'{a}}p and
	Sze Zheng Yong and
	Dmitry S. Yershov and
	Emilio Frazzoli},
	title     = {A Survey of Motion Planning and Control Techniques for Self-driving
	Urban Vehicles},
	journal   = {CoRR},
	volume    = {abs/1604.07446},
	year      = {2016},
	url       = {http://arxiv.org/abs/1604.07446},
	archivePrefix = {arXiv},
	eprint    = {1604.07446},
	timestamp = {Mon, 13 Aug 2018 16:49:15 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/PadenCYYF16.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Dijkstra1959,
	author = {Dijkstra, E. W.},
	doi = {10.1007/BF01386390},
	issn = {0029599X},
	journal = {Numerische Mathematik},
	title = {{A note on two problems in connexion with graphs}},
	year = {1959}
}

@INPROCEEDINGS{Fraichard1993,
	author={T. {Fraichard}},
	booktitle={Proceedings of 1993 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS '93)}, 
	title={Dynamic trajectory planning with dynamic constraints: A 'state-time space' approach}, 
	year={1993},
	volume={2},
	number={},
	pages={1393-1400 vol.2},}

@ARTICLE{Laumond1994,
	author={J. -. {Laumond} and P. E. {Jacobs} and M. {Taix} and R. M. {Murray}},
	journal={IEEE Transactions on Robotics and Automation}, 
	title={A motion planner for nonholonomic mobile robots}, 
	year={1994},
	volume={10},
	number={5},
	pages={577-593},
}

@article{Hart1968,
	abstract = {Although the problem of determining the minimum cost path through a graph arises naturally in a number of interesting applications, there has been no underlying theory to guide the development of efficient search procedures. Moreover, there is no adequate conceptual framework within which the various ad hoc search strategies proposed to date can be compared. This paper describes how heuristic information from the problem domain can be incorporated into a formal mathematical theory of graph searching and demonstrates an optimality property of a class of search strategies. Copyright {\textcopyright} 1968 by The Institute of Electrical and Electronics Engineers, Inc.},
	author = {Hart, Peter E. and Nilsson, Nils J. and Raphael, Bertram},
	doi = {10.1109/TSSC.1968.300136},
	issn = {21682887},
	journal = {IEEE Transactions on Systems Science and Cybernetics},
	title = {{A Formal Basis for the Heuristic Determination of Minimum Cost Paths}},
	year = {1968}
}

@inproceedings{Stentz1994,
	abstract = {The task of planning trajectories for a mobile robot has received considerable attention in the research literature. Most of the work assumes the robot has a complete and accurate model of its environment before it begins to move; less attention has been paid to the problem of partially known environments. This situation occurs for an exploratory robot or one that must move to a goal location without the benefit of a floorplan or terrain map. Existing approaches plan an initial path based on known information and then modify the plan locally or replan the entire path as the robot discovers obstacles with its sensors, sacrificing optimality or computational efficiency respectively. This paper introduces a new algorithm, D, capable of planning paths in unknown, partially known, and changing environments in an efficient, optimal, and complete manner.},
	author = {Stentz, Anthony},
	booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
	doi = {10.1109/robot.1994.351061},
	isbn = {0818653329},
	issn = {10504729},
	title = {{Optimal and efficient path planning for partially-known environments}},
	year = {1994}
}

@inproceedings{Ziegler2009,
	abstract = {We present a method for motion planning in the presence of moving obstacles that is aimed at dynamic on-road driving scenarios. Planning is performed within a geometric graph that is established by sampling deterministically from a manifold that is obtained by combining configuration space and time. We show that these graphs are acyclic and shortest path algorithms with linear runtime can be employed. By reparametrising the configuration space to match the course of the road, it can be sampled very economically with few vertices, and this reduces absolute runtime further. The trajectories generated are quintic splines. They are second order continuous, obey nonholonomic constraints and are optimised for minimum square of jerk. Planning time remains below 20 ms on general purpose hardware. {\textcopyright} 2009 IEEE.},
	author = {Ziegler, Julius and Stiller, Christoph},
	booktitle = {2009 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2009},
	doi = {10.1109/IROS.2009.5354448},
	isbn = {9781424438044},
	title = {{Spatiotemporal state lattices for fast trajectory planning in dynamic on-road driving scenarios}},
	year = {2009}
}

@article{Bohren2008,
	abstract = {This paper describes "Little Ben," an autonomous ground vehicle constructed by the Ben Franklin Racing Team for the 2007 DARPA Urban Challenge in under a year and for less than {\$}250,000. The sensing, planning, navigation, and actuation systems for Little Ben were carefully designed to meet the performance demands required of an autonomous vehicle traveling in an uncertain urban environment. We incorporated an array of a global positioning system (GPS)/inertial navigation system, LIDARs, and stereo cameras to provide timely information about the surrounding environment at the appropriate ranges. This sensor information was integrated into a dynamic map that could robustly handle GPS dropouts and errors. Our planning algorithms consisted of a high-level mission planner that used information from the provided route network definition and mission data files to select routes, whereas the lower level planner used the latest dynamic map information to optimize a feasible trajectory to the next waypoint. The vehicle was actuated by a cost-based controller that efficiently handled steering, throttle, and braking maneuvers in both forward and reverse directions. Our software modules were integrated within a hierarchical architecture that allowed rapid development and testing of the system performance. The resulting vehicle was one of six to successfully finish the Urban Challenge. {\textcopyright} 2008 Wiley Periodicals, Inc.},
	author = {Bohren, Jonathan and Foote, Tully and Keller, Jim and Kushleyev, Alex and Lee, Daniel and Stewart, Alex and Vernaza, Paul and Derenick, Jason and Spletzer, John and Satterfield, Brian},
	doi = {10.1002/rob.20260},
	issn = {15564959},
	journal = {Journal of Field Robotics},
	title = {{Little Ben: The Ben Franklin Racing Team's entry in the 2007 DARPA Urban Challenge}},
	year = {2008}
}

@article{Bacha2008,
	abstract = {The DARPA Urban Challenge required robotic vehicles to travel more than 90 km through an urban environment without human intervention and included situations such as stop intersections, traffic merges, parking, and roadblocks. Team VictorTango separated the problem into three parts: base vehicle, perception, and planning. A Ford Escape outfitted with a custom drive-by-wire system and computers formed the basis for Odin. Perception used laser scanners, global positioning system, and a priori knowledge to identify obstacles, cars, and roads. Planning relied on a hybrid deliberative/reactive architecture toanalyze the situation, select the appropriate behavior, and plan a safe path. All vehicle modules communicated using the JAUS (Joint Architecture for Unmanned Systems) standard. The performance of these components in the Urban Challenge is discussed and successes noted. The result of VictorTango's work was successful completion of the Urban Challenge and a third-place finish. {\textcopyright} 2008 Wiley Periodicals, Inc.},
	author = {Bacha, Andrew and Bauman, Cheryl and Faruque, Ruel and Fleming, Michael and Terwelp, Chris and Reinholtz, Charles and Hong, Dennis and Wicks, Al and Alberi, Thomas and Anderson, David and Cacciola, Stephen and Currier, Patrick and Dalton, Aaron and Farmer, Jesse and Hurdus, Jesse and Kimmel, Shawn and King, Peter and Taylor, Andrew and van Covern, David and Webster, Mike},
	doi = {10.1002/rob.20248},
	issn = {15564959},
	journal = {Journal of Field Robotics},
	title = {{Odin: Team VictorTango's entry in the DARPA Urban Challenge}},
	year = {2008}
}

@article{Montemerlo2008,
	abstract = {This article presents the architecture of Junior, a robotic vehicle capable of navigating urban environments autonomously. In doing so, the vehicle is able to select its own routes, perceive and interact with other traffic, and execute various urban driving skills including lane changes, U-turns, parking, and merging into moving traffic. The vehicle successfully finished and won second place in the DARPA Urban Challenge, a robot competition organized by the U.S. Government. {\textcopyright} 2008 Wiley Periodicals, Inc.},
	author = {Montemerlo, Michael and Becker, Jan and Shat, Suhrid and Dahlkamp, Hendrik and Dolgov, Dmitri and Ettinger, Scott and Haehnel, Dirk and Hilden, Tim and Hoffmann, Gabe and Huhnke, Burkhard and Johnston, Doug and Klumpp, Stefan and Langer, Dirk and Levandowski, Anthony and Levinson, Jesse and Marcil, Julien and Orenstein, David and Paefgen, Johannes and Penny, Isaac and Petrovskaya, Anna and Pflueger, Mike and Stanek, Ganymed and Stavens, David and Vogt, Antone and Thrun, Sebastian},
	doi = {10.1002/rob.20258},
	issn = {15564959},
	journal = {Journal of Field Robotics},
	title = {{Junior: The Stanford entry in the urban challenge}},
	year = {2008}
}

@article{Kammel2008,
	abstract = {This paper reports on AnnieWAY, an autonomous vehicle that is capable of driving through urban scenarios and that successfully entered the finals of the 2007 DARPA Urban Challenge competition. After describing the main challenges imposed and the major hardware components, we outline the underlying software structure and focus on selected algorithms. Environmental perception mainly relies on a recent laser scanner that delivers both range and reflectivity measurements. Whereas range measurements are used to provide three-dimensional scene geometry, measuring reflectivity allows for robust lane marker detection. Mission and maneuver planning is conducted using a hierarchical state machine that generates behavior in accordance with California traffic laws. We conclude with a report of the results achieved during the competition. {\textcopyright} 2008 Wiley Periodicals, Inc.},
	author = {Kammel, S{\"{o}}ren and Ziegler, Julius and Pitzer, Benjamin and Werling, Moritz and Gindele, Tobias and Jagzent, Daniel and Schr{\"{o}}der, Joachim and Thuy, Michael and Goebl, Matthias and von Hundelshausen, Felix and Pink, Oliver and Frese, Christian and Stiller, Christoph},
	doi = {10.1002/rob.20252},
	issn = {15564959},
	journal = {Journal of Field Robotics},
	title = {{Team AnnieWAY's autonomous system for the 2007 DARPA Urban Challenge}},
	year = {2008}
}

@article{Urmson2008,
	abstract = {Boss is an autonomous vehicle that uses on-board sensors (global positioning system, lasers, radars, and cameras) to track other vehicles, detect static obstacles, and localize itself relative to a road model. A three-layer planning system combines mission, behavioral, and motion planning to drive in urban environments. The mission planning layer considers which street to take to achieve a mission goal. The behavioral layer determines when to change lanes and precedence at intersections and performs error recovery maneuvers. The motion planning layer selects actions to avoid obstacles while making progress toward local goals. The system was developed from the ground up to address the requirements of the DARPA Urban Challenge using a spiral system development process with a heavy emphasis on regular, regressive system testing. During the National Qualification Event and the 85-km Urban Challenge Final Event, Boss demonstrated some of its capabilities, qualifying first and winning the challenge. {\textcopyright} 2008 Wiley Periodicals, Inc.},
	author = {Urmson, Chris and Anhalt, Joshua and Bagnell, Drew and Baker, Christopher and Bittner, Robert and Clark, M. N. and Dolan, John and Duggins, Dave and Galatali, Tugrul and Geyer, Chris and Gittleman, Michele and Harbaugh, Sam and Hebert, Martial and Howard, Thomas M. and Kolski, Sascha and Kelly, Alonzo and Likhachev, Maxim and McNaughton, Matt and Miller, Nick and Peterson, Kevin and Pilnick, Brian and Rajkumar, Raj and Rybski, Paul and Salesky, Bryan and Seo, Young Woo and Singh, Sanjiv and Snider, Jarrod and Stentz, Anthony and Whittaker, William and Wolkowicki, Ziv and Ziglar, Jason and Bae, Hong and Brown, Thomas and Demitrish, Daniel and Litkouhi, Bakhtiar and Nickolaou, Jim and Sadekar, Varsha and Zhang, Wende and Struble, Joshua and Taylor, Michael and Darms, Michael and Ferguson, Dave},
	doi = {10.1002/rob.20255},
	issn = {15564959},
	journal = {Journal of Field Robotics},
	title = {{Autonomous driving in urban environments: Boss and the urban challenge}},
	year = {2008}
}

@techreport{Lavalle98,
	author = {Steven M. Lavalle},
	title = {Rapidly-Exploring Random Trees: A New Tool for Path Planning},
	institution = {Computer Science Department, Iowa State University},
	year = {1998}
}

@inproceedings{Karaman2011,
	abstract = {During the last decade, sampling-based path planning algorithms, such as probabilistic roadmaps (PRM) and rapidly exploring random trees (RRT), have been shown to work well in practice and possess theoretical guarantees such as probabilistic completeness. However, little effort has been devoted to the formal analysis of the quality of the solution returned by such algorithms, e.g. as a function of the number of samples. The purpose of this paper is to fill this gap, by rigorously analyzing the asymptotic behavior of the cost of the solution returned by stochastic sampling-based algorithms as the number of samples increases. A number of negative results are provided, characterizing existing algorithms, e.g. showing that, under mild technical conditions, the cost of the solution returned by broadly used sampling-based algorithms converges almost surely to a non-optimal value. The main contribution of the paper is the introduction of new algorithms, namely, PRM* and RRT*, which are provably asymptotically optimal, i.e. such that the cost of the returned solution converges almost surely to the optimum. Moreover, it is shown that the computational complexity of the new algorithms is within a constant factor of that of their probabilistically complete (but not asymptotically optimal) counterparts. The analysis in this paper hinges on novel connections between stochastic sampling-based path planning algorithms and the theory of random geometric graphs. {\textcopyright} The Author(s) 2011.},
	author = {Karaman, Sertac and Frazzoli, Emilio},
	booktitle = {International Journal of Robotics Research},
	doi = {10.1177/0278364911406761},
	issn = {02783649},
	keywords = {Motion planning,Optimal path planning,Random geometric graphs,Sampling-based algorithms},
	title = {{Sampling-based algorithms for optimal motion planning}},
	year = {2011}
}

@article{Kavraki1996,
	abstract = {A new motion planning method for robots in static workspaces is presented. This method proceeds in two phases: a learning phase and a query phase. In the learning phase, a probabilistic roadmap is constructed and stored as a graph whose nodes correspond to collision-free configurations and whose edges correspond to feasible paths between these configurations. These paths are computed using a simple and fast local planner. In the query phase, any given start and goal configurations of the robot are connected to two nodes of the roadmap; the roadmap is then searched for a path joining these two nodes. The method is general and easy to implement. It can be applied to virtually any type of holonomic robot. It requires selecting certain parameters (e.g., the duration of the learning phase) whose values depend on the scene, that is the robot and its workspace. But these values turn out to be relatively easy to choose. Increased efficiency can also be achieved by tailoring some components of the method (e.g., the local planner) to the considered robots. In this paper the method is applied to planar articulated robots with many degrees of freedom. Experimental results show that path planning can be done in a fraction of a second on a contemporary workstation (≈ 150 MIPS), after learning for relatively short periods of time (a few dozen seconds). {\textcopyright} 1996 IEEE.},
	author = {Kavraki, Lydia E. and {\v{S}}vestka, Petr and Latombe, Jean Claude and Overmars, Mark H.},
	doi = {10.1109/70.508439},
	issn = {1042296X},
	journal = {IEEE Transactions on Robotics and Automation},
	title = {{Probabilistic roadmaps for path planning in high-dimensional configuration spaces}},
	year = {1996}
}

@inproceedings{Faust2018,
	abstract = {We present PRM-RL, a hierarchical method for long-range navigation task completion that combines sampling-based path planning with reinforcement learning (RL). The RL agents learn short-range, point-to-point navigation policies that capture robot dynamics and task constraints without knowledge of the large-scale topology. Next, the sampling-based planners provide roadmaps which connect robot configurations that can be successfully navigated by the RL agent. The same RL agents are used to control the robot under the direction of the planning, enabling long-range navigation. We use the Probabilistic Roadmaps (PRMs) for the sampling-based planner. The RL agents are constructed using feature-based and deep neural net policies in continuous state and action spaces. We evaluate PRM-RL, both in simulation and on-robot, on two navigation tasks with non-trivial robot dynamics: end-to-end differential drive indoor navigation in office environments, and aerial cargo delivery in urban environments with load displacement constraints. Our results show improvement in task completion over both RL agents on their own and traditional sampling-based planners. In the indoor navigation task, PRM-RL successfully completes up to 215 m long trajectories under noisy sensor conditions, and the aerial cargo delivery completes flights over 1000 m without violating the task constraints in an environment 63 million times larger than used in training.},
	archivePrefix = {arXiv},
	arxivId = {1710.03937},
	author = {Faust, Aleksandra and Oslund, Kenneth and Ramirez, Oscar and Francis, Anthony and Tapia, Lydia and Fiser, Marek and Davidson, James},
	booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
	doi = {10.1109/ICRA.2018.8461096},
	eprint = {1710.03937},
	isbn = {9781538630815},
	issn = {10504729},
	title = {{PRM-RL: Long-range robotic navigation tasks by combining reinforcement learning and sampling-based planning}},
	year = {2018}
}

@inproceedings{Lenz2016,
	abstract = {Human drivers use nonverbal communication and anticipation of other drivers' actions to master conflicts occurring in everyday driving situations. Without a high penetration of vehicle-to-vehicle communication an autonomous vehicle has to have the possibility to understand intentions of others and share own intentions with the surrounding traffic participants. This paper proposes a cooperative combinatorial motion planning algorithm without the need for inter vehicle communication based on Monte Carlo Tree Search (MCTS). We motivate why MCTS is particularly suited for the autonomous driving domain. Furthermore, adoptions to the MCTS algorithm are presented as for example simultaneous decisions, the usage of the Intelligent Driver Model as microscopic traffic simulation, and a cooperative cost function. We further show simulation results of merging scenarios in highway-like situations to underline the cooperative nature of the approach.},
	author = {Lenz, David and Kessler, Tobias and Knoll, Alois},
	booktitle = {IEEE Intelligent Vehicles Symposium, Proceedings},
	doi = {10.1109/IVS.2016.7535424},
	isbn = {9781509018215},
	title = {{Tactical cooperative planning for autonomous highway driving using Monte-Carlo Tree Search}},
	year = {2016}
}

@article{Latombe1991,
	abstract = {A car-like indoor mobile robot is a kinematically$\backslash$r$\backslash$nconstrained robot that can be modelled as a 2D$\backslash$r$\backslash$nobject translating and rotating in the horizontal$\backslash$r$\backslash$nplane among well-defined obstacles. The kinematic$\backslash$r$\backslash$nconstraints impose that the linear velocity of the$\backslash$r$\backslash$nrobot point along its main axis (no sidewise motion$\backslash$r$\backslash$nis possible) and restrict the range of admissible$\backslash$r$\backslash$nvalues for the steering angle. In this paper' we$\backslash$r$\backslash$ndescribe a fast path planner for such a robot. This$\backslash$r$\backslash$nplanner is one to two orders of magnitude faster$\backslash$r$\backslash$nthan previously implemented planners for the same$\backslash$r$\backslash$ntype of robot. In addition, it has an anytime flavor$\backslash$r$\backslash$nthat allows it to return a path in a short amount$\backslash$r$\backslash$nof time, and to improve that path through iterative$\backslash$r$\backslash$noptimization according to the amount of time$\backslash$r$\backslash$nthat is devoted to path planning, The planner is$\backslash$r$\backslash$nessentially a combination of preexisting ideas. Its$\backslash$r$\backslash$nefficiency derives from the good match between$\backslash$r$\backslash$nthese ideas and from various technical improvements$\backslash$r$\backslash$nbrought to them. },
	author = {Latombe, J.C.},
	journal = {Proceedings of the ninth National conference on Artificial intelligence},
	title = {{A Fast Path Planner for a Car-Like Indoor Mobile Robot}},
	year = {1991}
}

@inproceedings{Sanchez2002,
	author = {L., Abraham S\'{a}nchez and Zapata, Ren\'{e} and B., J. Abraham Arenas},
	title = {Motion Planning for Car-Like Robots Using Lazy Probabilistic Roadmap Method},
	year = {2002},
	isbn = {3540434755},
	publisher = {Springer-Verlag},
	address = {Berlin, Heidelberg},
	booktitle = {Proceedings of the Second Mexican International Conference on Artificial Intelligence: Advances in Artificial Intelligence},
	pages = {1–10},
	numpages = {10},
	series = {MICAI ’02}
}

@ARTICLE{Lamiraux2001,	
	author={F. {Lamiraux} and J. -. {Lammond}},
	journal={IEEE Transactions on Robotics and Automation}, 
	title={Smooth motion planning for car-like vehicles}, 
	year={2001},
	volume={17},
	number={4},
	pages={498-501},}

@inproceedings{Kocsis2006,
	abstract = {For large state-space Markovian Decision Problems Monte-Carlo planning is one of the few viable approaches to find near-optimal solutions. In this paper we introduce a new algorithm, UCT, that applies bandit ideas to guide Monte-Carlo planning. In finite-horizon or discounted MDPs the algorithm is shown to be consistent and finite sample bounds are derived on the estimation error due to sampling. Experimental results show that in several domains, UCT is significantly more efficient than its alternatives. {\textcopyright} Springer-Verlag Berlin Heidelberg 2006.},
	author = {Kocsis, Levente and Szepesv{\'{a}}ri, Csaba},
	booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	doi = {10.1007/11871842_29},
	isbn = {354045375X},
	issn = {16113349},
	title = {{Bandit based Monte-Carlo planning}},
	year = {2006}
}

@article{Reeds1990,
	abstract = {The path taken by a car with a given minimum turning radius has a lower bound on its radius of curvature at each point, but the path has cusps if the car shifts into or out of reverse gear. What is the shortest such path a car can travel between two points if its starting and ending directions are specified? One need consider only paths with at most 2 cusps or reversals. We give a set of paths which is sufficient in the sense that it always contains a shortest path and small in the sense that there are at most 68, but usually many fewer paths in the set for any pair of endpoints and directions. We give these paths by explicit formula. Calculating the length of each of these paths and selecting the (not necessarily unique) path with smallest length yields a simple algorithm for a shortest path in each case. These optimal paths or geodesies may be described as follows: If C is an arc of a circle of the minimal turning radius and S is a line segment, then it is sufficient to consider only certain paths of the form CCSCC where arcs and segments fit smoothly, one or more of the arcs or segments may vanish, and where reversals, or equivalently cusps, between arcs or segments are allowed. This contrasts with the case when cusps are not allowed, where Dubins (1957) has shown that paths of the form CCC and CSC suffice. {\textcopyright} 1990 by Pacific Journal of Mathematics.},
	author = {Reeds, J. A. and Shepp, L. A.},
	doi = {10.2140/pjm.1990.145.367},
	issn = {00308730},
	journal = {Pacific Journal of Mathematics},
	title = {{Optimal paths for a car that goes both forwards and backwards}},
	year = {1990}
}

@inproceedings{Xu2012,
	abstract = {In this paper, an efficient real-time autonomous driving motion planner with trajectory optimization is proposed. The planner first discretizes the plan space and searches for the best trajectory based on a set of cost functions. Then an iterative optimization is applied to both the path and speed of the resultant trajectory. The post-optimization is of low computational complexity and is able to converge to a higher-quality solution within a few iterations. Compared with the planner without optimization, this framework can reduce the planning time by 52{\%} and improve the trajectory quality. The proposed motion planner is implemented and tested both in simulation and on a real autonomous vehicle in three different scenarios. Experiments show that the planner outputs high-quality trajectories and performs intelligent driving behaviors. {\textcopyright} 2012 IEEE.},
	author = {Xu, Wenda and Wei, Junqing and Dolan, John M. and Zhao, Huijing and Zha, Hongbin},
	booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
	doi = {10.1109/ICRA.2012.6225063},
	isbn = {9781467314039},
	issn = {10504729},
	title = {{A real-time motion planner with trajectory optimization for autonomous vehicles}},
	year = {2012}
}

@inproceedings{Gonzalez2014,
	abstract = {This paper presents a continuous curvature planning algorithm with obstacle avoidance capabilities. The automated system generates a collision free path that considers vehicle's constraints, the road and different obstacles inside the horizon of view. The developed planning module was integrated in the RITS (former IMARA) autonomous vehicle architecture. The goal of this module is to obtain an accurate, continuous and safe path generation, by implementing parametric curves. To this end, a continuous curvature profile when calculating vehicle trajectory is introduced. It also permits to generate different speed profiles, improving the comfort by reducing lateral accelerations in the driving process. These algorithms have been implemented in simulated -ProSiVIC- and real platforms -Cybercars- showing good results in both cases. This approach is currently being implemented in the framework of the EU CityMobil2 project.},
	author = {Gonz{\'{a}}lez, David and P{\'{e}}rez, Joshue and Lattarulo, Ray and Milan{\'{e}}s, Vicente and Nashashibi, Fawzi},
	booktitle = {2014 17th IEEE International Conference on Intelligent Transportation Systems, ITSC 2014},
	doi = {10.1109/ITSC.2014.6957887},
	isbn = {9781479960781},
	title = {{Continuous curvature planning with obstacle avoidance capabilities in urban scenarios}},
	year = {2014}
}

@inproceedings{Funke2012,
	abstract = {This paper presents a novel approach to autonomous driving at the vehicle's handling limits. Such a system requires a high speed, consistent control signal as well as numerous safety features capable of monitoring and stopping the vehicle. When operating, the system's high level controller utilizes a highly accurate differential GPS and known friction values to drive a precomputed path at the friction limits of the vehicle. The system was tested in a variety of road conditions, including the challenging Pikes Peak Hill climb. Results from this work can be extended to improve driving safety and accident avoidance in vehicles. {\textcopyright} 2012 IEEE.},
	author = {Funke, Joseph and Theodosis, Paul and Hindiyeh, Rami and Stanek, Ganymed and Kritatakirana, Krisada and Gerdes, Chris and Langer, Dirk and Hernandez, Marcial and M{\"{u}}ller-Bessler, Bernhard and Huhnke, Burkhard},
	booktitle = {IEEE Intelligent Vehicles Symposium, Proceedings},
	doi = {10.1109/IVS.2012.6232212},
	isbn = {9781467321198},
	title = {{Up to the limits: Autonomous Audi TTS}},
	year = {2012}
}

@article{Pivtoraiko2005,
	abstract = {We propose a novel approach to constrained path planning that is based on a special search space which efficiently encodes feasible paths. The paths are encoded implicitly as connections between states, but only feasible and local connections are included. Once this search space is developed, we systematically generate a near-minimal set of spatially distinct path primitives. This set expresses the local connectivity of constrained motions and also eliminates redundancies. The set of primitives is used to define heuristic search, and thereby create a very efficient path planner at the chosen resolution. We also discuss a wide variety of space and terrestrial robotics applications where this motion planner can be especially useful.},
	author = {Pivtoraiko, Mihail and Kelly, Alonzo},
	issn = {03796566},
	journal = {European Space Agency, (Special Publication) ESA SP},
	keywords = {Control set,Differential constraints,Lattice,Nonholonomic,Path planning,Path/motion primitives},
	title = {{Efficient constrained path planning via search in state lattices}},
	year = {2005}
}

@ARTICLE{LaValle1998,	
	author={S. M. {LaValle} and S. A. {Hutchinson}},
	journal={IEEE Transactions on Robotics and Automation}, 
	title={Optimal motion planning for multiple robots having independent goals}, 
	year={1998},
	volume={14},
	number={6},
	pages={912-925},
}


@INPROCEEDINGS{Svestka1995,
	author={P. {Svestka} and M. H. {Overmars}},
	booktitle={Proceedings of 1995 IEEE International Conference on Robotics and Automation}, 
	title={Coordinated motion planning for multiple car-like robots using probabilistic roadmaps}, 
	year={1995},
	volume={2},
	number={},
	pages={1631-1636 vol.2},
}

@inproceedings{Altche2016,
	abstract = {In this paper, we address the problem of timeoptimal coordination of mobile robots under kinodynamic constraints along specified paths. We propose a novel approach based on time discretization that leads to a mixed-integer linear programming (MILP) formulation. This problem can be solved using general-purpose MILP solvers in a reasonable time, resulting in a resolution-optimal solution. Moreover, unlike previous work found in the literature, our formulation allows an exact linear modeling (up to the discretization resolution) of second-order dynamic constraints. Extensive simulations are performed to demonstrate the effectiveness of our approach.},
	archivePrefix = {arXiv},
	arxivId = {1603.04610},
	author = {Altch{\'{e}}, Florent and Qian, Xiangjun and {De La Fortelle}, Arnaud},
	booktitle = {IEEE International Conference on Intelligent Robots and Systems},
	doi = {10.1109/IROS.2016.7759737},
	eprint = {1603.04610},
	isbn = {9781509037629},
	issn = {21530866},
	title = {{Time-optimal coordination of mobile robots along specified paths}},
	year = {2016}
}

@inproceedings{Altche2016b,
	abstract = {The deployment of Cooperative Intelligent Transportation Systems (C-ITS) raises the question of future traffic management systems, which will be operating with an increasing amount of information and control over the infrastructure and the vehicles. This topic of research shares some similarities with robot coordination problems, inspiring our research on autonomous intersection management. In this article, we use a mixed-integer linear programming formulation for time-optimal robots coordination along specified paths and apply it to intersection management for autonomous vehicles. Our formulation allows to simultaneously solve a discrete optimal vehicle ordering problem, and a (discretized) continuous optimal velocity planning problem taking into account kinodynamics constraints. This allows faster pruning of the decision tree for the discrete problem, thus reducing computation time. A possible application for ITS is to evaluate the efficiency loss from a given vehicle ordering policy, or dynamically adapt policies to improve their efficiency. Moreover, any intermediary solution found by the solver can be used as a heuristically good policy, with proved bounds on sub-optimality.},
	author = {Altche, Florent and {De La Fortelle}, Arnaud},
	booktitle = {IEEE Intelligent Vehicles Symposium, Proceedings},
	doi = {10.1109/IVS.2016.7535369},
	isbn = {9781509018215},
	title = {{Analysis of optimal solutions to robot coordination problems to improve autonomous intersection management policies}},
	year = {2016}
}

@article{Altche2017,
	abstract = {Before reaching full autonomy, vehicles will gradually be equipped with more and more advanced driver assistance systems (ADAS), effectively rendering them semi-autonomous. However, current ADAS technologies seem unable to handle complex traffic situations, notably when dealing with vehicles arriving from the sides, either at intersections or when merging on highways. The high rate of accidents in these settings proves that they constitute difficult driving situations. Moreover, intersections and merging lanes are often the source of important traffic congestion and, sometimes, deadlocks. In this paper, we propose a cooperative framework to safely coordinate semi-autonomous vehicles in such settings, removing the risk of collision or deadlocks while remaining compatible with human driving. More specifically, we present a supervised coordination scheme that overrides control inputs from human drivers when they would result in an unsafe or blocked situation. To avoid unnecessary intervention and remain compatible with human driving, overriding only occurs when collisions or deadlocks are imminent. In this case, safe overriding controls are chosen while ensuring they deviate minimally from those originally requested by the drivers. Simulation results based on a realistic physics simulator show that our approach is scalable to real-world scenarios, and computations can be performed in real time on a standard computer for up to a dozen simultaneous vehicles.},
	author = {Altche, Florent and Qian, Xiangjun and {De La Fortelle}, Arnaud},
	doi = {10.1109/TITS.2017.2736532},
	issn = {15249050},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	keywords = {Semi-autonomous driving,safety,supervised driving,supervisor},
	title = {{An Algorithm for Supervised Driving of Cooperative Semi-Autonomous Vehicles}},
	year = {2017}
}


@Comment{Chap2_1_3,
	title = {Imitation Learning},
}

@incollection{Pomerleau89,
	title = {ALVINN: An Autonomous Land Vehicle in a Neural Network},
	author = {Pomerleau, Dean A.},
	booktitle = {Advances in Neural Information Processing Systems 1},
	editor = {D. S. Touretzky},
	pages = {305--313},
	year = {1989},
	publisher = {Morgan-Kaufmann},
	url = {http://papers.nips.cc/paper/95-alvinn-an-autonomous-land-vehicle-in-a-neural-network.pdf}
}

@misc{Levine2016,
	abstract = {Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.},
	author = {Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
	booktitle = {Journal of Machine Learning Research},
	issn = {15337928},
	keywords = {Neural networks,Optimal control,Reinforcement learning,Vision},
	title = {{End-to-end training of deep visuomotor policies}},
	year = {2016}
}

@inproceedings{Ross2011,
	abstract = {Sequential prediction problems such as imitation learning, where future observations depend on previous predictions (actions), violate the common i.i.d. assumptions made in statistical learning. This leads to poor performance in theory and often in practice. Some recent approaches (Daum{\'{e}} III et al., 2009; Ross and Bagnell, 2010) provide stronger guarantees in this setting, but remain somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations. In this paper, we propose a new iterative algorithm, which trains a stationary deterministic policy, that can be seen as a no regret algorithm in an online learning setting. We show that any such no regret algorithm, combined with additional reduction assumptions, must find a policy with good performance under the distribution of observations it induces in such sequential settings. We demonstrate that this new approach outperforms previous approaches on two challenging imitation learning problems and a benchmark sequence labeling problem. Copyright 2011 by the authors.},
	author = {Ross, St{\'{e}}phane and Gordon, Geoffrey J. and Bagnell, J. Andrew},
	booktitle = {Journal of Machine Learning Research},
	issn = {15324435},
	title = {{A reduction of imitation learning and structured prediction to no-regret online learning}},
	year = {2011}
}

@article{Bojarski2016,
	author    = {Mariusz Bojarski and
	Davide Del Testa and
	Daniel Dworakowski and
	Bernhard Firner and
	Beat Flepp and
	Prasoon Goyal and
	Lawrence D. Jackel and
	Mathew Monfort and
	Urs Muller and
	Jiakai Zhang and
	Xin Zhang and
	Jake Zhao and
	Karol Zieba},
	title     = {End to End Learning for Self-Driving Cars},
	journal   = {CoRR},
	volume    = {abs/1604.07316},
	year      = {2016},
	url       = {http://arxiv.org/abs/1604.07316},
	archivePrefix = {arXiv},
	eprint    = {1604.07316},
	timestamp = {Mon, 13 Aug 2018 16:47:06 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/BojarskiTDFFGJM16.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Xu2017,
	abstract = {Robust perception-action models should be learned from training data with diverse visual appearances and realistic behaviors, yet current approaches to deep visuomotor policy learning have been generally limited to in-situ models learned from a single vehicle or simulation environment. We advocate learning a generic vehicle motion model from large scale crowd-sourced video data, and develop an endto-end trainable architecture for learning to predict a distribution over future vehicle egomotion from instantaneous monocular camera observations and previous vehicle state. Our model incorporates a novel FCN-LSTM architecture, which can be learned from large-scale crowd-sourced vehicle action data, and leverages available scene segmentation side tasks to improve performance under a privileged learning paradigm. We provide a novel large-scale dataset of crowd-sourced driving behavior suitable for training our model, and report results predicting the driver action on held out sequences across diverse conditions.},
	archivePrefix = {arXiv},
	arxivId = {1612.01079},
	author = {Xu, Huazhe and Gao, Yang and Yu, Fisher and Darrell, Trevor},
	booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
	doi = {10.1109/CVPR.2017.376},
	eprint = {1612.01079},
	isbn = {9781538604571},
	title = {{End-to-end learning of driving models from large-scale video datasets}},
	year = {2017}
}

@misc{Eraqi2017,
	title={End-to-End Deep Learning for Steering Autonomous Vehicles Considering Temporal Dependencies},
	author={Hesham M. Eraqi and Mohamed N. Moustafa and Jens Honer},
	year={2017},
	eprint={1710.03804},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@inproceedings{Ho2016,
	abstract = {Consider learning a policy from example expert behavior, without interaction with the expert or access to a reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments.},
	archivePrefix = {arXiv},
	arxivId = {1606.03476},
	author = {Ho, Jonathan and Ermon, Stefano},
	booktitle = {Advances in Neural Information Processing Systems},
	eprint = {1606.03476},
	issn = {10495258},
	title = {{Generative adversarial imitation learning}},
	year = {2016}
}

@inproceedings{Kuefler2017,
	author={A. {Kuefler} and J. {Morton} and T. {Wheeler} and M. {Kochenderfer}},
	booktitle={2017 IEEE Intelligent Vehicles Symposium (IV)},
	title={Imitating driver behavior with generative adversarial networks},
	year={2017},
	pages={204-211},
}

@article{Bhattacharyya2018,
	author    = {Raunak P. Bhattacharyya and
	Derek J. Phillips and
	Blake Wulfe and
	Jeremy Morton and
	Alex Kuefler and
	Mykel J. Kochenderfer},
	title     = {Multi-Agent Imitation Learning for Driving Simulation},
	journal   = {CoRR},
	volume    = {abs/1803.01044},
	year      = {2018},
	url       = {http://arxiv.org/abs/1803.01044},
	archivePrefix = {arXiv},
	eprint    = {1803.01044},
	timestamp = {Mon, 13 Aug 2018 16:46:17 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1803-01044.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Codevilla2018,
	abstract = {Deep networks trained on demonstrations of human driving have learned to follow roads and avoid obstacles. However, driving policies trained via imitation learning cannot be controlled at test time. A vehicle trained end-to-end to imitate an expert cannot be guided to take a specific turn at an upcoming intersection. This limits the utility of such systems. We propose to condition imitation learning on high-level command input. At test time, the learned driving policy functions as a chauffeur that handles sensorimotor coordination but continues to respond to navigational commands. We evaluate different architectures for conditional imitation learning in vision-based driving. We conduct experiments in realistic three-dimensional simulations of urban driving and on a 1/5 scale robotic truck that is trained to drive in a residential area. Both systems drive based on visual input yet remain responsive to high-level navigational commands.},
	archivePrefix = {arXiv},
	arxivId = {1710.02410},
	author = {Codevilla, Felipe and Miiller, Matthias and Lopez, Antonio and Koltun, Vladlen and Dosovitskiy, Alexey},
	booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
	doi = {10.1109/ICRA.2018.8460487},
	eprint = {1710.02410},
	isbn = {9781538630815},
	issn = {10504729},
	title = {{End-to-End Driving Via Conditional Imitation Learning}},
	year = {2018}
}

@inproceedings{Rhinehart2020,
	title={Deep Imitative Models for Flexible Inference, Planning, and Control},
	author={Nicholas Rhinehart and Rowan McAllister and Sergey Levine},
	year={2020},
	booktitle={International Conference on Learning Representations (ICLR)}
}

@Comment{Chap2_2,
	title =	{Partial Observability},
}

@misc{ObjCode2017,
	author = {{Editions Nationales du Permis de Conduire}},
	title = {{Objectif Code !}},
	year = {2017}
}

@article{Astrom1965,
	author       = {Åström, Karl Johan},
	issn         = {0022-247X},
	language     = {eng},
	pages        = {174--205},
	publisher    = {Elsevier},
	journal      = {Journal of Mathematical Analysis and Applications},
	title        = {Optimal Control of Markov Processes with Incomplete State Information I},
	url          = {https://lup.lub.lu.se/search/ws/files/5323668/8867085.pdf},
	doi          = {10.1016/0022-247X(65)90154-X},
	volume       = {10},
	number       = {1},
	year         = {1965},
}

@inproceedings{Ulbrich2013,
	abstract = {The Stadtpilot project aims at fully automated driving on Braunschweig's inner city ring road. The TU Braunschweig's research vehicle 'Leonie' is one of the first vehicles having the ability of fully automated driving in real urban traffic scenarios. This paper shows our decision making approach for performing lane changes while driving fully automated in urban environments. We apply an online Partially Observable Markov Decision Process (POMDP) to accommodate inevitable sensor noise to be faced in urban traffic scenarios. In this paper we propose a two step algorithm to keep the complexity of the POMDP low enough for real-time decision making while driving. The presented approach has been integrated in our vehicle and was evaluated in real urban traffic. {\textcopyright} 2013 IEEE.},
	author = {Ulbrich, Simon and Maurer, Markus},
	booktitle = {IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC},
	doi = {10.1109/ITSC.2013.6728533},
	isbn = {9781479929146},
	title = {{Probabilistic online POMDP decision making for lane changes in fully automated driving}},
	year = {2013}
}

@article{Du2010,
	abstract = {Motion planning in uncertain and dynamic environments is critical for reliable operation of autonomous robots. Partially observable Markov decision processes (POMDPs) provide a principled general framework for such planning tasks and have been successfully applied to several moderately complex robotic tasks, including navigation, manipulation, and target tracking. The challenge now is to scale up POMDP planning algorithms and handle more complex, realistic tasks. This paper outlines ideas aimed at overcoming two major obstacles to the efficiency of POMDP planning: the “curse of dimensionality” and the “curse of history”. Our main objective is to show that using these ideas along with others POMDP algorithms can be used successfully for motion planning under uncertainty for robotic tasks with a large number of states or a long time horizon. We implemented some of our algorithms as a software package Ap- proximate POMDP Planning Library (APPL), now available for download at http://motion.comp.nus.edu.sg/ projects/pomdp/pomdp.html. Introduction},
	author = {Du, Yanzhu and Hsu, David and Kurniawati, Hanna and Lee, Wee and Ong, Sylvie and Png, Shao},
	journal = {ICAPS POMDP Practitioners Workshop},
	title = {{A POMDP Approach to Robot Motion Planning Under Uncertainty}},
	year = {2010}
}

@inproceedings{Brechtel2013,
	abstract = {Discrete POMDPs of medium complexity can be approximately solved in reasonable time. However, most applications have a continuous and thus uncountably infinite state space. We propose the novel concept of learning a discrete representation of the continuous state space to solve the integrals in continuous POMDPs efficiently and generalize sparse calculations over the continuous space. The representation is iteratively refined as part of a novel Value Iteration step and does not depend on prior knowledge. Consistency for the learned generalization is asserted by a self-correction algorithm. The presented concept is implemented for continuous state and observation spaces based on Monte Carlo approximation to allow for arbitrary POMDP models. In an experimental comparison it yields higher values in significantly shorter time than state of the art algorithms and solves higher-dimensional problems. Copyright 2013 by the author(s).},
	author = {Brechtel, Sebastian and Gindele, Tobias and Dillmann, R{\"{u}}diger},
	booktitle = {30th International Conference on Machine Learning, ICML 2013},
	title = {{Solving continuous POMDPs: Value iteration with incremental learning of an efficient space representation}},
	year = {2013}
}

@inproceedings{Bouton2018,
	author={M. {Bouton} and A. {Nakhaei} and K. {Fujimura} and M. J. {Kochenderfer}},
	booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)}, 
	title={Scalable Decision Making with Sensor Occlusions for Autonomous Driving}, 
	year={2018},
	volume={},
	number={},
	pages={2076-2081},
}

@inproceedings{VanDenBerg2011,
	abstract = {In this paper we present LQG-MP (linear-quadratic Gaussian motion planning), a new approach to robot motion planning that takes into account the sensors and the controller that will be used during the execution of the robot's path. LQG-MP is based on the linear-quadratic controller with Gaussian models of uncertainty, and explicitly characterizes in advance (i.e. before execution) the a priori probability distributions of the state of the robot along its path. These distributions can be used to assess the quality of the path, for instance by computing the probability of avoiding collisions. Many methods can be used to generate the required ensemble of candidate paths from which the best path is selected; in this paper we report results using rapidly exploring random trees (RRT). We study the performance of LQG-MP with simulation experiments in three scenarios: (A) a kinodynamic car-like robot, (B) multi-robot planning with differential-drive robots, and (C) a 6-DOF serial manipulator. We also present a method that applies Kalman smoothing to make paths Ck-continuous and apply LQG-MP to precomputed roadmaps using a variant of Dijkstra's algorithm to efficiently find high-quality paths. {\textcopyright} The Author(s) 2011.},
	author = {{Van Den Berg}, Jur and Abbee, Pieter and Goldberg, Ken},
	booktitle = {International Journal of Robotics Research},
	doi = {10.1177/0278364911406562},
	issn = {02783649},
	keywords = {Control,Planning,Uncertainty},
	title = {{LQG-MP: Optimized path planning for robots with motion uncertainty and imperfect state information}},
	year = {2011}
}

@inproceedings{Brechtel2014,
	abstract = {This paper presents a generic approach for tactical decision-making under uncertainty in the context of driving. The complexity of this task mainly stems from the fact that rational decision-making in this context must consider several sources of uncertainty: The temporal evolution of situations cannot be predicted without uncertainty because other road users behave stochastically and their goals and plans cannot be measured. Even more important, road users are only able to perceive a tiny part of the current situation with their sensors because measurements are noisy and most of the environment is occluded. In order to anticipate the consequences of decisions a probabilistic approach, considering both forms of uncertainty, is necessary. We address this by formulating the task of driving as a continuous Partially Observable Markov Decision Process (POMDP) that can be automatically optimized for different scenarios. As driving is a continuous-space problem, the belief space is infinite-dimensional. We do not use a symbolic representation or discretize the state space a priori because there is no representation of the state space that is optimal for every situation. Instead, we employ a continuous POMDP solver that learns a good representation of the specific situation.},
	author = {Brechtel, Sebastian and Gindele, Tobias and Dillmann, Rudiger},
	booktitle = {2014 17th IEEE International Conference on Intelligent Transportation Systems, ITSC 2014},
	doi = {10.1109/ITSC.2014.6957722},
	isbn = {9781479960781},
	title = {{Probabilistic decision-making under uncertainty for autonomous driving using continuous POMDPs}},
	year = {2014}
}

@inproceedings{Bandyopadhyay2013,
	abstract = {As robots venture into new application domains as autonomous vehicles on the road or as domestic helpers at home, they must recognize human intentions and behaviors in order to operate effectively. This paper investigates a new class of motion planning problems with uncertainty in human intention. We propose a method for constructing a practical model by assuming a finite set of unknown intentions. We first construct a motion model for each intention in the set and then combine these models together into a single Mixed Observability Markov Decision Process (MOMDP), which is a structured variant of the more common Partially Observable Markov Decision Process (POMDP). By leveraging the latest advances in POMDP/MOMDP approximation algorithms, we can construct and solve moderately complex models for interesting robotic tasks. Experiments in simulation and with an autonomous vehicle show that the proposed method outperforms common alternatives because of its ability in recognizing intentions and using the information effectively for decision making.},
	author = {Bandyopadhyay, Tirthankar and Won, Kok Sung and Frazzoli, Emilio and Hsu, David and Lee, Wee Sun and Rus, Daniela},
	booktitle = {Springer Tracts in Advanced Robotics},
	doi = {10.1007/978-3-642-36279-8_29},
	isbn = {9783642362781},
	issn = {1610742X},
	title = {{Intention-aware motion planning}},
	year = {2013}
}

@inproceedings{Barbier2018,
	title = {{Probabilistic Decision-Making at Road Intersections: Formulation and Quantitative Evaluation}},
	author = {Barbier, Mathieu and Laugier, Christian and Simonin, Olivier and Iba{\~n}ez-Guzm{\'a}n, Javier},
	url = {https://hal.inria.fr/hal-01940392},
	booktitle = {{ICARCV 2018 - 15 th International Conference on Control, Automation, Robotics and Vision}},
	address = {Singapour, Singapore},
	pages = {1-8},
	year = {2018},
	month = Nov,
	pdf = {https://hal.inria.fr/hal-01940392/file/paper_icarcv_2018_submitted.pdf},
	hal_id = {hal-01940392},
	hal_version = {v1},
}

@inproceedings{Bouton2017,
	abstract = {Urban intersections represent a complex environment for autonomous vehicles with many sources of uncertainty. The vehicle must plan in a stochastic environment with potentially rapid changes in driver behavior. Providing an efficient strategy to navigate through urban intersections is a difficult task. This paper frames the problem of navigating unsignalized intersections as a partially observable Markov decision process (POMDP) and solves it using a Monte Carlo sampling method. Empirical results in simulation show that the resulting policy outperforms a threshold-based heuristic strategy on several relevant metrics that measure both safety and efficiency.},
	archivePrefix = {arXiv},
	arxivId = {1704.04322},
	author = {Bouton, Maxime and Cosgun, Akansel and Kochenderfer, Mykel J.},
	booktitle = {IEEE Intelligent Vehicles Symposium, Proceedings},
	doi = {10.1109/IVS.2017.7995818},
	eprint = {1704.04322},
	isbn = {9781509048045},
	title = {{Belief state planning for autonomously navigating urban intersections}},
	year = {2017}
}


@inproceedings{Sunberg2017,
	abstract = {Safe interaction with human drivers is one of the primary challenges for autonomous vehicles. In order to plan driving maneuvers effectively, the vehicle's control system must infer and predict how humans will behave based on their latent internal state (e.g., intentions and aggressiveness). This research uses a simple model for human behavior with unknown parameters that make up the internal states of the traffic participants and presents a method for quantifying the value of estimating these states and planning with their uncertainty explicitly modeled. An upper performance bound is established by an omniscient Monte Carlo Tree Search (MCTS) planner that has perfect knowledge of the internal states. A baseline lower bound is established by planning with MCTS assuming that all drivers have the same internal state. MCTS variants are then used to solve a partially observable Markov decision process (POMDP) that models the internal state uncertainty to determine whether inferring the internal state offers an advantage over the baseline. Applying this method to a freeway lane changing scenario reveals that there is a significant performance gap between the upper bound and baseline. POMDP planning techniques come close to closing this gap, especially when important hidden model parameters are correlated with measurable parameters.},
	archivePrefix = {arXiv},
	arxivId = {1702.00858},
	author = {Sunberg, Zachary N. and Ho, Christopher J. and Kochenderfer, Mykel J.},
	booktitle = {Proceedings of the American Control Conference},
	doi = {10.23919/ACC.2017.7963408},
	eprint = {1702.00858},
	isbn = {9781509059928},
	issn = {07431619},
	title = {{The value of inferring the internal state of traffic participants for autonomous freeway driving}},
	year = {2017}
}

@inproceedings{Bry2011,
	abstract = {In this paper we address the problem of motion planning in the presence of state uncertainty, also known as planning in belief space. The work is motivated by planning domains involving nontrivial dynamics, spatially varying measurement properties, and obstacle constraints. To make the problem tractable, we restrict the motion plan to a nominal trajectory stabilized with a linear estimator and controller. This allows us to predict distributions over future states given a candidate nominal trajectory. Using these distributions to ensure a bounded probability of collision, the algorithm incrementally constructs a graph of trajectories through state space, while efficiently searching over candidate paths through the graph at each iteration. This process results in a search tree in belief space that provably converges to the optimal path. We analyze the algorithm theoretically and also provide simulation results demonstrating its utility for balancing information gathering to reduce uncertainty and finding low cost paths. {\textcopyright} 2011 IEEE.},
	author = {Bry, Adam and Roy, Nicholas},
	booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
	doi = {10.1109/ICRA.2011.5980508},
	isbn = {9781612843865},
	issn = {10504729},
	title = {{Rapidly-exploring random belief trees for motion planning under uncertainty}},
	year = {2011}
}

@inproceedings{Xu2014,
	abstract = {We present a motion planning framework for autonomous on-road driving considering both the uncertainty caused by an autonomous vehicle and other traffic participants. The future motion of traffic participants is predicted using a local planner, and the uncertainty along the predicted trajectory is computed based on Gaussian propagation. For the autonomous vehicle, the uncertainty from localization and control is estimated based on a Linear-Quadratic Gaussian (LQG) framework. Compared with other safety assessment methods, our framework allows the planner to avoid unsafe situations more efficiently, thanks to the direct uncertainty information feedback to the planner. We also demonstrate our planner's ability to generate safer trajectories compared to planning only with a LQG framework.},
	author = {Xu, Wenda and Pan, Jia and Wei, Junqing and Dolan, John M.},
	booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
	doi = {10.1109/ICRA.2014.6907209},
	issn = {10504729},
	title = {{Motion planning under uncertainty for on-road autonomous driving}},
	year = {2014}
}

@inproceedings{Pineau2003,
	abstract = {This paper introduces the Point-Based Value Iteration (PBVI) algorithm for POMDP planning. PBVI approximates an exact value iteration solution by selecting a small set of representative belief points and then tracking the value and its derivative for those points only. By using stochastic trajectories to choose belief points, and by maintaining only one value hyper-plane per point, PBVI successfully solves large problems: we present results on a robotic laser tag problem as well as three test domains from the literature.},
	author = {Pineau, Joelle and Gordon, Geoff and Thrun, Sebastian},
	booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
	issn = {10450823},
	title = {{Point-based value iteration: An anytime algorithm for POMDPs}},
	year = {2003}
}

@article{Porta2006,
	abstract = {We propose a novel approach to optimize Partially Observable Markov Decisions Processes (POMDPs) defined on continuous spaces. To date, most algorithms for model-based POMDPs are restricted to discrete states, actions, and observations, but many real-world problems such as, for instance, robot navigation, are naturally defined on continuous spaces. In this work, we demonstrate that the value function for continuous POMDPs is convex in the beliefs over continuous state spaces, and piecewise-linear convex for the particular case of discrete observations and actions but still continuous states. We also demonstrate that continuous Bellman backups are contracting and isotonic ensuring the monotonic convergence of value-iteration algorithms. Relying on those properties, we extend the PERSEUS algorithm, originally developed for discrete POMDPs, to work in continuous state spaces by representing the observation, transition, and reward models using Gaussian mixtures, and the beliefs using Gaussian mixtures or particle sets. With these representations, the integrals that appear in the Bellman backup can be computed in closed form and, therefore, the algorithm is computationally feasible. Finally, we further extend PERSEUS to deal with continuous action and observation sets by designing effective sampling approaches.},
	author = {Porta, Josep M. and Vlassis, Nikos and Spaan, Matthijs T.J. and Poupart, Pascal},
	doi = {10.13039/501100000780},
	issn = {15337928},
	journal = {Journal of Machine Learning Research},
	keywords = {Continuous action space,Continuous observation space,Continuous state space,Partially observable Markov decision processes,Planning under uncertainty,Point-based value iteration},
	title = {{Point-based value iteration for continuous POMDPs}},
	year = {2006}
}

@inproceedings{Silver2010,
	abstract = {This paper introduces a Monte-Carlo algorithm for online planning in large POMDPs. The algorithm combines a Monte-Carlo update of the agent's belief state with a Monte-Carlo tree search from the current belief state. The new algorithm, POMCP, has two important properties. First, Monte-Carlo sampling is used to break the curse of dimensionality both during belief state updates and during planning. Second, only a black box simulator of the POMDP is required, rather than explicit probability distributions. These properties enable POMCP to plan effectively in significantly larger POMDPs than has previously been possible. We demonstrate its effectiveness in three large POMDPs. We scale up a well-known benchmark problem, rocksample, by several orders of magnitude. We also introduce two challenging new POMDPs: 10 × 10 battleship and partially observable PacMan, with approximately 10 18 and 10 56 states respectively. Our Monte-Carlo planning algorithm achieved a high level of performance with no prior knowledge, and was also able to exploit simple domain knowledge to achieve better results with less search. POMCP is the first general purpose planner to achieve high performance in such large and unfactored POMDPs.},
	author = {Silver, David and Veness, Joel},
	booktitle = {Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010, NIPS 2010},
	isbn = {9781617823800},
	title = {{Monte-Carlo planning in large POMDPs}},
	year = {2010}
}

@article{Kalman1960,
	abstract = {The classical filtering and prediction problem is re-examined using the Bode-Sliannon representation of random processes and the “state-transition” method of analysis of dynamic systems. New results are: (1) The formulation and methods of solution of the problem apply without modification to stationary and nonstationary statistics and to growing-memory and infinitememory filters. (2) A nonlinear difference (or differential) equation is derived for the covariance matrix of the optimal estimation error. From the solution of this equation the coefficients of the difference (or differential) equation of the optimal linear filter are obtained without further calculations. (3) The filtering problem is shown to be the dual of the noise-free regulator problem. The new method developed here is applied to two well-known problems, confirming and extending earlier results. The discussion is largely self-contained and proceeds from first principles; basic concepts of the theory of random processes are reviewed in the Appendix. {\textcopyright} 1960 by ASME.},
	author = {Kalman, R. E.},
	doi = {10.1115/1.3662552},
	issn = {1528901X},
	journal = {Journal of Fluids Engineering, Transactions of the ASME},
	title = {{A new approach to linear filtering and prediction problems}},
	year = {1960}
}

@inproceedings{VanDenBerg2017,
	abstract = {We present an approach to motion planning under motion and sensing un-certainty, formally described as a continuous partially-observable Markov decision process (POMDP). Our approach is designed for non-linear dynamics and observation models, and follows the general POMDP solution framework in which we represent beliefs by Gaussian distributions, approximate the belief dynamics using an extended Kalman filter (EKF), and represent the value function by a quadratic function that is valid in the vicinity of a nominal trajectory through belief space. Using a variant of differential dynamic programming, our approach iterates with second-order convergence towards a linear control policy over the belief space that is locally-optimal with respect to a user-defined cost function. Unlike previous work, our approach does not assume maximum-likelihood observations, does not assume fixed estimator or control gains, takes into account obstacles in the environment, and does not require discretization of the belief space. The running time of the algorithm is polynomial in the dimension of the state space. We demonstrate the potential of our approach in several continuous partially-observable planning domains with obstacles for robots with non-linear dynamics and observation models.},
	author = {{Van Den Berg}, Jur and Patil, Sachin and Alterovitz, Ron},
	booktitle = {Springer Tracts in Advanced Robotics},
	doi = {10.1007/978-3-319-29363-9_27},
	isbn = {9783319293622},
	issn = {1610742X},
	title = {{Motion planning under uncertainty using differential dynamic programming in belief space}},
	year = {2017}
}

@inproceedings{Brechtel2011,
	abstract = {This paper presents a method for high-level decision making in traffic environments. In contrast to the usual approach of modeling decision policies by hand, a Markov Decision Process (MDP) is employed to plan the optimal policy by assessing the outcomes of actions. Using probability theory, decisions are deduced automatically from the knowledge about how road users behave over time. This approach does neither depend on an explicit situation recognition nor is it limited to only a variety of situations or types of descriptions. Hence it is versatile and powerful. The contribution of this paper is a mathematical framework to derive abstract symbolic states from complex continuous temporal models encoded as Dynamic Bayesian Networks (DBN). For this purpose discrete MDP states are interpreted by random variables. To make computation feasible this space grows adaptively during planning and according to the problem to be solved. {\textcopyright} 2011 IEEE.},
	author = {Brechtel, Sebastian and Gindele, Tobias and Dillmann, Rudiger},
	booktitle = {IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC},
	doi = {10.1109/ITSC.2011.6082928},
	isbn = {9781457721984},
	title = {{Probabilistic MDP-behavior planning for cars}},
	year = {2011}
}

@inproceedings{Sun2019,
	abstract = {Autonomous cars have to navigate in dynamic environment which can be full of uncertainties. The uncertainties can come either from sensor limitations such as occlusions and limited sensor range, or from probabilistic prediction of other road participants, or from unknown social behavior in a new area. To safely and efficiently drive in the presence of these uncertainties, the decision-making and planning modules of autonomous cars should intelligently utilize all available information and appropriately tackle the uncertainties so that proper driving strategies can be generated. In this paper, we propose a social perception scheme which treats all road participants as distributed sensors in a sensor network. By observing the individual behaviors as well as the group behaviors, uncertainties of the three types can be updated uniformly in a belief space. The updated beliefs from the social perception are then explicitly incorporated into a probabilistic planning framework based on Model Predictive Control (MPC). The cost function of the MPC is learned via inverse reinforcement learning (IRL). Such an integrated probabilistic planning module with socially enhanced perception enables the autonomous vehicles to generate behaviors which are defensive but not overly conservative, and socially compatible. The effectiveness of the proposed framework is verified in simulation on an representative scenario with sensor occlusions.},
	archivePrefix = {arXiv},
	arxivId = {1905.00988},
	author = {Sun, Liting and Zhan, Wei and Chan, Ching Yao and Tomizuka, Masayoshi},
	booktitle = {IEEE Intelligent Vehicles Symposium, Proceedings},
	doi = {10.1109/IVS.2019.8814223},
	eprint = {1905.00988},
	isbn = {9781728105604},
	title = {{Behavior planning of autonomous cars with social perception}},
	year = {2019}
}

@Comment{Chap2_3,
	title =	{Temporal abstraction},
}

@article{Sutton1999,
	abstract = {Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key, longstanding challenges in AI. The ways how to address these challenges within the mathematical framework of reinforcement learning and Markov decision processes (MDP) are discussed. A set options defined over an MDP constitutes a semi-MDP (SMDP), and the theory of SMDPs provides the foundation for the theory of options.},
	author = {Sutton, Richard S. and Precup, Doina and Singh, Satinder},
	doi = {10.1016/S0004-3702(99)00052-1},
	issn = {00043702},
	journal = {Artificial Intelligence},
	title = {{Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning}},
	year = {1999}
}

@misc{ShalevShwartz2017,
    title={On a Formal Model of Safe and Scalable Self-driving Cars},
    author={Shai Shalev-Shwartz and Shaked Shammah and Amnon Shashua},
    year={2017},
    eprint={1708.06374},
    archivePrefix={arXiv},
    primaryClass={cs.RO}
}

@misc{ShalevShwartz2016,
    title={Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving},
    author={Shai Shalev-Shwartz and Shaked Shammah and Amnon Shashua},
    year={2016},
    eprint={1610.03295},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@inproceedings{Paxton2017,
	abstract = {Task and motion planning subject to Linear Temporal Logic (LTL) specifications in complex, dynamic environments requires efficient exploration of many possible future worlds. Model-free reinforcement learning has proven successful in a number of challenging tasks, but shows poor performance on tasks that require long-term planning. In this work, we integrate Monte Carlo Tree Search with hierarchical neural net policies trained on expressive LTL specifications. We use reinforcement learning to find deep neural networks representing both low-level control policies and task-level 'option policies' that achieve high-level goals. Our combined architecture generates safe and responsive motion plans that respect the LTL constraints. We demonstrate our approach in a simulated autonomous driving setting, where a vehicle must drive down a road in traffic, avoid collisions, and navigate an intersection, all while obeying rules of the road.},
	archivePrefix = {arXiv},
	arxivId = {1703.07887},
	author = {Paxton, Chris and Raman, Vasumathi and Hager, Gregory D. and Kobilarov, Marin},
	booktitle = {IEEE International Conference on Intelligent Robots and Systems},
	doi = {10.1109/IROS.2017.8206505},
	eprint = {1703.07887},
	isbn = {9781538626825},
	issn = {21530866},
	title = {{Combining neural networks and tree search for task and motion planning in challenging environments}},
	year = {2017}
}

@inproceedings{Bacon2017,
	abstract = {Temporal abstraction is key to scaling up learning and planning in reinforcement learning. While planning with temporally extended actions is well understood, creating such abstractions autonomously from data has remained challenging. We tackle this problem in the framework of options [Sutton, Precup {\&}Singh, 1999; Precup, 2000]. We derive policy gradient theorems for options and propose a new option-critic architecture capable of learning both the internal policies and the termination conditions of options, in tandem with the policy over options, and without the need to provide any additional rewards or subgoals. Experimental results in both discrete and continuous environments showcase the flexibility and efficiency of the framework.},
	archivePrefix = {arXiv},
	arxivId = {1609.05140},
	author = {Bacon, Pierre Luc and Harb, Jean and Precup, Doina},
	booktitle = {31st AAAI Conference on Artificial Intelligence, AAAI 2017},
	eprint = {1609.05140},
	title = {{The option-critic architecture}},
	year = {2017}
}

@inproceedings{Vezhnevets2017,
abstract = {We introduce FeUdal Networks (FuNs): a novel architecture for hierarchical reinforcement learning. Our approach is inspired by the feudal reinforcement learning proposal of Dayan and Hinton, and gains power and efficacy by decoupling end-to-end learning across multiple levels-allowing it to utilise different resolutions of time. Our framework employs a Manager module and a Worker module. The Manager operates at a lower temporal resolution and sets abstract goals which are conveyed to and enacted by the Worker. The Worker generates primitive actions at every tick of the environment. The decoupled structure of FuN conveys several benefits-in ad-dition to facilitating very long timescale credit assignment it also encourages the emergence of sub-policies associated with different goals set by the Manager. These properties allow FuN to dramatically outperform a strong baseline agent on tasks that involve long-term credit assignment or memorisation.},
archivePrefix = {arXiv},
arxivId = {1703.01161},
author = {Vezhnevets, Alexander Sasha and Osindero, Simon and Schaul, Tom and Heess, Nicolas and Jaderberg, Max and Silver, David and Kavukcuoglu, Koray},
booktitle = {34th International Conference on Machine Learning, ICML 2017},
eprint = {1703.01161},
isbn = {9781510855144},
title = {{FeUdal networks for hierarchical reinforcement learning}},
year = {2017}
}

@misc{Heess2016,
    title={Learning and Transfer of Modulated Locomotor Controllers},
    author={Nicolas Heess and Greg Wayne and Yuval Tassa and Timothy Lillicrap and Martin Riedmiller and David Silver},
    year={2016},
    eprint={1610.05182},
    archivePrefix={arXiv},
    primaryClass={cs.RO}
}


@Comment{Chap2_4,
	title = {Inverse Reinforcement Learning},
}

@inproceedings{Kuderer2015,
	abstract = {It is expected that autonomous vehicles capable of driving without human supervision will be released to market within the next decade. For user acceptance, such vehicles should not only be safe and reliable, they should also provide a comfortable user experience. However, individual perception of comfort may vary considerably among users. Whereas some users might prefer sporty driving with high accelerations, others might prefer a more relaxed style. Typically, a large number of parameters such as acceleration profiles, distances to other cars, speed during lane changes, etc., characterize a human driver's style. Manual tuning of these parameters may be a tedious and error-prone task. Therefore, we propose a learning from demonstration approach that allows the user to simply demonstrate the desired style by driving the car manually. We model the individual style in terms of a cost function and use feature-based inverse reinforcement learning to find the model parameters that fit the observed style best. Once the model has been learned, it can be used to efficiently compute trajectories for the vehicle in autonomous mode. We show that our approach is capable of learning cost functions and reproducing different driving styles using data from real drivers.},
	author = {Kuderer, Markus and Gulati, Shilpa and Burgard, Wolfram},
	booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
	doi = {10.1109/ICRA.2015.7139555},
	issn = {10504729},
	title = {{Learning driving styles for autonomous vehicles from demonstration}},
	year = {2015}
}

@inproceedings{Ziebart2008,
	abstract = {We present PROCAB, an efficient method for Probabilistically Reasoning from Observed Context-Aware Behavior. It models the context-dependent utilities and underlying reasons that people take different actions. The model generalizes to unseen situations and scales to incorporate rich contextual information. We train our model using the route preferences of 25 taxi drivers demonstrated in over 100,000 miles of collected data, and demonstrate the performance of our model by inferring: (1) decision at next intersection, (2) route to known destination, and (3) destination given partially traveled route. {\textcopyright} 2008 ACM.},
	author = {Ziebart, Brian D. and Maas, Andrew L. and Dey, Anind K. and Bagnell, J. Andrew},
	booktitle = {UbiComp 2008 - Proceedings of the 10th International Conference on Ubiquitous Computing},
	doi = {10.1145/1409635.1409678},
	isbn = {9781605581361},
	keywords = {Decision modeling,Route prediction,Vehicle navigation},
	title = {{Navigate like a cabbie: Probabilistic reasoning from observed context-aware behavior}},
	year = {2008}
}

@inproceedings{Ziebart2009,
	abstract = {We present a novel approach for determining robot movements that efficiently accomplish the robot's tasks while not hindering the movements of people within the environment. Our approach models the goal-directed trajectories of pedestrians using maximum entropy inverse optimal control. The advantage of this modeling approach is the generality of its learned cost function to changes in the environment and to entirely different environments. We employ the predictions of this model of pedestrian trajectories in a novel incremental planner and quantitatively show the improvement in hindrance-sensitive robot trajectory planning provided by our approach. {\textcopyright} 2009 IEEE.},
	author = {Ziebart, Brian D. and Ratliff, Nathan and Gallagher, Garratt and Mertz, Christoph and Peterson, Kevin and Bagnell, J. Andrew and Hebert, Martial and Dey, Anind K. and Srinivasa, Siddhartha},
	booktitle = {2009 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2009},
	doi = {10.1109/IROS.2009.5354147},
	isbn = {9781424438044},
	title = {{Planning-based prediction for pedestrians}},
	year = {2009}
}


@inproceedings{Sadigh2016,
	abstract = {Traditionally, autonomous cars make predictions about other drivers' future trajectories, and plan to stay out of their way. This tends to result in defensive and opaque behaviors. Our key insight is that an autonomous car's actions will actually affect what other cars will do in response, whether the car is aware of it or not. Our thesis is that we can leverage these responses to plan more efficient and communicative behaviors. We model the interaction between an autonomous car and a human driver as a dynamical system, in which the robot's actions have immediate consequences on the state of the car, but also on human actions. We model these consequences by approximating the human as an optimal planner, with a reward function that we acquire through Inverse Reinforcement Learning. When the robot plans with this reward function in this dynamical system, it comes up with actions that purposefully change human state: it merges in front of a human to get them to slow down or to reach its own goal faster; it blocks two lanes to get them to switch to a third lane; or it backs up slightly at an intersection to get them to proceed first. Such behaviors arise from the optimization, without relying on hand-coded signaling strategies and without ever explicitly modeling communication. Our user study results suggest that the robot is indeed capable of eliciting desired changes in human state by planning using this dynamical system.},
	author = {Sadigh, Dorsa and Sastry, Shankar and Seshia, Sanjit A. and Dragan, Anca D.},
	booktitle = {Robotics: Science and Systems},
	doi = {10.15607/rss.2016.xii.029},
	isbn = {9780992374723},
	issn = {2330765X},
	title = {{Planning for autonomous cars that leverage effects on human actions}},
	year = {2016}
}

@Comment{Chap2_5,
	title = {Transfer},
}

@misc{levine2020offline,
	title={Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems},
	author={Sergey Levine and Aviral Kumar and George Tucker and Justin Fu},
	year={2020},
	eprint={2005.01643},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@inproceedings{Laroche2019,
	title={Safe Policy Improvement with Baseline Bootstrapping},
	author={Laroche, Romain and Trichelair, Paul and Tachet des Combes, R\'emi},
	booktitle={Proceedings of the International Conference on Machine Learning (ICML)},
	year={2019}
}

@article{Nadjahi2019,
	abstract = {Batch Reinforcement Learning (Batch RL) consists in training a policy using trajectories collected with another policy, called the behavioural policy. Safe policy improvement (SPI) provides guarantees with high probability that the trained policy performs better than the behavioural policy, also called baseline in this setting. Previous work shows that the SPI objective improves mean performance as compared to using the basic RL objective, which boils down to solving the MDP with maximum likelihood. Here, we build on that work and improve more precisely the SPI with Baseline Bootstrapping algorithm (SPIBB) by allowing the policy search over a wider set of policies. Instead of binarily classifying the state-action pairs into two sets (the $\backslash$textit{\{}uncertain{\}} and the $\backslash$textit{\{}safe-to-train-on{\}} ones), we adopt a softer strategy that controls the error in the value estimates by constraining the policy change according to the local model uncertainty. The method can take more risks on uncertain actions all the while remaining provably-safe, and is therefore less conservative than the state-of-the-art methods. We propose two algorithms (one optimal and one approximate) to solve this constrained optimization problem and empirically show a significant improvement over existing SPI algorithms both on finite MDPs and on infinite MDPs with a neural network function approximation.},
	archivePrefix = {arXiv},
	arxivId = {1907.05079},
	author = {Nadjahi, Kimia and Laroche, Romain and des Combes, R{\'{e}}mi Tachet},
	eprint = {1907.05079},
	file = {:D$\backslash$:/Downloads/339.pdf:pdf},
	mendeley-groups = {Safety},
	pages = {1--17},
	title = {{Safe Policy Improvement with Soft Baseline Bootstrapping}},
	url = {http://arxiv.org/abs/1907.05079},
	year = {2019}
}

@article{Kakade2002,
	abstract = {Abstract In order to solve realistic reinforcement learning problems, it is critical that approximate algorithms be used. In this paper, we present the conservative policy iteration algorithm which finds an" approximately" optimal policy, given access to a restart ... $\backslash$n},
	author = {Kakade, Sham and Langford, John},
	isbn = {1-55860-873-7},
	journal = {Proceedings of the 19th International Conference on Machine Learning},
	title = {{Approximately Optimal Approximate Reinforcement Learning}},
	year = {2002}
}

@inproceedings{Schulman2015,
	abstract = {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
	archivePrefix = {arXiv},
	arxivId = {1502.05477},
	author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael and Abbeel, Pieter},
	booktitle = {32nd International Conference on Machine Learning, ICML 2015},
	eprint = {1502.05477},
	isbn = {9781510810587},
	title = {{Trust region policy optimization}},
	year = {2015}
}


@inproceedings{Pan2017,
	abstract = {Reinforcement learning is considered as a promising direction for driving policy learning. However, training autonomous driving vehicle with reinforcement learning in real environment involves non-affordable trial-and-error. It is more desirable to first train in a virtual environment and then transfer to the real environment. In this paper, we propose a novel realistic translation network to make model trained in virtual environment be workable in real world. The proposed network can convert non-realistic virtual image input into a realistic one with similar scene structure. Given realistic frames as input, driving policy trained by reinforcement learning can nicely adapt to real world driving. Experiments show that our proposed virtual to real (VR) reinforcement learning (RL) works pretty well. To our knowledge, this is the first successful case of driving policy trained by reinforcement learning that can adapt to real world driving data.},
	archivePrefix = {arXiv},
	arxivId = {1704.03952},
	author = {Pan, Xinlei and You, Yurong and Wang, Ziyan and Lu, Cewu},
	booktitle = {British Machine Vision Conference 2017, BMVC 2017},
	doi = {10.5244/c.31.11},
	eprint = {1704.03952},
	isbn = {190172560X},
	title = {{Virtual to real reinforcement learning for autonomous driving}},
	year = {2017}
}

@misc{Liang2019,
	title={Federated Transfer Reinforcement Learning for Autonomous Driving},
	author={Xinle Liang and Yang Liu and Tianjian Chen and Ming Liu and Qiang Yang},
	year={2019},
	eprint={1910.06001},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@inproceedings{Tobin2017,
	abstract = {Bridging the 'reality gap' that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to 1.5 cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control.},
	archivePrefix = {arXiv},
	arxivId = {1703.06907},
	author = {Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
	booktitle = {IEEE International Conference on Intelligent Robots and Systems},
	doi = {10.1109/IROS.2017.8202133},
	eprint = {1703.06907},
	isbn = {9781538626825},
	issn = {21530866},
	title = {{Domain randomization for transferring deep neural networks from simulation to the real world}},
	year = {2017}
}

@misc{openai2019solving,
	title={Solving Rubik's Cube with a Robot Hand},
	author={OpenAI and Ilge Akkaya and Marcin Andrychowicz and Maciek Chociej and Mateusz Litwin and Bob McGrew and Arthur Petron and Alex Paino and Matthias Plappert and Glenn Powell and Raphael Ribas and Jonas Schneider and Nikolas Tezak and Jerry Tworek and Peter Welinder and Lilian Weng and Qiming Yuan and Wojciech Zaremba and Lei Zhang},
	year={2019},
	eprint={1910.07113},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@inproceedings{Prakash2019,
	abstract = {We present structured domain randomization (SDR), a variant of domain randomization (DR) that takes into account the structure of the scene in order to add context to the generated data. In contrast to DR, which places objects and distractors randomly according to a uniform probability distribution, SDR places objects and distractors randomly according to probability distributions that arise from the specific problem at hand. In this manner, SDR-generated imagery enables the neural network to take the context around an object into consideration during detection. We demonstrate the power of SDR for the problem of 2D bounding box car detection, achieving competitive results on real data after training only on synthetic data. On the KITTI easy, moderate, and hard tasks, we show that SDR outperforms other approaches to generating synthetic data (VKITTI, Sim 200k, or DR), as well as real data collected in a different domain (BDD100K). Moreover, synthetic SDR data combined with real KITTI data outperforms real KITTI data alone.11Video is at http://youtu.be/1WdjWJYx9AY.},
	archivePrefix = {arXiv},
	arxivId = {1810.10093},
	author = {Prakash, Aayush and Boochoon, Shaad and Brophy, Mark and Acuna, David and Cameracci, Eric and State, Gavriel and Shapira, Omer and Birchfield, Stan},
	booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
	doi = {10.1109/ICRA.2019.8794443},
	eprint = {1810.10093},
	isbn = {9781538660263},
	issn = {10504729},
	title = {{Structured domain randomization: Bridging the reality gap by context-aware synthetic data}},
	year = {2019}
}

@inproceedings{Pouyanfar2019,
	abstract = {End-to-end deep learning has emerged as a simple and promising approach for autonomous driving recently. However, collecting large-scale real-world data representing the full spectrum of road scenarios and rare events remains the main hurdle in this area. For this purpose, this paper addresses the problem of end-to-end collision-free deep driving using only simulation data. It extends the idea of domain randomization to bridge the reality gap between simulation and the real world. Using a range of domain randomization flavors in a primitive simulation, it is shown that a model can learn to drive in realistic environments without seeing any real or photo-realistic images. The proposed work dramatically reduces the need for collecting large real-world or high-fidelity simulated datasets, along with allowing for the creation of rare events in the simulation. Finally, this is the first time domain randomization is used for the application of 'deep driving' which can avoid obstacles. The effectiveness of the proposed method is demonstrated with extensive experiments on both simulation and real-world datasets.},
	author = {Pouyanfar, Samira and Saleem, Muneeb and George, Nikhil and Chen, Shu Ching},
	booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
	doi = {10.1109/CVPRW.2019.00166},
	isbn = {9781728125060},
	issn = {21607516},
	title = {{ROADS: Randomization for obstacle avoidance and driving in simulation}},
	year = {2019}
}

@InProceedings{Mueller2018,
	title = 	 {Driving Policy Transfer via Modularity and Abstraction},
	author = 	 {Mueller, Matthias and Dosovitskiy, Alexey and Ghanem, Bernard and Koltun, Vladlen},
	booktitle = 	 {Proceedings of The 2nd Conference on Robot Learning},
	pages = 	 {1--15},
	year = 	 {2018},
	editor = 	 {Billard, Aude and Dragan, Anca and Peters, Jan and Morimoto, Jun},
	volume = 	 {87},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {},
	month = 	 {29--31 Oct},
	publisher =  {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v87/mueller18a/mueller18a.pdf},
	url = 	 {http://proceedings.mlr.press/v87/mueller18a.html},
	abstract = 	 {End-to-end approaches to autonomous driving have high sample complexity and are difficult to scale to realistic urban driving. Simulation can help end-to-end driving systems by providing a cheap, safe, and diverse training environment. Yet training driving policies in simulation brings up the problem of transferring such policies to the real world. We present an approach to transferring driving policies from simulation to reality via modularity and abstraction. Our approach is inspired by classic driving systems and aims to combine the benefits of modular architectures and end-to-end deep learning approaches. The key idea is to encapsulate the driving policy such that it is not directly exposed to raw perceptual input or low-level vehicle dynamics. We evaluate the presented approach in simulated urban environments and in the real world. In particular, we transfer a driving policy trained in simulation to a 1/5-scale robotic truck that is deployed in a variety of conditions, with no finetuning, on two continents.}
}

@incollection{Liu2017,
	title = {Unsupervised Image-to-Image Translation Networks},
	author = {Liu, Ming-Yu and Breuel, Thomas and Kautz, Jan},
	booktitle = {Advances in Neural Information Processing Systems 30},
	editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	pages = {700--708},
	year = {2017},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/6672-unsupervised-image-to-image-translation-networks.pdf}
}

@INPROCEEDINGS{Ros2016,
	author={G. {Ros} and L. {Sellart} and J. {Materzynska} and D. {Vazquez} and A. M. {Lopez}},
	booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
	title={The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes}, 
	year={2016},
	volume={},
	number={},
	pages={3234-3243},
}

@inproceedings{Cordts2016,
	abstract = {Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations, 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.},
	archivePrefix = {arXiv},
	arxivId = {1604.01685},
	author = {Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
	booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	doi = {10.1109/CVPR.2016.350},
	eprint = {1604.01685},
	isbn = {9781467388504},
	issn = {10636919},
	title = {{The Cityscapes Dataset for Semantic Urban Scene Understanding}},
	year = {2016}
}


@Comment{Chap2_6,
	title = {Safety},
}

@article{Garcia2015,
	abstract = {Safe Reinforcement Learning can be defined as the process of learning policies that maxi-mize the expectation of the return in problems in which it is important to ensure reasonable system performance and/or respect safety constraints during the learning and/or deploy-ment processes. We categorize and analyze two approaches of Safe Reinforcement Learning. The first is based on the modification of the optimality criterion, the classic discounted fi-nite/infinite horizon, with a safety factor. The second is based on the modification of the exploration process through the incorporation of external knowledge or the guidance of a risk metric. We use the proposed classification to survey the existing literature, as well as suggesting future directions for Safe Reinforcement Learning.},
	author = {Garc{\'{i}}a, Javier and Fern{\'{a}}ndez, Fernando},
	doi = {10.1109/TNNLS.2017.2654539},
	file = {::},
	issn = {15337928},
	journal = {Journal of Machine Learning Research},
	keywords = {reinforcement learning,risk sensitivity,safe exploration,teacher advice},
	pages = {1437--1480},
	title = {{A Comprehensive Survey on Safe Reinforcement Learning}},
	volume = {16},
	year = {2015}
}

@book{Markowitz59,
	ISBN = {9780300013726},
	URL = {http://www.jstor.org/stable/j.ctt1bh4c8h},
	abstract = {Applies modern techniques of analysis and computation to the problem of finding combinations of securities that best meet the needs of the private institutional investor. Written primarily with the nonmathematician in mind, although it contains mathematical development of the subject in appendixes.},
	author = {Harry M. Markowitz},
	publisher = {Yale University Press},
	title = {Portfolio Selection: Efficient Diversification of Investments},
	year = {1959}
}

@article{Moody2001,
	abstract = {We present methods for optimizing portfolios, asset allocations, and trading systems based on direct reinforcement (DR). In this approach, investment decision making is viewed as a stochastic control problem, and strategies are discovered directly. We present an adaptive algorithm called recurrent reinforcement learning (RRL) for discovering investment policies. The need to build forecasting models is eliminated, and better trading performance is obtained. The direct reinforcement approach differs from dynamic programming and reinforcement algorithms such as TD-learning and Q-learning, which attempt to estimate a value function for the control problem. We find that the RRL direct reinforcement framework enables a simpler problem representation, avoids Bellman's curse of dimensionality and offers compelling advantages in efficiency. We demonstrate how direct reinforcement can be used to optimize risk-adjusted investment returns (including the differential Sharpe ratio), while accounting for the effects of transaction costs. In extensive simulation work using real financial data, we find that our approach based on RRL produces better trading strategies than systems utilizing Q-Learning (a value function method). Real-world applications include an intra-daily currency trader and a monthly asset allocation system for the S{\&}P 500 Stock Index and T-Bills.},
	author = {Moody, John and Saffell, Matthew},
	doi = {10.1109/72.935097},
	issn = {10459227},
	journal = {IEEE Transactions on Neural Networks},
	keywords = {Differential Sharpe ratio,Direct reinforcement (DR),Downside deviation,Policy gradient,Q-learning,Recurrent reinforcement learning,Risk,TD-learning,Trading,Value function},
	title = {{Learning to trade via direct reinforcement}},
	year = {2001}
}

@inproceedings{Tamar2012,
	abstract = {Managing risk in dynamic decision problems is of cardinal importance in many fields such as finance and process control. The most common approach to defining risk is through various variance related criteria such as the Sharpe Ratio or the standard deviation adjusted reward. It is known that optimizing many of the variance related risk criteria is NP-hard. In this paper we devise a framework for local policy gradient style algorithms for reinforcement learning for variance related criteria. Our starting point is a new formula for the variance of the cost-to-go in episodic tasks. Using this formula we develop policy gradient algorithms for criteria that involve both the expected cost and the variance of the cost. We prove the convergence of these algorithms to local minima and demonstrate their applicability in a portfolio planning problem. Copyright 2012 by the author(s)/owner(s).},
	author = {Tamar, Aviv and {Di Castro}, Dotan and Mannor, Shie},
	booktitle = {Proceedings of the 29th International Conference on Machine Learning, ICML 2012},
	isbn = {9781450312851},
	title = {{Policy gradients with variance related risk criteria}},
	year = {2012}
}

@inproceedings{Prashanth2013,
	abstract = {In many sequential decision-making problems we may want to manage risk by minimizing some measure of variability in rewards in addition to maximizing a standard criterion. Variance-related risk measures are among the most common risk-sensitive criteria in finance and operations research. However, optimizing many such criteria is known to be a hard problem. In this paper, we consider both discounted and average reward Markov decision processes. For each formulation, we first define a measure of variability for a policy, which in turn gives us a set of risk-sensitive criteria to optimize. For each of these criteria, we derive a formula for computing its gradient. We then devise actor-critic algorithms for estimating the gradient and updating the policy parameters in the ascent direction. We establish the convergence of our algorithms to locally risk-sensitive optimal policies. Finally, we demonstrate the usefulness of our algorithms in a traffic signal control application.},
	author = {Prashanth, L. A. and Ghavamzadeh, Mohammad},
	booktitle = {Advances in Neural Information Processing Systems},
	issn = {10495258},
	title = {{Actor-critic algorithms for risk-sensitive MDPs}},
	year = {2013}
}

@article{Chow2018,
	abstract = {In many sequential decision-making problems one is interested in minimizing an expected cumulative cost while taking into account risk, i.e., increased awareness of events of small probability and high consequences. Accordingly, the objective of this paper is to present efficient reinforcement learning algorithms for risk-constrained Markov decision processes (MDPs), where risk is represented via a chance constraint or a constraint on the conditional value-at-risk (CVaR) of the cumulative cost. We collectively refer to such problems as percentile risk-constrained MDPs. Specifically, we first derive a formula for computing the gradient of the Lagrangian function for percentile risk-constrained MDPs. Then, we devise policy gradient and actor-critic algorithms that (1) estimate such gradient, (2) update the policy in the descent direction, and (3) update the Lagrange multiplier in the ascent direction. For these algorithms we prove convergence to locally optimal policies. Finally, we demonstrate the effectiveness of our algorithms in an optimal stopping problem and an online marketing application.},
	archivePrefix = {arXiv},
	arxivId = {1512.01629},
	author = {Chow, Yinlam and Ghavamzadeh, Mohammad and Janson, Lucas and Pavone, Marco},
	eprint = {1512.01629},
	issn = {15337928},
	journal = {Journal of Machine Learning Research},
	keywords = {Actor-Critic Algorithms,Chance-Constrained Optimization,Conditional Value-at-Risk,Markov Decision Process,Policy Gradient Algorithms,Reinforcement Learning},
	title = {{Risk-constrained reinforcement learning with percentile risk criteria}},
	year = {2018}
}

@article{Artzner1999,
	abstract = {In this paper we study both market risks and nonmarket risks, without complete markets assumption, and discuss methods of measurement of these risks. We present and justify a set of four desirable properties for measures of risk, and call the measures satisfying these properties "coherent." We examine the measures of risk provided and the related actions required by SPAN, by the SEC/NASD rules, and by quantile-based methods. We demonstrate the universality of scenario-based methods for providing coherent measures. We offer suggestions concerning the SEC method. We also suggest a method to repair the failure of subadditivity of quantile-based methods.},
	author = {Artzner, Philippe and Delbaen, Freddy and Eber, Jean Marc and Heath, David},
	doi = {10.1111/1467-9965.00068},
	issn = {09601627},
	journal = {Mathematical Finance},
	keywords = {Aggregation of risks,Butterfly,Capital requirement,Coherent risk measure,Concentration of risks,Currency risk,Decentralization,Extremal events risk,Insurance risk,Margin requirement,Market risk,Mean excess function,Measure of risk,Model risk},
	title = {{Coherent measures of risk}},
	year = {1999}
}

@article{Delage2010,
	abstract = {Markov decision processes are an effective tool in modeling decision making in uncertain dynamic environments. Because the parameters of these models typically are estimated from data or learned from experience, it is not surprising that the actual performance of a chosen strategy often differs significantly from the designer's initial expectations due to unavoidable modeling ambiguity. In this paper, we present a set of percentile criteria that are conceptually natural and representative of the trade-off between optimistic and pessimistic views of the question. We study the use of these criteria under different forms of uncertainty for both the rewards and the transitions. Some forms are shown to be efficiently solvable and others highly intractable. In each case, we outline solution concepts that take parametric uncertainty into account in the process of decision making. {\textcopyright} 2010 INFORMS.},
	author = {Delage, Erick and Mannor, Shie},
	doi = {10.1287/opre.1080.0685},
	issn = {0030364X},
	journal = {Operations Research},
	keywords = {Chance-constrained optimization,Finite state,Markov decision processes,Parameter uncertainty,Stochastic model applications,Stochastic programming,Value at risk},
	title = {{Percentile optimization for Markov decision processes with parameter uncertainty}},
	year = {2010}
}

@incollection{Tamar2015,
	title = {Policy Gradient for Coherent Risk Measures},
	author = {Tamar, Aviv and Chow, Yinlam and Ghavamzadeh, Mohammad and Mannor, Shie},
	booktitle = {Advances in Neural Information Processing Systems 28},
	editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
	pages = {1468--1476},
	year = {2015},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/5923-policy-gradient-for-coherent-risk-measures.pdf}
}

@article{Geibel2005,
	abstract = {In this paper, we consider Markov Decision Processes (MDPs) with error states. Error states are those states entering which is undesirable or dangerous. We define the risk with respect to a policy as the probability of entering such a state when the policy is pursued. We consider the problem of finding good policies whose risk is smaller than some user-specified threshold, and formalize it as a constrained MDP with two criteria. The first criterion corresponds to the value function originally given. We will show that the risk can be formulated as a second criterion function based on a cumulative return, whose definition is independent of the original value function. We present a model free, heuristic reinforcement learning algorithm that aims at finding good deterministic policies. It is based on weighting the original value function and the risk. The weight parameter is adapted in order to find a feasible solution for the constrained problem that has a good performance with respect to the value function. The algorithm was successfully applied to the control of a feed tank with stochastic inflows that lies upstream of a distillation column. This control task was originally formulated as an optimal control problem with chance constraints, and it was solved under certain assumptions on the model to obtain an optimal solution. The power of our learning algorithm is that it can be used even when some of these restrictive assumptions are relaxed. {\textcopyright}2005 AI Access Foundation. All rights reserved.},
	archivePrefix = {arXiv},
	arxivId = {1109.2147},
	author = {Geibel, Peter and Wysotzki, Fritz},
	doi = {10.1613/jair.1666},
	eprint = {1109.2147},
	issn = {10769757},
	journal = {Journal of Artificial Intelligence Research},
	title = {{Risk-sensitive reinforcement learning applied to control under constraints}},
	year = {2005}
}

@inproceedings{Berkenkamp2015,
	abstract = {This paper introduces a learning-based robust control algorithm that provides robust stability and performance guarantees during learning. The approach uses Gaussian process (GP) regression based on data gathered during operation to update an initial model of the system and to gradually decrease the uncertainty related to this model. Embedding this data-based update scheme in a robust control framework guarantees stability during the learning process. Traditional robust control approaches have not considered online adaptation of the model and its uncertainty before. As a result, their controllers do not improve performance during operation. Typical machine learning algorithms that have achieved similar high-performance behavior by adapting the model and controller online do not provide the guarantees presented in this paper. In particular, this paper considers a stabilization task, linearizes the nonlinear, GP-based model around a desired operating point, and solves a convex optimization problem to obtain a linear robust controller. The resulting performance improvements due to the learning-based controller are demonstrated in experiments on a quadrotor vehicle.},
	author = {Berkenkamp, Felix and Schoellig, Angela P.},
	booktitle = {2015 European Control Conference, ECC 2015},
	doi = {10.1109/ECC.2015.7330913},
	isbn = {9783952426937},
	title = {{Safe and robust learning control with Gaussian processes}},
	year = {2015}
}

@inproceedings{Tamar2014,
	abstract = {We consider large-scale Markov decision processes (MDPs) with parameter uncertainty, un-der the robust MDP paradigm. Previous studies showed that robust MDPs, based on a minimax approach to handling uncertainty, can be solved using dynamic programming for small to medium sized problems. However, due to the "curse of dimensionality", MDPs that model real-life problems are typically prohibitively large for such approaches. In this work we employ a reinforcement learning approach to tackle this planning problem: we develop a robust approximate dynamic programming method based on a projected fixed point equation to approximately solve large scale robust MDPs. We show that the proposed method provably succeeds under certain technical conditions, and demonstrate its effectiveness through simulation of an option pricing problem. To the best of our knowledge, this is the first attempt to scale up the robust MDP paradigm.},
	author = {Tamar, Aviv and Mannor, Shie and Xu, Huan},
	booktitle = {31st International Conference on Machine Learning, ICML 2014},
	isbn = {9781634393973},
	title = {{Scaling up robust MDPs using function approximation}},
	year = {2014}
}


@inproceedings{Bouton2019,
	abstract = {Navigating urban environments represents a complex task for automated vehicles. They must reach their goal safely and efficiently while considering a multitude of traffic participants. We propose a modular decision making algorithm to autonomously navigate intersections, addressing challenges of existing rule-based and reinforcement learning (RL) approaches. We first present a safe RL algorithm relying on a model-checker to ensure safety guarantees. To make the decision strategy robust to perception errors and occlusions, we introduce a belief update technique using a learning based approach. Finally, we use a scene decomposition approach to scale our algorithm to environments with multiple traffic participants. We empirically demonstrate that our algorithm outperforms rule-based methods and reinforcement learning techniques on a complex intersection scenario.},
	archivePrefix = {arXiv},
	arxivId = {1904.11483},
	author = {Bouton, Maxime and Nakhaei, Alireza and Fujimura, Kikuo and Kochenderfer, Mykel J.},
	booktitle = {IEEE Intelligent Vehicles Symposium, Proceedings},
	doi = {10.1109/IVS.2019.8813803},
	eprint = {1904.11483},
	isbn = {9781728105604},
	title = {{Safe reinforcement learning with scene decomposition for navigating complex urban environments}},
	year = {2019}
}

@article{Bouton2019workshop,
	title={Reinforcement learning with probabilistic guarantees for autonomous driving},
	author={Bouton, Maxime and Karlsson, Jesper and Nakhaei, Alireza and Fujimura, Kikuo and Kochenderfer, Mykel J and Tumova, Jana},
	journal={Workshop on Safety Risk and Uncertainty in Reinforcement Learning, Conference on Uncertainty in Artificial Intelligence (UAI)},
	year={2019}
}

@inproceedings{Koller2019,
	abstract = {Learning-based methods have been successful in solving complex control tasks without significant prior knowledge about the system. However, these methods typically do not provide any safety guarantees, which prevents their use in safety-critical, real-world applications. In this paper, we present a learning-based model predictive control scheme that can provide provable high-probability safety guarantees. To this end, we exploit regularity assumptions on the dynamics in terms of a Gaussian process prior to construct provably accurate confidence intervals on predicted trajectories. Unlike previous approaches, we do not assume that model uncertainties are independent. Based on these predictions, we guarantee that trajectories satisfy safety constraints. Moreover, we use a terminal set constraint to recursively guarantee the existence of safe control actions at every iteration. In our experiments, we show that the resulting algorithm can be used to safely and efficiently explore and learn about dynamic systems.},
	archivePrefix = {arXiv},
	arxivId = {1803.08287},
	author = {Koller, Torsten and Berkenkamp, Felix and Turchetta, Matteo and Krause, Andreas},
	booktitle = {Proceedings of the IEEE Conference on Decision and Control},
	doi = {10.1109/CDC.2018.8619572},
	eprint = {1803.08287},
	isbn = {9781538613955},
	issn = {07431546},
	title = {{Learning-Based Model Predictive Control for Safe Exploration}},
	year = {2019}
}

@techreport{Williams2018,
	abstract = {We present an algorithmic framework for stochastic model predictive control that is able to optimize non-linear systems with cost functions that have sparse, discontinuous gradient information. The proposed framework combines the benefits of sampling-based model predictive control with linearization-based trajectory optimization methods. The resulting algorithm consists of a novel utilization of Tube-based model predictive control. We demonstrate robust algorithmic performance on a variety of simulated tasks, and on a real-world fast autonomous driving task.},
	author = {Williams, Grady and Goldfain, Brian and Drews, Paul and Saigol, Kamil and Rehg, James M and Theodorou, Evangelos A},
	title = {{Robust Sampling Based Model Predictive Control with Sparse Objective Information}},
	year = {2018}
}

@article{Naghshvar2018,
	archivePrefix = {arXiv},
	arxivId = {arXiv:1812.01254v1},
	author = {Naghshvar, Mohammad and Sadek, Ahmed K and Wiggers, Auke J},
	eprint = {arXiv:1812.01254v1},
	title = {{Risk-averse Behavior Planning for Autonomous Driving under Uncertainty}},
	year = {2018}
}

@inproceedings{Berkenkamp2016,
	abstract = {One of the most fundamental problems when designing controllers for dynamic systems is the tuning of the controller parameters. Typically, a model of the system is used to obtain an initial controller, but ultimately the controller parameters must be tuned manually on the real system to achieve the best performance. To avoid this manual tuning step, methods from machine learning, such as Bayesian optimization, have been used. However, as these methods evaluate different controller parameters on the real system, safety-critical system failures may happen. In this paper, we overcome this problem by applying, for the first time, a recently developed safe optimization algorithm, SafeOpt, to the problem of automatic controller parameter tuning. Given an initial, low-performance controller, SafeOpt automatically optimizes the parameters of a control law while guaranteeing safety. It models the underlying performance measure as a Gaussian process and only explores new controller parameters whose performance lies above a safe performance threshold with high probability. Experimental results on a quadrotor vehicle indicate that the proposed method enables fast, automatic, and safe optimization of controller parameters without human intervention.},
	archivePrefix = {arXiv},
	arxivId = {1509.01066},
	author = {Berkenkamp, Felix and Schoellig, Angela P. and Krause, Andreas},
	booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
	doi = {10.1109/ICRA.2016.7487170},
	eprint = {1509.01066},
	isbn = {9781467380263},
	issn = {10504729},
	title = {{Safe controller optimization for quadrotors with Gaussian processes}},
	year = {2016}
}

@inproceedings{Turchetta2016,
	abstract = {In classical reinforcement learning agents accept arbitrary short term loss for long term gain when exploring their environment. This is infeasible for safety critical applications such as robotics, where even a single unsafe action may cause system failure or harm the environment. In this paper, we address the problem of safely exploring finite Markov decision processes (MDP). We define safety in terms of an a priori unknown safety constraint that depends on states and actions and satisfies certain regularity conditions expressed via a Gaussian process prior. We develop a novel algorithm, SAFEMDP, for this task and prove that it completely explores the safely reachable part of the MDP without violating the safety constraint. To achieve this, it cautiously explores safe states and actions in order to gain statistical confidence about the safety of unvisited state-action pairs from noisy observations collected while navigating the environment. Moreover, the algorithm explicitly considers reachability when exploring the MDP, ensuring that it does not get stuck in any state with no safe way out. We demonstrate our method on digital terrain models for the task of exploring an unknown map with a rover.},
	archivePrefix = {arXiv},
	arxivId = {1606.04753},
	author = {Turchetta, Matteo and Berkenkamp, Felix and Krause, Andreas},
	booktitle = {Advances in Neural Information Processing Systems},
	eprint = {1606.04753},
	issn = {10495258},
	title = {{Safe exploration in finite Markov decision processes with Gaussian processes}},
	year = {2016}
}

@book{Altman1999,
	title = {Constrained Markov Decision Processes},
	author = {Altman, Eitan},
	year = {1999},
	publisher = {Chapman and Hall/CRC},
}

@inproceedings{Achiam2017,
	abstract = {For many applications of reinforcement learning it can be more convenient to specify both a reward function and constraints, rather than trying to design behavior through the reward function. For example, systems that physically interact with or around humans should satisfy safety constraints. Recent advances in policy search algorithms (Mnih et al., 2016; Schulman et al., 2015; Lillicrap et al., 2016; Levine et al., 2016) have enabled new capabilities in high-dimensional control, but do not consider the constrained setting. We propose Constrained Policy Optimization (CPO), the first generalpurpose policy search algorithm for constrained reinforcement learning with guarantees for nearconstraint satisfaction at each iteration. Our method allows us to train neural network policies for high-dimensional control while making guarantees about policy behavior all throughout training. Our guarantees are based on a new theoretical result, which is of independent interest: we prove a bound relating the expected returns of two policies to an average divergence between them. We demonstrate the effectiveness of our approach on simulated robot locomotion tasks where the agent must satisfy constraints motivated by safety.},
	archivePrefix = {arXiv},
	arxivId = {1705.10528},
	author = {Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter},
	booktitle = {34th International Conference on Machine Learning, ICML 2017},
	eprint = {1705.10528},
	isbn = {9781510855144},
	title = {{Constrained policy optimization}},
	year = {2017}
}

@inproceedings{Tessler2019,
	abstract = {Solving tasks in Reinforcement Learning is no easy feat. As the goal of the agent is to maximize the accumulated reward, it often learns to exploit loopholes and misspecifications in the reward signal resulting in unwanted behavior. While constraints may solve this issue, there is no closed form solution for general constraints. In this work, we present a novel multi-timescale approach for constrained policy optimization, called 'Reward Constrained Policy Optimization' (RCPO), which uses an alternative penalty signal to guide the policy towards a constraint satisfying one. We prove the convergence of our approach and provide empirical evidence of its ability to train constraint satisfying policies.},
	archivePrefix = {arXiv},
	arxivId = {1805.11074},
	author = {Tessler, Chen and Mankowitz, Daniel J. and Mannor, Shie},
	booktitle = {7th International Conference on Learning Representations, ICLR 2019},
	eprint = {1805.11074},
	title = {{Reward constrained policy optimization}},
	year = {2019}
}

@article{leung2018infusing,
	title={On infusing reachability-based safety assurance within probabilistic planning frameworks for human-robot vehicle interactions},
	author={Leung, Karen and Schmerling, Edward and Chen, Mo and Talbot, John and Gerdes, J Christian and Pavone, Marco},
	journal={arXiv preprint arXiv:1812.11315},
	year={2018}
}

@article{Fisac2019,
	abstract = {The proven efficacy of learning-based control schemes strongly motivates their application to robotic systems operating in the physical world. However, guaranteeing correct operation during the learning process is currently an unresolved issue, which is of vital importance in safety-critical systems. We propose a general safety framework based on Hamilton-Jacobi reachability methods that can work in conjunction with an arbitrary learning algorithm. The method exploits approximate knowledge of the system dynamics to guarantee constraint satisfaction while minimally interfering with the learning process. We further introduce a Bayesian mechanism that refines the safety analysis as the system acquires new evidence, reducing initial conservativeness when appropriate while strengthening guarantees through real-Time validation. The result is a least-restrictive, safety-preserving control law that intervenes only when the computed safety guarantees require it, or confidence in the computed guarantees decays in light of new observations. We prove theoretical safety guarantees combining probabilistic and worst-case analysis and demonstrate the proposed framework experimentally on a quadrotor vehicle. Even though safety analysis is based on a simple point-mass model, the quadrotor successfully arrives at a suitable controller by policy-gradient reinforcement learning without ever crashing, and safely retracts away from a strong external disturbance introduced during flight.},
	archivePrefix = {arXiv},
	arxivId = {1705.01292},
	author = {Fisac, Jaime F. and Akametalu, Anayo K. and Zeilinger, Melanie N. and Kaynama, Shahab and Gillula, Jeremy and Tomlin, Claire J.},
	doi = {10.1109/TAC.2018.2876389},
	eprint = {1705.01292},
	issn = {15582523},
	journal = {IEEE Transactions on Automatic Control},
	keywords = {Gaussian processes,Safety,autonomous systems,robot learning,robust optimal control},
	title = {{A General Safety Framework for Learning-Based Control in Uncertain Robotic Systems}},
	year = {2019}
}

@inproceedings{Le2019,
	abstract = {When learning policies for real-world domains, two important questions arise: (i) how to efficiently use pre-collected off-policy, non-optimal behavior data; and (ii) how to mediate among different competing objectives and constraints. We thus study the problem of batch policy learning under multiple constraints, and offer a systematic solution. We first propose a flexible meta-algorithm that admits any batch reinforcement learning and online learning procedure as subroutines. We then present a specific algorithmic instantiation and provide performance guarantees for the main objective and all constraints. As part of off-policy learning, we propose a simple method for off-policy policy evaluation (OPR) and derive PAC-style bounds. Our algorithm achieves strong empirical results in different domains, including in a challenging problem of simulated car driving subject to multiple constraints such as lane keeping and smooth driving. We also show experimentally that our OPR method outperforms other popular OPR techniques on a standalone basis, especially in a high-dimensional setting.},
	archivePrefix = {arXiv},
	arxivId = {1903.08738},
	author = {Le, Hoang M. and Voloshin, Cameron and Yue, Yisong},
	booktitle = {36th International Conference on Machine Learning, ICML 2019},
	eprint = {1903.08738},
	isbn = {9781510886988},
	title = {{Batch policy learning under constraints}},
	year = {2019}
}

@inproceedings{Torossian19a,
	title = 	 {$\mathcal{X}$-Armed Bandits: Optimizing Quantiles, CVaR and Other Risks},
	author = 	 {Torossian, L\'eonard and Garivier, Aur\'elien and Picheny, Victor},
	booktitle = 	 {Proceedings of The Eleventh Asian Conference on Machine Learning},
	pages = 	 {252--267},
	year = 	 {2019},
	editor = 	 {Lee, Wee Sun and Suzuki, Taiji},
	volume = 	 {101},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Nagoya, Japan},
	month = 	 {17--19 Nov},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v101/torossian19a/torossian19a.pdf},
	url = 	 {http://proceedings.mlr.press/v101/torossian19a.html},
	abstract = 	 {We propose and analyze StoROO, an algorithm for risk optimization on stochastic black-box functions derived from StoOO. Motivated by risk-averse decision making fields like agriculture, medicine, biology or finance, we do not focus on the mean payoff but on generic functionals of the return distribution. We provide a generic regret analysis of StoROO and illustrate its applicability with two examples: the optimization of quantiles and CVaR. Inspired by the bandit literature and black-box mean optimizers, StoROO relies on the possibility to construct confidence intervals for the targeted functional based on random-size samples. We detail their construction in the case of quantiles, providing tight bounds based on Kullback-Leibler divergence. We finally present numerical experiments that show a dramatic impact of tight bounds for the optimization of quantiles and CVaR.}
}

@techreport{Fraichard2014,
	TITLE = {{Will the Driver Seat Ever Be Empty?}},
	AUTHOR = {Fraichard, Thierry},
	URL = {https://hal.inria.fr/hal-00965176},
	TYPE = {Research Report},
	NUMBER = {RR-8493},
	INSTITUTION = {{INRIA}},
	YEAR = {2014},
	MONTH = Mar,
	PDF = {https://hal.inria.fr/hal-00965176/file/14-rr-fraichard.pdf},
	HAL_ID = {hal-00965176},
	HAL_VERSION = {v2},
}


@article{Fukushima2007,
	abstract = {This paper proposes an adaptive model predictive control (MPC) algorithm for a class of constrained linear systems, which estimates system parameters on-line and produces the control input satisfying input/state constraints for possible parameter estimation errors. The key idea is to combine the robust MPC method based on the comparison model with an adaptive parameter estimation method suitable for MPC. To this end, first, a new parameter update method based on the moving horizon estimation is proposed, which allows to predict an estimation error bound over the prediction horizon. Second, an adaptive MPC algorithm is developed by combining the on-line parameter estimation with an MPC method based on the comparison model, suitably modified to cope with the time-varying case. This method guarantees feasibility and stability of the closed-loop system in the presence of state/input constraints. A numerical example is given to demonstrate its effectiveness. {\textcopyright} 2006 Elsevier Ltd. All rights reserved.},
	author = {Fukushima, Hiroaki and Kim, Tae Hyoung and Sugie, Toshiharu},
	doi = {10.1016/j.automatica.2006.08.026},
	issn = {00051098},
	journal = {Automatica},
	keywords = {Adaptive estimation,Comparison principle,Constrained systems,Model predictive control,Robust stability},
	title = {{Adaptive model predictive control for a class of constrained linear systems based on the comparison model}},
	year = {2007}
}


@article{Adetola2009,
	abstract = {In this paper, a method is proposed for the adaptive model predictive control of constrained nonlinear system. Rather than relying on the inherent robustness properties of standard NMPC, the developed technique explicitly account for the transient effect of parametric estimation error by combining a parameter adjustment mechanism with robust MPC algorithms. The parameter estimation routine employed guarantees non-increase of the estimation error vector. This means that the controller employs a process model which approaches that of the true system over time. These estimates are used to update the parameter uncertainty set at every time step, resulting in a gradual reduction in the conservative and/or computational effects of the incorporated robust features. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
	author = {Adetola, Veronica and DeHaan, Darryl and Guay, Martin},
	doi = {10.1016/j.sysconle.2008.12.002},
	issn = {01676911},
	journal = {Systems and Control Letters},
	keywords = {Adaptive control,Model predictive control,Nonlinear systems,Robust control},
	title = {{Adaptive model predictive control for constrained nonlinear systems}},
	year = {2009}
}


@article{Aswani2013,
	abstract = {Controller design faces a trade-off between robustness and performance, and the reliability of linear controllers has caused many practitioners to focus on the former. However, there is renewed interest in improving system performance to deal with growing energy constraints. This paper describes a learning-based model predictive control (LBMPC) scheme that provides deterministic guarantees on robustness, while statistical identification tools are used to identify richer models of the system in order to improve performance; the benefits of this framework are that it handles state and input constraints, optimizes system performance with respect to a cost function, and can be designed to use a wide variety of parametric or nonparametric statistical tools. The main insight of LBMPC is that safety and performance can be decoupled under reasonable conditions in an optimization framework by maintaining two models of the system. The first is an approximate model with bounds on its uncertainty, and the second model is updated by statistical methods. LBMPC improves performance by choosing inputs that minimize a cost subject to the learned dynamics, and it ensures safety and robustness by checking whether these same inputs keep the approximate model stable when it is subject to uncertainty. Furthermore, we show that if the system is sufficiently excited, then the LBMPC control action probabilistically converges to that of an MPC computed using the true dynamics. {\textcopyright} 2013 Elsevier Ltd. All rights reserved.},
	author = {Aswani, Anil and Gonzalez, Humberto and Sastry, S. Shankar and Tomlin, Claire},
	doi = {10.1016/j.automatica.2013.02.003},
	issn = {00051098},
	journal = {Automatica},
	keywords = {Learning control,Predictive control,Robustness,Safety analysis,Statistics},
	title = {{Provably safe and robust learning-based model predictive control}},
	year = {2013}
}

@Article{Lorenzen2017,
	author={Lorenzen, Matthias
	and Allg{\"o}wer, Frank
	and Cannon, Mark},
	title={Adaptive Model Predictive Control with Robust Constraint Satisfaction},
	journal={IFAC-PapersOnLine},
	year={2017},
	month={Jul},
	day={01},
	volume={50},
	number={1},
	pages={3313-3318},
	keywords={Model Predictive Control; Adaptive Control; Constraint Satisfaction Problems; Uncertain Linear Systems; System Identification},
	abstract={Adaptive control for constrained, linear systems is addressed and a solution based on Model Predictive Control (MPC) and set-membership system identification is presented. The paper introduces a computationally tractable solution which uses observations of past state and input trajectories to update the model and improve control performance while maintaining guaranteed constraint satisfaction and recursive feasibility. The developed approach is applied to a stabilizing MPC scheme and practical stability under persistent, additive disturbance is proved. A numerical example and brief comparison with non-adaptive MPC is provided.},
	issn={2405-8963}
}

@inproceedings{Lu2019,
	abstract = {An adaptive Model Predictive Control (adaptive MPC) strategy is proposed for linear systems with constant unknown model parameters, bounded additive disturbances and state and control constraints. By combining online set-based identification and robust tube MPC, the proposed controller reduces the conservativeness of constraint handling, guarantees recursive feasibility and provides asymptotic bounds on the closed loop system state that depend explicitly on the the identified parameter set. Computational tractability is ensured by using fixed complexity polytopic sets to bound the model parameters and predicted states. Convex conditions for persistence of excitation are considered. The results are illustrated by a numerical example.},
	author = {Lu, Xiaonan and Cannon, Mark},
	booktitle = {Proceedings of the American Control Conference},
	isbn = {9781538679265},
	issn = {07431619},
	title = {{Robust adaptive tube model predictive control}},
	year = {2019}
}

@INPROCEEDINGS{Kohler2019,
	author={J. {Köhler} and E. {Andina} and R. {Soloperto} and M. A. {Müller} and F. {Allgöwer}},
	booktitle={2019 IEEE 58th Conference on Decision and Control (CDC)}, 
	title={Linear robust adaptive model predictive control: Computational complexity and conservatism}, 
	year={2019},
	volume={},
	number={},
	pages={1383-1388},
}

@article{Macek2009,
	abstract = {This paper describes the deliberative part of a navigation architecture designed for safe vehicle navigation in dynamic urban environments. It comprises two key modules working together in a hierarchical fashion: (a) the Route Planner whose purpose is to compute a valid itinerary towards the a given goal. An itinerary comprises a geometric path augmented with additional information based on the structure of the environment considered and traffic regulations, and (b) the Partial Motion Planner whose purpose is to ensure the proper following of the itinerary while dealing with the moving objects present in the environment (eg other vehicles, pedestrians). In the architecture proposed, a special attention is paid to the motion safety issue, ie the ability to avoid collisions. Different safety levels are explored and their operational conditions are explicitly spelled out (something which is usually not done).},
	author = {Ma{\^{c}}ek, Kristijan and Vasquez, Dizan and Fraichard, Thierry and Siegwart, Roland},
	issn = {00051144},
	journal = {Automatika},
	keywords = {Dynamic environment,Motion planning,Motion safety,Urban navigation},
	title = {{Towards safe vehicle navigation in dynamic urban scenarios}},
	year = {2009}
}

@article{Bouraine2012,
	abstract = {This paper addresses the problem of navigating in a provably safe manner a mobile robot with a limited field-of-view placed in a unknown dynamic environment. In such a situation, absolute motion safety (in the sense that no collision will ever take place whatever happens in the environment) is impossible to guarantee in general. It is therefore settled for a weaker level of motion safety dubbed passive motion safety: it guarantees that, if a collision takes place, the robot will be at rest. The primary contribution of this paper is the concept of Braking Inevitable Collision States (ICS), i.e. a version of the ICS corresponding to passive motion safety. Braking ICS are defined as states such that, whatever the future braking trajectory followed by the robot, a collision occurs before it is at rest. Passive motion safety is obtained by avoiding Braking ICS at all times. It is shown that Braking ICS verify properties that allow the design of an efficient Braking ICS-Checking algorithm, i.e. an algorithm that determines whether a given state is a Braking ICS or not. To validate the Braking ICS concept and demonstrate its usefulness, the Braking ICS-Checking algorithm is integrated in a reactive navigation scheme called PASSAVOID. It is formally established that PASSAVOID is provably passively safe in the sense that it is guaranteed that the robot will always stay away from Braking ICS no matter what happens in the environment. {\textcopyright} 2011 Springer-Verlag.},
	author = {Bouraine, Sara and Fraichard, Thierry and Salhi, Hassen},
	doi = {10.1007/s10514-011-9258-8},
	issn = {09295593},
	journal = {Autonomous Robots},
	keywords = {Autonomous navigation,Collision avoidance,Dynamic environments,Inevitable collision states,Mobile robots,Motion safety},
	title = {{Provably safe navigation for mobile robots with limited field-of-views in dynamic environments}},
	year = {2012}
}

@inproceedings{Bouraine2014,
	abstract = {This paper addresses the problem of planning the motion of a mobile robot with a limited sensory field-of-view in an unknown dynamic environment. In such a situation, the upper-bounded planning time prevents from computing a complete motion to the goal, partial motion planning is in order. Besides the presence of moving obstacles whose future behaviour is unknown precludes absolute motion safety (in the sense that no collision will ever take place whatever happens) is impossible to guarantee. The stance taken herein is to settle for a weaker level of motion safety called passive motion safety: it guarantees that, if a collision takes place, the robot will be at rest. The primary contribution of this paper is PassPMP, a partial motion planner enforcing passive motion safety. PassPMP periodically computes a passively safe partial trajectory designed to drive the robot towards its goal state. Passive motion safety is handled using a variant of the Inevitable Collision State (ICS) concept called Braking ICS, i.e. states such that, whatever the future braking trajectory of the robot, a collision occurs before it is at rest. Simulation results demonstrate how PassPMP operates and handles limited sensory field-of-views, occlusions and moving obstacles with unknown future behaviour. More importantly, PassPMP is provably passively safe.},
	author = {Bouraine, S. and Fraichard, Th and Azouaoui, O. and Salhi, Hassen},
	booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
	doi = {10.1109/ICRA.2014.6907375},
	issn = {10504729},
	title = {{Passively safe partial motion planning for mobile robots with limited field-of-views in unknown dynamic environments}},
	year = {2014}
}

@article{Mitsch2017,
	abstract = {This article answers fundamental safety questions for ground robot navigation: under which circumstances does which control decision make a ground robot safely avoid obstacles? Unsurprisingly, the answer depends on the exact formulation of the safety objective, as well as the physical capabilities and limitations of the robot and the obstacles. Because uncertainties about the exact future behavior of a robot's environment make this a challenging problem, we formally verify corresponding controllers and provide rigorous safety proofs justifying why the robots can never collide with the obstacle in the respective physical model. To account for ground robots in which different physical phenomena are important, we analyze a series of increasingly strong properties of controllers for increasingly rich dynamics and identify the impact that the additional model parameters have on the required safety margins. We analyze and formally verify: (i) static safety, which ensures that no collisions can happen with stationary obstacles; (ii) passive safety, which ensures that no collisions can happen with stationary or moving obstacles while the robot moves; (iii) the stronger passive-friendly safety, in which the robot further maintains sufficient maneuvering distance for obstacles to avoid collision as well; and (iv) passive orientation safety, which allows for imperfect sensor coverage of the robot, i.e., the robot is aware that not everything in its environment will be visible. We formally prove that safety can be guaranteed despite sensor uncertainty and actuator perturbation. We complement these provably correct safety properties with liveness properties: we prove that provably safe motion is flexible enough to let the robot navigate waypoints and pass intersections. To account for the mixed influence of discrete control decisions and the continuous physical motion of the ground robot, we develop corresponding hybrid system models and use differential dynamic logic theorem-proving techniques to formally verify their correctness. Since these models identify a broad range of conditions under which control decisions are provably safe, our results apply to any control algorithm for ground robots with the same dynamics. As a demonstration, we also synthesize provably correct runtime monitor conditions that check the compliance of any control algorithm with the verified control decisions.},
	archivePrefix = {arXiv},
	arxivId = {1605.00604},
	author = {Mitsch, Stefan and Ghorbal, Khalil and Vogelbacher, David and Platzer, Andr{\'{e}}},
	doi = {10.1177/0278364917733549},
	eprint = {1605.00604},
	issn = {17413176},
	journal = {International Journal of Robotics Research},
	keywords = {Provable correctness,ground robot,hybrid systems,navigation,obstacle avoidance,theorem proving},
	title = {{Formal verification of obstacle avoidance and navigation of ground robots}},
	year = {2017}
}
